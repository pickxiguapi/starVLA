# InternVLA-M1

> ä¸€ä½“åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œ (Vision-Language-Action, VLA) å¼€æºæ¡†æ¶  
> End-to-end, modular, research-friendly.

<!-- TODO: åœ¨æ­¤å¤„æ’å…¥é¡¹ç›® Logo / æ¶æ„å›¾ / åŠ¨å›¾ -->
<!-- TODO: Demo Video å ä½: å°† demo.mp4 æ”¾åˆ° ./assets å¹¶åœ¨æ­¤å¼•ç”¨ -->

# Introduction
InternVLA-M1 is a open-source, end-to-end visionâ€“languageâ€“action (VLA) framework. 

## ğŸ”¥ æ ¸å¿ƒç‰¹æ€§ (Key Features)

1. Modular & Extensible  
   Core components (VLM, Action Model, Projector, DINO, Trainer) are fully decoupled. You can plug in custom vision-language backbones, action policies, or feature projectors without touching the rest. A unified data interface (e.g., LeRobot + custom robotics datasets) lowers integration and research iteration cost.

2. Dual-System and Dual-Supervision
InternVLA-M1 integrates a unified architecture with both a language head and an action head, enabling collaborative training under dual supervision, combining both language and action signals. This design supports learning from multimodal data, especially robotic perception data, significantly improving instruction-following capability.

3. Efficient Training & Fast Convergence  
   Learns spatial / visual priors from large-scale multimodal pretraining, then transfers them via spatial prompt fine-tuning. Achieves strong performance (e.g., OXE SOTA-level convergence in ~2.5 epochs without separate action pretraining). Builtâ€‘in optimizations: FlashAttention2, BF16, gradient accumulation, distributed (torchrun / DeepSpeedâ€‘ready).



---

## ğŸ“‚ ç›®å½•ç»“æ„ (Repo Structure)

```text
InternVLA
â”œâ”€â”€ model
â”‚   â”œâ”€â”€ framework            # ä¸»æ¡†æ¶ (æ•°æ®æµ / loss / forward)
â”‚   â”œâ”€â”€ modules
â”‚   â”‚   â”œâ”€â”€ vlm              # å„ç±»å¤šæ¨¡æ€ / è¯­è¨€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ action_model     # åŠ¨ä½œç­–ç•¥ / æ§åˆ¶æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ projector        # ç‰¹å¾å¯¹é½ / ç©ºé—´æ˜ å°„
â”‚   â”‚   â”œâ”€â”€ dino_model       # è§†è§‰ç»†èŠ‚ç‰¹å¾
â”œâ”€â”€ dataloader
â”‚   â”œâ”€â”€ groot_lerobot        # LeRobot / Groot æ•°æ®é€‚é…
â”œâ”€â”€ training
â”‚   â”œâ”€â”€ train_vlm
â”‚   â”œâ”€â”€ train_vla
â”‚   â”œâ”€â”€ train_vla_withCotrain
â”œâ”€â”€ config                   # å…¨å±€ç»Ÿä¸€å®éªŒé…ç½® (YAML)
â”œâ”€â”€ real_deployment          # éƒ¨ç½²ä¸æ¨ç†
â”‚   â”œâ”€â”€ deploy/server_policy.py
â”œâ”€â”€ scripts                  # è®­ç»ƒä¸è¯„ä¼°è„šæœ¬
â”œâ”€â”€ playground               # å»ºè®®å°†ç¬¦å·é“¾æ¥æ”¾åœ¨æ­¤
â”‚   â”œâ”€â”€ Datasets             
â”‚   â”œâ”€â”€ Pretrain_models
```

---

## ğŸ›  ç¯å¢ƒå‡†å¤‡ (Environment Setup)

å¿«é€Ÿå®‰è£…ï¼š

```bash
conda create -n internVLA python=3.10 -y
conda activate internVLA

# åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# FlashAttention2 (ç¡®ä¿ Torch/CUDA ç‰ˆæœ¬åŒ¹é…)
pip install flash-attn --no-build-isolation

# å¯ç¼–è¾‘å®‰è£…
pip install -e .


```

---


## ğŸš€ å¿«é€Ÿä¸Šæ‰‹ (Quick Start)
### Jinhui åœ¨å†™è¿™éƒ¨åˆ†ï¼Œ å°±æ˜¯é«˜æ•°å…¶ä»–äººå¦‚æœæˆ‘ä»¬è¦ follow æˆ‘ä»¬çš„å·¥ä½œï¼Œ å¤§æ¦‚çš„è·¯çº¿
å¤ç° --> å‡†å¤‡æ•°æ® --> å‡†å¤‡æ¨¡å‹ --> è®­ç»ƒ --> æµ‹è¯•

## 1 å¤ç°æˆ‘ä»¬çš„ç»“æœ on SimplerENV
æˆ‘ä»¬ä¼šåœ¨examples/ 
è¿™é‡Œ æä¾› å¦‚æœ reproduce internVLA-M1 


## ğŸ§© æ‰©å±•InternVLA to your work (How to Extend)

### 1. Data Format & Loadingï¼š
æˆ‘ä»¬æ•°æ®æ•°æ®æ ¼å¼å€Ÿé‰´äº†å¼€æºçš„æœ€ä½³å®è·µã€‚ä¾‹å¦‚actionæ•°æ®é‡‡ç”¨ LeRobot provide by GR00T : https://github.com/NVIDIA/Isaac-GR00T. å¤šæ¨¡æ€æ•°æ® follow Qwen2.5-VL : https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune

æˆ‘å¯ä»¥å‚è€ƒä»–ä»¬çš„è§„èŒƒå‡†å¤‡æ•°æ®ã€‚and in æˆ‘ä»¬codebase, 
å¯¹äº action æ•°æ® åº”è¯¥æ˜¯
python ./InternVLA/dataloader/lerobot_datasets_oxe.py 
æŸ¥çœ‹æ˜¯å¦èƒ½å¦æˆåŠŸè¿”å›æ•°æ®
æ¯ä¸ªæ•°æ®çš„åˆ†ä¼šè§„èŒƒ æ˜¯
****

åŒç†ï¼Œ åˆ©ç”¨ python ./InternVLA/dataloader/vlm_datasets.py debug æŸ¥çœ‹ ä½ çš„dataloader æ˜¯å¦å¥½ã€‚

ä¸€æ—¦ dataloader è¿”å›çš„å†…å®¹ç¬¦åˆä½ çš„é¢„æœŸï¼Œ æ³¨å†Œ your own dataloader InternVLA/dataloader/__init__.py


### 2. æ¨¡å‹å¼€å‘ï¼š
ä½ æˆ–è®¸ä¼šå¼€å‘ä½ è‡ªå·±çš„æ¨¡å‹
æˆ‘ä»¬çº¦å®š ä½ åªèƒ½æœ‰ä¸€ä¸ª framework.py (ä¾‹å¦‚ InternVLA/model/framework/M1.py ) align with ther framework fig in your papar. 
ä½ å¯ä»¥åœ¨ InternVLA/model/modules ä¸­å®šä¹‰ä½ framework éœ€è¦çš„æ¨¡å—ã€‚ but ä½ æƒ³è¦ä¿è¯ you can python framework.py. then build your model and forward your more with batch sample. 
 
 then 
register your framework in InternVLA/model/framework/__init__.py.


### 3. æ¨¡å‹éƒ¨ç½²

æ‰€æœ‰çš„framework èƒ½å¤Ÿé€šè¿‡
from InternVLA.model.framework.yourmodel import Yourframeork
your_model_ckpt="playground/Checkpoints/debug_0910_internM1_cotrain/checkpoints/steps_2000_pytorch_model.pt"

your_model = Yourframeork.from_pretrained(your_model_ckpt)

then your kan
your_model.predict_action()

ä½ èƒ½å¤Ÿä½¿ç”¨ deployment/model_server çš„æœåŠ¡æ¥ server è¯„æµ‹


### å‚æ•°é…ç½®ï¼š
InternVLA-M1 é‡‡ç”¨ yaml æ–‡ä»¶æ¥ç®¡ç†

åªæœ‰ä¸€ä¸ª global å‚æ•°, ä»–ä»¬è¢«ç»Ÿä¸€ ç®¡ç†åœ¨ InternVLA/config/training/qwenvla_cotrain_oxe.yamlã€‚ è¿™æ˜¯ä¸€ä¸ª çµæ´»å‚æ•°å¯¹è±¡ï¼ˆä¾‹å¦‚dictï¼‰ï¼Œ å…¨å±€éƒ½å¯¹å®Œæ•´çš„å‚æ•°å¯¹è±¡å¯è§ï¼Œä½†æ˜¯åªæœ‰è¦ä½¿ç”¨çš„æ—¶å€™æ‰è®¿é—®å¯¹åº”çš„valueï¼Œ which mean å‚æ•°å¯¹è±¡ å¯ä»¥å†—ä½™ï¼Œbut ä¸èƒ½ç¼ºå¤± if åœ¨ä½ çš„å·¥ç¨‹ä¸­æ˜ç¡®è¦ä½¿ç”¨ã€‚

InternVLA-M1 å·²ç»å¯¹å‚æ•°è¿›è¡Œäº†åˆæ­¥çš„åˆ†ç»„ã€‚ ä¾‹å¦‚ datasets, framework, trainer
å‚æ•°çš„ä¼˜å…ˆçº§æ˜¯ ä½ å¯ä»¥é€š CMD è¦†ç›–æˆ–è€…å¢åŠ  å‚æ•°ã€‚

æœ€åç”Ÿæ•ˆçš„å‚æ•°ä¼šè¢«ç»Ÿä¸€ save åœ¨ckpt æ–‡ä»¶å¤¹ï¼Œ which æ–¹ä¾¿åç»­çš„å»è¯»


---

## ğŸ“ˆ Model Zoo (å ä½)

| æ¨¡å‹ | å‚æ•°è§„æ¨¡ | é¢„è®­ç»ƒæ•°æ® | ä¸‹æ¸¸ (LIBERO) æˆç»© | ä¸‹è½½ |
|------|----------|------------|--------------------|------|
| InternVLA-M1 Base | ~ | ~ | ~ | TODO |
| InternVLA-M1 Large | ~ | ~ | ~ | TODO |

ï¼ˆTODO: åç»­è¡¥å……æƒé‡ä¸æ—¥å¿—ï¼‰

---


## ğŸ§© æ‰©å±•æŒ‡å— (How to Extend)
<!-- as toDO -->
<!-- æ–°å¢ your own VLAï¼š
æˆ‘ä»¬çº¦å®š ä½ åªèƒ½æœ‰ä¸€ä¸ª framework.py (ä¾‹å¦‚ InternVLA/model/framework/M1.py ) align with ther framework fig in your papar. 
ä½ å¯ä»¥åœ¨ InternVLA/model/modules ä¸­å®šä¹‰ä½ framework éœ€è¦çš„æ¨¡å—ã€‚ but ä½ æƒ³è¦ä¿è¯ you can python framework.py. then build your model and forward your more with batch sample. 

then  register your framework in InternVLA/model/framework/__init__.py. æˆ‘ä»¬é¿å… ä½¿ç”¨ REGISTRY æ–¹æ³•ä»¥ä¿ç•™æ›´å¥½çš„å¯è¯»æ€§and æ–¹ä¾¿ç”¨æˆ· review code.

æ–°å¢ your own è®­ç»ƒå‚æ•°ï¼š
InternVLA-M1 åªæœ‰ä¸€ä¸ª global å‚æ•°, ä»–ä»¬è¢«ç»Ÿä¸€ ç®¡ç†åœ¨ InternVLA/config/training/qwenvla_cotrain_oxe.yamlã€‚ è¿™æ˜¯ä¸€ä¸ª çµæ´»å‚æ•°å¯¹è±¡ï¼ˆä¾‹å¦‚dictï¼‰ï¼Œä½ å¯ä»¥é€š CMD è¦†ç›–æˆ–è€…å¢åŠ  å‚æ•°ã€‚ å…¨å±€éƒ½å¯¹å®Œæ•´çš„å‚æ•°å¯¹è±¡å¯è§ï¼Œä½†æ˜¯åªæœ‰è¦ä½¿ç”¨çš„æ—¶å€™æ‰è®¿é—®å¯¹åº”çš„valueï¼Œ which mean å‚æ•°å¯¹è±¡ å¯ä»¥å†—ä½™ï¼Œbut ä¸èƒ½ç¼ºå¤± if åœ¨ä½ çš„å·¥ç¨‹ä¸­æ˜ç¡®è¦ä½¿ç”¨ã€‚
InternVLA-M1 å·²ç»å¯¹å‚æ•°è¿›è¡Œäº†åˆæ­¥çš„åˆ†ç»„ã€‚ ä¾‹å¦‚ datasets, framework, trainer

æ–°å¢ è®­ç»ƒç­–ç•¥ï¼š
InternVLA-M1 çš„trainer æ˜¯ è‡ªå»ºçš„base æœ€åŸºç¡€çš„ torch å‡½æ•°å®Œæˆï¼Œ ä¾‹å¦‚å¯¹äº freeze modular,  é€šè¿‡ trainer.freeze_modules ç›´æ¥å£°æ˜ ç”¨æˆ·è‡ªå·±framework moduels nameï¼Œ and æˆ‘ä»¬é€šè¿‡RE å»æŸ¥çœ‹æ¨¡å‹æ¨¡å—ï¼Œ å¹¶ä½¿ç”¨ é‡‡ç”¨æœ€åŸºç¡€çš„ torch å‡½æ•°å®Œæˆ å‚æ•°çš„å†»ç»“ï¼ˆçœ‹TrainerUtilsï¼‰


yepï¼ŒInternVLA-M1 might not ä¸Šæ‰‹å°±æ¥ï¼Œå› ä¸ºä»–ä½¿ç”¨äº†å¾ˆå¤š æœ€åŸºç¡€çš„ pytorch å·¥å…·æ¥å®Œæˆ codebase å®ç°æ›´å¥½çš„è§£å¶å’Œ ä¿æŒæ›´å¥½çš„æ‰©å±•æ€§ã€‚ but å¦‚æœtryï¼Œ your will find it ä¼˜åŠ¿ã€‚ -->

---

## ğŸ“œ Citation (å¼•ç”¨)

å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼ˆå ä½ï¼‰ï¼š

```bibtex
@misc{internvla2024,
  title  = {InternVLA-M1: An Open Vision-Language-Action Framework},
  author = {InternVLA Contributors},
  year   = {2024},
  url    = {https://github.com/...}
}
```

---

## âœ… TODO Roadmap

- [ ] å‘å¸ƒæ¨¡å‹æƒé‡
- [ ] å¢åŠ å¤šä»»åŠ¡æ··åˆè®­ç»ƒç¤ºä¾‹
- [ ] é›†æˆ Deepspeed / FSDP
- [ ] å‘å¸ƒçœŸå®æœºå™¨äºº Demo
- [ ] æ·»åŠ æ—¥å¿—å¯è§†åŒ– (TensorBoard / WandB)
- [ ] ç»Ÿä¸€è¯„ä¼°è„šæœ¬æŒ‡æ ‡è¾“å‡º

---

## ğŸ¤ Contributing

æ¬¢è¿ PR / Issueï¼š

---

## ğŸ” License

MIT License

---

## ğŸ“¬ è”ç³»

- Issueï¼šæäº¤è¯¦ç»†æ—¥å¿— + å¤ç°æ­¥éª¤
- é‚®ä»¶ï¼šTODO (å¦‚éœ€æ·»åŠ )
- äº¤æµç¾¤ï¼šTODO (å¯æ”¾é£ä¹¦/é’‰é’‰/å¾®ä¿¡ç¾¤äºŒç»´ç )

---

æ„Ÿè°¢ä½¿ç”¨ InternVLA-M1ï¼ğŸ¯ å¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œæ¬¢è¿ Star æ”¯æŒã€‚

