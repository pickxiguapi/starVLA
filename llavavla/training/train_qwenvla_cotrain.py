"""
train.py
"""
# Standard Library
import argparse
import json
import os
from pathlib import Path
from typing import Tuple
from torch.utils.data import DataLoader
import numpy as np
# Third-Party Libraries
import torch
import torch.distributed as dist
import wandb
import yaml
from accelerate import Accelerator, DeepSpeedPlugin
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from omegaconf import OmegaConf
from tqdm import tqdm
from transformers import AutoProcessor, get_scheduler

# Local Modules
# è¿™é‡Œçš„å˜åŒ–éœ€è¦ğŸ“¦å°è£… Dataloader
from llavavla.dataloader import build_dataloader

from llavavla.dataloader.vlm_datasets import make_vlm_dataloader

from llavavla.training.metrics import normalize_dotlist_args
from llavavla.model.framework import build_framework
from llavavla.training.metrics import only_main_process
from llavavla.training.metrics import TrainerUtils
from llavavla.dataloader import save_dataset_statistics

# from prismatic.overwatch import initialize_overwatch # TODO ä¹‹åè¦ç§»åŠ¨å‡ºæ¥ï¼Œ æ³¨æ„ copyrightï¼Œ è€ƒå¯Ÿå’Œloger çš„å·®å¼‚ï¼Œ ä¸ºä»€ä¹ˆè¦ç”¨å®ƒï¼Ÿ # æ„Ÿè§‰å¾—æ”¾å¼ƒæ‰ï¼Œæ€»ç»“ç”¨logger
# from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics


deepspeed_plugin = DeepSpeedPlugin()# è¿™ä¸ªæ’ä»¶æ˜¯å¦èƒ½ä½¿ç”¨åˆ° config çš„å‚æ•°å‘¢ï¼Ÿ å…¶å®è¿™é‡Œåº”è¯¥æ˜¯å¯ä»¥é£æ˜¾ç¤ºç”¨çš„ï¼Œ æ„Ÿè§‰æœ‰ç‰ˆæœ¬é—®é¢˜ #zero_stage=2, gradient_accumulation_steps=1 ï¼šv2: hf_ds_config="scripts/run_scripts/ds_config.yaml"
accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin)
accelerator.print(accelerator.state) # TODO ä¹‹åè¦ç§»åŠ¨åˆ°trainer å†…éƒ¨ï¼Œ --> ç›´æ¥æ¬LLaVA trainer

# Sane Defaults
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# Initialize Overwatch =>> Wraps `logging.Logger`
logger = get_logger(__name__)

def load_fast_tokenizer():
    fast_tokenizer = AutoProcessor.from_pretrained(
        "physical-intelligence/fast", trust_remote_code=True
    )
    return fast_tokenizer



def setup_directories(cfg) -> Path:
    """åˆ›å»ºè¾“å‡ºç›®å½•å¹¶ä¿å­˜é…ç½®"""
    cfg.output_dir = os.path.join(cfg.run_root_dir, cfg.run_id)
    output_dir = Path(cfg.output_dir)
    
    if not dist.is_initialized() or dist.get_rank() == 0:
        # åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ£€æŸ¥ç‚¹ç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(output_dir / "checkpoints", exist_ok=True)
        
        # ä¿å­˜é…ç½®
        OmegaConf.save(cfg, output_dir / "config.yaml")
        with open(output_dir / "config.yaml", "r") as f_yaml, \
                open(output_dir / "config.json", "w") as f_json:
            yaml_cfg = yaml.safe_load(f_yaml)
            json.dump(yaml_cfg, f_json, indent=2)
        
    return output_dir



def prepare_data(cfg, accelerator, output_dir) -> Tuple[DataLoader, DataLoader]:
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    # TODO @JinhuiYE å¯ä»¥å˜å¾—æ›´åŠ é€šç”¨ï¼Œ ä¸å¦‚ä½¿ç”¨ dict æ¥ä¼ é€’å‚æ•°
    # TODO é€»è¾‘åº”è¯¥å°ä½åˆ° llavavla.dataloader é‡Œé¢
    # VLA æ•°æ®é›†
    logger.info(f"Creating VLA Dataset with Mixture `{cfg.datasets.vla_data.data_mix}`")
    vla_dataset, collate_fn = build_dataloader( # è¿™ä¸ªå†™åœ¨dataload.py å†…éƒ¨
        cfg=cfg)
    
    # VLA æ•°æ®åŠ è½½å™¨ #  -->  TODO è¿™ä¸ªé€»è¾‘è¦å†™åˆ° build_dataloader å†…éƒ¨
    vla_train_dataloader = DataLoader(
        vla_dataset,
        batch_size=cfg.datasets.vla_data.per_device_batch_size,
        collate_fn=collate_fn,
        num_workers=16,
        # shuffle=True # RLSD ä¸èƒ½åšè¿™ä¸ªäº‹æƒ…
    )
    
    # VLM æ•°æ®åŠ è½½å™¨
    vlm_data_module = make_vlm_dataloader(cfg)
    vlm_train_dataloader = vlm_data_module["train_dataloader"]
    
    # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    if accelerator.is_main_process: # TODO åç»­è¦è€ƒè™‘ç»Ÿä¸€åˆ¤æ–­ rank = 0
        # save_dataset_statistics(vla_dataset.dataset_statistics, output_dir)
        vla_dataset.save_dataset_statistics(output_dir / "dataset_statistics.json")
    
    # æ‹’ç»è‡ªåŠ¨åˆ†å‘ # TODO åº”è¯¥å†™åˆ° accelerator config
    accelerator.dataloader_config.dispatch_batches =  False
    dist.barrier()

    return vla_train_dataloader, vlm_train_dataloader

def setup_optimizer_and_scheduler(
    model, cfg
) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler]:
    """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    param_groups = build_param_lr_groups(model=model, cfg=cfg)
    optimizer = torch.optim.AdamW(
        param_groups,
        # lr=cfg.trainer.learning_rate.base,
        betas=tuple(cfg.trainer.optimizer.betas),
        weight_decay=cfg.trainer.optimizer.weight_decay,
        eps=cfg.trainer.optimizer.eps,
    )
    
    # æ‰“å°ä¼˜åŒ–å™¨ç»„ä¿¡æ¯
    if dist.is_initialized() and dist.get_rank() == 0:
        for i, group in enumerate(optimizer.param_groups):
            logger.info(f"LR Group {group['name']}: lr={group['lr']}, num_params={len(group['params'])}")
    
    # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        name=cfg.trainer.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=cfg.trainer.num_warmup_steps,
        num_training_steps=cfg.trainer.max_train_steps,
        scheduler_specific_kwargs=cfg.trainer.scheduler_specific_kwargs,  # æœ€å°å­¦ä¹ ç‡
    )
    
    # TODO mv to trainer
    # # å‡†å¤‡æ‰€æœ‰ç»„ä»¶
    # (model, optimizer, vla_train_dataloader, vlm_train_dataloader) = accelerator.prepare(
    #     model, optimizer, vla_train_dataloader, vlm_train_dataloader
    # )
    
    return optimizer, lr_scheduler

class VLAMTrainer(TrainerUtils):
    def __init__(self, cfg, model, vla_train_dataloader, vlm_train_dataloader, optimizer, lr_scheduler, accelerator):
        self.config = cfg
        self.model = model
        self.vla_train_dataloader = vla_train_dataloader
        self.vlm_train_dataloader = vlm_train_dataloader
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.accelerator = accelerator
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.completed_steps = 0
        self.total_batch_size = self._calculate_total_batch_size()
        
        
    def prepare_training(self):
        rank = dist.get_rank() if dist.is_initialized() else 0
        seed = self.config.seed + rank if hasattr(self.config, 'seed') else rank + 3047
        set_seed(seed)

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if (hasattr(self.config.trainer, 'pretrained_checkpoint') and self.config.trainer.pretrained_checkpoint):
            pretrained_checkpoint = self.config.trainer.pretrained_checkpoint
            reload_modules = self.config.trainer.reload_modules if hasattr(self.config.trainer, 'reload_modules') else None
            self.model = self.load_pretrained_backbones(self.model, pretrained_checkpoint, reload_modules=reload_modules)
        
        # å†»ç»“å‚æ•°
        freeze_modules = ( # æˆ‘è§‰å¾—å…¨å±€å°±åº”è¯¥åªæœ‰ä¸€ä¸ªconfigï¼Œ ä½¿ç”¨æ²¡å¿…è¦ç›¸å¯¹è·¯å¾„
            self.config.trainer.freeze_modules
            if (self.config and hasattr(self.config.trainer, "freeze_modules"))
            else None
        )
        self.model = self.freeze_backbones(self.model, freeze_modules=freeze_modules) # TODO æ€è€ƒä¸€ä¸‹self.config æ˜¯å…¨å±€ä¼ å‚æ•°ï¼Œ è¿˜æ˜¯ç›¸å¯¹ä¼ å‚æ•°ï¼Ÿ

        #  æ‰“å°æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ï¼š --> TODO ä»–åº”è¯¥æ˜¯è¦æœ€å æ€»ç»“checkçš„ï¼Œ è€ƒè™‘é›†æƒç®¡ç†
        self.print_trainable_parameters(self.model)

        # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç»„ä»¶
        self.model, self.optimizer, self.vla_train_dataloader, self.vlm_train_dataloader = self.setup_distributed_training(
            self.accelerator, # must be the first param
            self.model,
            self.optimizer,
            self.vla_train_dataloader,
            self.vlm_train_dataloader
        )


        self._init_wandb()
        self._init_checkpointing()
    

    def _calculate_total_batch_size(self):
        """è®¡ç®—å…¨å±€æ‰¹é‡å¤§å°"""
        return (
            self.config.datasets.vla_data.per_device_batch_size
            * self.accelerator.num_processes
            * self.accelerator.gradient_accumulation_steps
        )
    
    def _init_wandb(self):
        """åˆå§‹åŒ–Weights & Biases"""
        if self.accelerator.is_main_process:
            wandb.init(
                name=self.config.run_id,
                dir=os.path.join(self.config.output_dir, "wandb"),
                project=self.config.wandb_project,
                entity=self.config.wandb_entity,
                group="vla-train",
            )
    
    def _init_checkpointing(self):
        """åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç›®å½•"""
        self.checkpoint_dir = os.path.join(self.config.output_dir, "checkpoints")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        pretrained_checkpoint = getattr(self.config.trainer, "pretrained_checkpoint", None)
        is_resume = getattr(self.config.trainer, "is_resume", False)

        # æ¢å¤è®­ç»ƒçŠ¶æ€
        # è¦åˆ¤æ–­æ˜¯å¦æœ‰self.config.trainer.pretrained_checkpoint
        if pretrained_checkpoint and is_resume: # TODO è¿™é‡Œè¿˜æ²¡èƒ½å¤Ÿä¿å­˜state, æ€è€ƒæ˜¯å¦å¿…è¦ (state çš„å­˜å‚¨å¤ªå¤§äº†ï¼Œ éœ€è¦å®ç°keep last/best çš„é€»è¾‘ï¼Œ åŒ…æ‹¬ckpt)
            self._load_checkpoint(self.config.resume_from_checkpoint)
    
    def _load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self.accelerator.load_state(checkpoint_path)
        self.accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
        # TODO: æ¢å¤è®­ç»ƒæ­¥æ•°å’Œå…¶ä»–çŠ¶æ€
    
    def _save_checkpoint(self):
        """ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€"""

        if accelerator.is_main_process:
            
            checkpoint_path = os.path.join(self.checkpoint_dir, f"steps_{self.completed_steps}")
            # ä¿å­˜æ¨¡å‹çŠ¶æ€
            state_dict = self.accelerator.get_state_dict(self.model)
            torch.save(state_dict, checkpoint_path + "_pytorch_model.pt")
            
            # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®
            summary_data = {
                "steps": self.completed_steps,
                # TODO: æ·»åŠ å…¶ä»–éœ€è¦ä¿å­˜çš„è®­ç»ƒçŠ¶æ€
            }
            with open(os.path.join(self.config.output_dir, "summary.jsonl"), "a") as f:
                f.write(json.dumps(summary_data) + "\n")
            self.accelerator.print(f"âœ… Checkpoint saved at {checkpoint_path}")
        accelerator.wait_for_everyone()

    def _log_metrics(self, metrics):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        if self.completed_steps % self.config.trainer.logging_frequency == 0: # æœ‰äº›å‚æ•°åº”è¯¥æ˜¯éœ€è¦intial ç»™ class çš„äº†
            if self.accelerator.is_main_process:
                # è®¡ç®—æ¢¯åº¦èŒƒæ•° # TODO check accelerator ä¸‹ä»»ä½•è·å¾— normï¼Ÿ
                # total_norm = 0.0
                # for p in self.model.parameters():
                #     if p.grad is not None:
                #         total_norm += p.grad.data.norm(2).item() ** 2
                # metrics["grad_norm"] = total_norm ** 0.5
                
                # æ·»åŠ å­¦ä¹ ç‡
                metrics["learning_rate"] = self.lr_scheduler.get_last_lr()[0] # TODO æŸ¥çœ‹æ˜¯å¦æ˜¯æœ‰lr group
                
                # æ·»åŠ epochä¿¡æ¯
                metrics["epoch"] = round(self.completed_steps / len(self.vla_train_dataloader), 2)
                
                # è®°å½•åˆ°W&B
                wandb.log(metrics, step=self.completed_steps)
                # è°ƒè¯•è¾“å‡º
                logger.info(f"Step {self.completed_steps}, Loss: {metrics})")
    
    def _create_data_iterators(self):
        """åˆ›å»ºæ•°æ®è¿­ä»£å™¨"""
        self.vla_iter = iter(self.vla_train_dataloader)
        self.vlm_iter = iter(self.vlm_train_dataloader)
    
    def _get_next_batch(self):
        """è·å–ä¸‹ä¸€æ‰¹æ•°æ®ï¼ˆè‡ªåŠ¨å¤„ç†æ•°æ®å¾ªç¯ï¼‰"""
        try:
            batch_vla = next(self.vla_iter)
        except StopIteration:
            # éœ€è¦æ”¹å˜trainer çš„seed --> å…¶å®ä¸è¦å›ºå®šseed å°±ä¸ä¼šæœ‰è¿™äº›é—®é¢˜ # TODO æœªæ¥è¦çœ‹æ€ä¹ˆæ ·è‡ªåŠ¨å¤„ç†è¿™äº›äº‹æƒ…ã€‚
            # å…ˆåˆ¤æ–­æ˜¯å¦æœ‰è¿™ä¸ª self.vla_epoch_count
            if not hasattr(self, 'vla_epoch_count'):
                self.vla_epoch_count = 0
            # TODO éœ€è¦æ£€éªŒæ˜¯å¦ ç”Ÿæ•ˆ
            self.vla_iter, self.vla_epoch_count = TrainerUtils._reset_dataloader(
                self.vla_train_dataloader, self.vla_epoch_count
            )
            batch_vla = next(self.vla_iter)
        
        try:
            batch_vlm = next(self.vlm_iter) # TODO é¦–å°¾å¾ªç¯åº”è¯¥æ˜¯dataset è‡ªå·±çš„åŠŸèƒ½ï¼Œ è¿™é‡Œæ˜¯è€ƒè™‘åˆ°å¾ˆå¤šäººçš„dataset æ˜¯æ²¡æœ‰è¿™ä¸ªåŠŸèƒ½çš„
        except StopIteration: 
            if not hasattr(self, 'vlm_epoch_count'):
                self.vlm_epoch_count = 0
            self.vlm_iter, self.vlm_epoch_count = self._reset_dataloader(
                self.vlm_train_dataloader, self.vlm_epoch_count
            )
            batch_vlm = next(self.vlm_iter)

        return batch_vla, batch_vlm
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒå¾ªç¯"""
        # æ‰“å°è®­ç»ƒé…ç½®
        self._log_training_config()
        
        # å‡†å¤‡æ•°æ®è¿­ä»£å™¨
        self._create_data_iterators()
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(
            range(self.config.trainer.max_train_steps),
            disable=not self.accelerator.is_local_main_process
        )
        
        # ä¸»è®­ç»ƒå¾ªç¯
        while self.completed_steps < self.config.trainer.max_train_steps:
            # è·å–æ•°æ®æ‰¹æ¬¡
            batch_vla, batch_vlm = self._get_next_batch()
            
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            step_metrics = self._train_step(batch_vla, batch_vlm)
            
            # æ›´æ–°è¿›åº¦
            if self.accelerator.sync_gradients:
                progress_bar.update(1)
                self.completed_steps += 1
            
            # è¯„ä¼°æ¨¡å‹
            step_metrics = self.eval_action_model(step_metrics)

            # è®°å½•æŒ‡æ ‡
            self._log_metrics(step_metrics)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.completed_steps % self.config.trainer.save_interval == 0 and self.completed_steps > 0:
                self._save_checkpoint()

                dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥, é¿å… timeout
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if self.completed_steps >= self.config.trainer.max_train_steps:
                break
        
        # è®­ç»ƒç»“æŸå¤„ç†
        self._finalize_training()
    
        # æ‰§è¡Œè¯„ä¼°æ­¥éª¤
    def eval_action_model(self, step_metrics:dict = None) -> float:
        """
        Evaluate the model on the given dataset using the specified metric function.

        :param eval_dataset: List of evaluation samples, each containing 'image', 'instruction', and 'action'.
        :param metric_fn: Function to compute the distance between predicted and ground truth actions.
        :return: Average metric score across the evaluation dataset.
        """
        
        if self.accelerator.is_main_process and self.completed_steps % self.config.trainer.eval_interval == 0:
            
            examples, vlm_data = self._get_next_batch()
            
            score = 0.0 # æƒ³åŠæ³•çœ‹çœ‹è¯æ˜å˜æˆbatch æ¨ç†
            num_samples = len(examples)

            # @Jinhui TBD TODO 
            images = [example["image"] for example in examples]  #  TODO check æ˜¯ä»€ä¹ˆ
            instructions = [example["lang"] for example in examples]  # [B, str]
            actions = [example["action"] for example in examples] #label

            # Predict actions using the model
            predicted_solutions, normalized_actions = self.model.predict_action_withCoT( # TODO è¿™é‡Œæœ‰ æ¨¡å‹æ–¹æ³• ä¾èµ–å…³ç³», å¦‚æœä½ è¦ä¿æŒtrainerçš„ç‹¬ç«‹æ€§ï¼Œè¿™é‡Œåº”è¯¥æ€ä¹ˆè®¾è®¡ï¼Ÿ
                images=images,
                instructions=instructions,
                use_ddim=True,
                num_ddim_steps=20)
            


            # æå‰è½¬æ¢ actions ä¸º numpy.ndarray
            actions = np.array(actions)  # å°† actions è½¬æ¢ä¸º numpy.ndarray
            # B, Chunk, dim = actions.shape
            num_pots = np.prod(actions.shape)
            # Compute the metric score
            score = TrainerUtils.euclidean_distance(normalized_actions, actions)
            average_score = score / num_pots
            step_metrics["mse_score"] = average_score

        # dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥ TODO çœ‹çœ‹æ˜¯å¦éœ€è¦è®©å…¶ä»–è¿›ç¨‹ç­‰
        return step_metrics


    def _log_training_config(self):
        """è®°å½•è®­ç»ƒé…ç½®"""
        if self.accelerator.is_main_process:
            logger.info("***** Training Configuration *****")
            logger.info(f"  Total optimization steps = {self.config.trainer.max_train_steps}")
            logger.info(f" Per device batch size = {self.config.datasets.vla_data.per_device_batch_size}")
            logger.info(f"  Gradient accumulation steps = {self.config.trainer.gradient_accumulation_steps}")
            logger.info(f"  Total batch size = {self.total_batch_size}")

        # TODO è¿™é‡Œåº”è¯¥æ‰“å°å…¨éƒ¨ è®­ç»ƒä¸­å…³é”®çš„ä¿¡æ¯ï¼š model size, freezeï¼Œ lr group and so on.
    
    def _train_step(self, batch_vla, batch_vlm):
        """æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤"""
        log_dict = {}
        # TODO: å®ç°æ¢¯åº¦ç´¯ç§¯ @Yioutpi
        with self.accelerator.accumulate(self.model):
            self.optimizer.zero_grad()
            
            # VLAä»»åŠ¡å‰å‘ä¼ æ’­
            with torch.autocast("cuda", dtype=torch.bfloat16):
                action_loss, action_vlm_loss = self.model.forward(batch_vla)
                total_loss = action_loss + action_vlm_loss #@DEBUG
            self.accelerator.backward(total_loss)
            
            # VLMä»»åŠ¡å‰å‘ä¼ æ’­
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                vlm_output = self.model.qwen_vl_interface(**batch_vlm)
                vlm_loss = vlm_output.loss * self.config.trainer.loss_scale.vlm
            
            vis_grad_angle = 0 # @Jinhui @DEBUG
            if self.accelerator.is_main_process and vis_grad_angle:
                """æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤ï¼Œå†…ç½® PCGrad å’Œæ¢¯åº¦å¤¹è§’ç»Ÿè®¡"""
                # æ‹¿åˆ°æ‰€æœ‰ qwen_vl_interface çš„å‚æ•°åˆ—è¡¨
                # interface_params = list(self.model.qwen_vl_interface.model.model.visual.patch_embed.parameters())
                interface_params = list(self.model.qwen_vl_interface.model.model.language_model.layers[-1].mlp.down_proj.parameters())
                # interface_params = list(self.model.qwen_vl_interface.model.model.language_model.layers[0].mlp.down_proj.parameters())
                # interface_params = list(self.model.qwen_vl_interface.model.model.language_model.layers[-1].self_attn.v_proj.parameters())
                # interface_params = list(self.model.qwen_vl_interface.model.model.language_model.layers[0].self_attn.v_proj.parameters())
                
                # 1) å…ˆåˆ†åˆ«ç”¨ torch.autograd.grad å¾—åˆ° grads_action, grads_vlm
                grads_action = torch.autograd.grad(action_loss, interface_params, retain_graph=True)
                # grads_vlm    = torch.autograd.grad(action_vlm_loss,    interface_params, retain_graph=True)
                grads_vlm    = torch.autograd.grad(vlm_loss,    interface_params, retain_graph=True)

                # 2) ç»Ÿè®¡å¤¹è§’
                mean_angle_deg, angle_variance = TrainerUtils.compute_grad_angle_with_stats(grads_action, grads_vlm)
                log_dict["vl_action_grad_angle"] = mean_angle_deg
                log_dict["angle_variance"] = angle_variance
                # 3) PCGrad æŠ•å½±
                # grads_vlm = TrainerUtils.pcgrad_project(grads_action, grads_vlm)
            # VLAåå‘ä¼ æ’­
            # self.accelerator.backward(total_loss)
            # # VLMåå‘ä¼ æ’­ @DEBUG
            self.accelerator.backward(vlm_loss)
            
            pass

            # dist.barrier() #@DEBUG
            # æ¢¯åº¦è£å‰ª
            if self.config.trainer.gradient_clipping is not None:
                self.accelerator.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.trainer.gradient_clipping
                )
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.optimizer.step()
            self.lr_scheduler.step()

            log_dict.update({
            "action_dit_loss": action_loss.item(),
            "action_vlm_loss": action_vlm_loss.item(),
            "vlm_loss": vlm_loss.item(),
            })
        return log_dict
    
    def _finalize_training(self):
        """è®­ç»ƒç»“æŸå¤„ç†"""
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if self.accelerator.is_main_process:
            final_checkpoint = os.path.join(self.config.output_dir, "final_model")
            os.makedirs(final_checkpoint, exist_ok=True)
            state_dict = self.accelerator.get_state_dict(self.model)
            torch.save(state_dict, os.path.join(final_checkpoint, "pytorch_model.pt"))
            logger.info(f"Training complete. Final model saved at {final_checkpoint}")
        
        # å…³é—­W&B
        if self.accelerator.is_main_process:
            wandb.finish()
        
        self.accelerator.wait_for_everyone()

from llavavla.training.metrics import build_param_lr_groups
def main(cfg) -> None:
    logger.info("VLA Training :: Warming Up")

    # åˆ›å»ºè¾“å‡ºç›®å½•å¹¶ä¿å­˜é…ç½®
    output_dir = setup_directories(cfg=cfg)
    
    # æ„å»ºæ¨¡å‹
    vla = build_framework(cfg)
    # å‡†å¤‡æ•°æ®
    vla_train_dataloader, vlm_train_dataloader = prepare_data(cfg=cfg, accelerator=accelerator, output_dir=output_dir)
    # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, lr_scheduler = setup_optimizer_and_scheduler(model=vla, cfg=cfg)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    # Run VLA Training
    trainer = VLAMTrainer(
        cfg=cfg,
        model=vla,
        vla_train_dataloader=vla_train_dataloader,
        vlm_train_dataloader=vlm_train_dataloader,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler,
        accelerator=accelerator
    )
    
    # æ‰§è¡Œè®­ç»ƒå‰çš„å‡†å¤‡
    trainer.prepare_training()
    # æ‰§è¡Œè®­ç»ƒ
    trainer.train()

    # And... we're done!
    logger.info("... and that's all, folks!")
    dist.barrier()
    dist.destroy_process_group()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config_yaml", type=str, default="llavavla/conf/qwenact.yaml", help="Path to YAML config")
    args, clipargs = parser.parse_known_args()

    # Load YAML config & Convert CLI overrides to dotlist config
    cfg = OmegaConf.load(args.config_yaml)
    dotlist = normalize_dotlist_args(clipargs)  # Normalize CLI args to dotlist format
    cli_cfg = OmegaConf.from_dotlist(dotlist)
    cfg = OmegaConf.merge(cfg, cli_cfg)

    # if cfg.is_debug:
    if cfg.is_debug and dist.is_initialized() and dist.get_rank() == 0:
        import debugpy
        debugpy.listen(("0.0.0.0", 10092))
        print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
        debugpy.wait_for_client()

    main(cfg)
