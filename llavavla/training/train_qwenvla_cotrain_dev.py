"""
train.py
"""

import json
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Tuple, Union

import draccus
import torch
import torch.distributed as dist

import yaml
import wandb

from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from transformers import AutoProcessor
from transformers import get_scheduler

from tqdm import tqdm
import wandb
from torch.utils.data import Dataset, DataLoader
from typing import Optional
import argparse
from omegaconf import OmegaConf
from hydra import initialize

from llavavla.training.metrics import normalize_dotlist_args

from prismatic.overwatch import initialize_overwatch
# from prismatic.vla import get_vla_dataset_and_collator
from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics


from llavavla.dataloader.vlm_datasets import make_vlm_dataloader

from llavavla.dataloader.rlds_datasets import get_vla_dataset, collate_fn# TODO è¦ç§»åŠ¨åˆ°dataloader ä¸‹é¢
from accelerate import Accelerator, DeepSpeedPlugin

deepspeed_plugin = DeepSpeedPlugin()# è¿™ä¸ªæ’ä»¶æ˜¯å¦èƒ½ä½¿ç”¨åˆ° config çš„å‚æ•°å‘¢ï¼Ÿ å…¶å®è¿™é‡Œåº”è¯¥æ˜¯å¯ä»¥é£æ˜¾ç¤ºç”¨çš„ï¼Œ æ„Ÿè§‰æœ‰ç‰ˆæœ¬é—®é¢˜ #zero_stage=2, gradient_accumulation_steps=1 ï¼šv2: hf_ds_config="scripts/run_scripts/ds_config.yaml"
accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin)
accelerator.print(accelerator.state) # TODO ä¹‹åè¦ç§»åŠ¨åˆ°trainer å†…éƒ¨ï¼Œ --> ç›´æ¥æ¬LLaVA trainer

# Sane Defaults
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__) # åæœŸç§»é™¤ï¼Œ ä¸è¦åŸºäº prismatic æ¥ç©è¾“å‡º
logger = get_logger(__name__)


from llavavla.model.framework.qwenpi_dev import build_model_framework

def load_fast_tokenizer():
    fast_tokenizer = AutoProcessor.from_pretrained(
        "physical-intelligence/fast", trust_remote_code=True
    )
    return fast_tokenizer


class VLAMTrainer:
    def __init__(self, cfg, model, vla_train_dataloader, vlm_train_dataloader, optimizer, lr_scheduler, accelerator):
        self.config = cfg
        self.model = model
        self.vla_train_dataloader = vla_train_dataloader
        self.vlm_train_dataloader = vlm_train_dataloader
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.accelerator = accelerator
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.completed_steps = 0
        self.total_batch_size = self._calculate_total_batch_size()
        
        # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
        self._init_wandb()
        self._init_checkpointing()
    
    def _calculate_total_batch_size(self):
        """è®¡ç®—å…¨å±€æ‰¹é‡å¤§å°"""
        return (
            self.config.datasets.vla_data.per_device_batch_size
            * self.accelerator.num_processes
            * self.accelerator.gradient_accumulation_steps
        )
    
    def _init_wandb(self):
        """åˆå§‹åŒ–Weights & Biases"""
        if self.accelerator.is_main_process:
            wandb.init(
                name=self.config.run_id,
                dir=os.path.join(self.config.output_dir, "wandb"),
                project=self.config.wandb_project,
                entity=self.config.wandb_entity,
                group="vla-train",
            )
    
    def _init_checkpointing(self):
        """åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç›®å½•"""
        self.checkpoint_dir = os.path.join(self.config.output_dir, "checkpoints")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        pretrained_checkpoint = getattr(self.config.trainer, "pretrained_checkpoint", None)
        is_resume = getattr(self.config.trainer, "is_resume", False)

        # æ¢å¤è®­ç»ƒçŠ¶æ€
        # è¦åˆ¤æ–­æ˜¯å¦æœ‰self.config.trainer.pretrained_checkpoint
        if pretrained_checkpoint and is_resume: # TODO è¿™é‡Œè¿˜æ²¡èƒ½å¤Ÿä¿å­˜state, æ€è€ƒæ˜¯å¦å¿…è¦
            self._load_checkpoint(self.config.resume_from_checkpoint)
    
    def _load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self.accelerator.load_state(checkpoint_path)
        self.accelerator.print(f"Resumed from checkpoint: {checkpoint_path}")
        # TODO: æ¢å¤è®­ç»ƒæ­¥æ•°å’Œå…¶ä»–çŠ¶æ€
    
    def _save_checkpoint(self):
        """ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€"""

        if accelerator.is_main_process:
            
            checkpoint_path = os.path.join(self.checkpoint_dir, f"steps_{self.completed_steps}")
            # ä¿å­˜æ¨¡å‹çŠ¶æ€
            state_dict = self.accelerator.get_state_dict(self.model)
            torch.save(state_dict, checkpoint_path + "_pytorch_model.pt")
            
            # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®
            summary_data = {
                "steps": self.completed_steps,
                # TODO: æ·»åŠ å…¶ä»–éœ€è¦ä¿å­˜çš„è®­ç»ƒçŠ¶æ€
            }
            with open(os.path.join(self.config.output_dir, "summary.jsonl"), "a") as f:
                f.write(json.dumps(summary_data) + "\n")
            
        self.accelerator.print(f"âœ… Checkpoint saved at {checkpoint_path}")
        accelerator.wait_for_everyone()

    def _log_metrics(self, metrics):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        if self.completed_steps % self.config.trainer.logging_frequency == 0: # æœ‰äº›å‚æ•°åº”è¯¥æ˜¯éœ€è¦intial ç»™ class çš„äº†
            if self.accelerator.is_main_process:
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                total_norm = 0.0
                for p in self.model.parameters():
                    if p.grad is not None:
                        total_norm += p.grad.data.norm(2).item() ** 2
                metrics["grad_norm"] = total_norm ** 0.5
                
                # æ·»åŠ å­¦ä¹ ç‡
                metrics["learning_rate"] = self.lr_scheduler.get_last_lr()[0]
                
                # æ·»åŠ epochä¿¡æ¯
                metrics["epoch"] = self.completed_steps // len(self.vla_train_dataloader)
                
                # è®°å½•åˆ°W&B
                wandb.log(metrics, step=self.completed_steps)
                
                # è°ƒè¯•è¾“å‡º
                if self.config.is_debug:
                    print(f"Step {self.completed_steps}: {metrics}")
    
    def _create_data_iterators(self):
        """åˆ›å»ºæ•°æ®è¿­ä»£å™¨"""
        self.vla_iter = iter(self.vla_train_dataloader)
        self.vlm_iter = iter(self.vlm_train_dataloader)
    
    def _get_next_batch(self):
        """è·å–ä¸‹ä¸€æ‰¹æ•°æ®ï¼ˆè‡ªåŠ¨å¤„ç†æ•°æ®å¾ªç¯ï¼‰"""
        try:
            batch_vla = next(self.vla_iter)
        except StopIteration:
            self.vla_iter = iter(self.vla_train_dataloader)
            batch_vla = next(self.vla_iter)
        
        try:
            batch_vlm = next(self.vlm_iter) # TODO é¦–å°¾å¾ªç¯åº”è¯¥æ˜¯dataset è‡ªå·±çš„åŠŸèƒ½ï¼Œ è¿™é‡Œæ˜¯è€ƒè™‘åˆ°å¾ˆå¤šäººçš„dataset æ˜¯æ²¡æœ‰è¿™ä¸ªåŠŸèƒ½çš„
        except StopIteration:
            self.vlm_iter = iter(self.vlm_train_dataloader)
            batch_vlm = next(self.vlm_iter)
        
        return batch_vla, batch_vlm
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒå¾ªç¯"""
        # æ‰“å°è®­ç»ƒé…ç½®
        self._log_training_config()
        
        # å‡†å¤‡æ•°æ®è¿­ä»£å™¨
        self._create_data_iterators()
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(
            range(self.config.trainer.max_train_steps),
            disable=not self.accelerator.is_local_main_process
        )
        
        # ä¸»è®­ç»ƒå¾ªç¯
        while self.completed_steps < self.config.trainer.max_train_steps:
            # è·å–æ•°æ®æ‰¹æ¬¡
            batch_vla, batch_vlm = self._get_next_batch()
            
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            step_metrics = self._train_step(batch_vla, batch_vlm)
            
            # æ›´æ–°è¿›åº¦
            if self.accelerator.sync_gradients:
                progress_bar.update(1)
                self.completed_steps += 1
            
            # è®°å½•æŒ‡æ ‡
            self._log_metrics(step_metrics)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.completed_steps % self.config.trainer.save_interval == 0 and self.completed_steps > 0:
                self._save_checkpoint()
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if self.completed_steps >= self.config.trainer.max_train_steps:
                break
        
        # è®­ç»ƒç»“æŸå¤„ç†
        self._finalize_training()
    
    def _log_training_config(self):
        """è®°å½•è®­ç»ƒé…ç½®"""
        if self.accelerator.is_main_process:
            logger.info("***** Training Configuration *****")
            logger.info(f"  Total optimization steps = {self.config.trainer.max_train_steps}")
            logger.info(f" Per device batch size = {self.config.datasets.vla_data.per_device_batch_size}")
            logger.info(f"  Gradient accumulation steps = {self.config.trainer.gradient_accumulation_steps}")
            logger.info(f"  Total batch size = {self.total_batch_size}")

    
    def _train_step(self, batch_vla, batch_vlm):
        """æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤"""
        # TODO: å®ç°æ¢¯åº¦ç´¯ç§¯
        with self.accelerator.accumulate(self.model):
            self.optimizer.zero_grad()
            
            # VLAä»»åŠ¡å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                action_loss, action_vlm_loss = self.model.forward(batch_vla)
                total_loss = action_loss + action_vlm_loss
            
            # VLAåå‘ä¼ æ’­
            self.accelerator.backward(total_loss)
            
            # VLMä»»åŠ¡å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                vlm_output = self.model.qwen_vl_interface(**batch_vlm)
                vlm_loss = vlm_output.loss * self.config.trainer.loss_scale.vlm
            
            # VLMåå‘ä¼ æ’­
            self.accelerator.backward(vlm_loss)
            
            # æ¢¯åº¦è£å‰ª
            if self.config.trainer.gradient_clipping is not None:
                self.accelerator.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.trainer.gradient_clipping
                )
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.optimizer.step()
            self.lr_scheduler.step()
        
        return {
            "action_dit_loss": action_loss.item(),
            "action_vlm_loss": action_vlm_loss.item(),
            "vlm_loss": vlm_loss.item(),
        }
    
    def _finalize_training(self):
        """è®­ç»ƒç»“æŸå¤„ç†"""
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if self.accelerator.is_main_process:
            final_checkpoint = os.path.join(self.config.output_dir, "final_model")
            os.makedirs(final_checkpoint, exist_ok=True)
            state_dict = self.accelerator.get_state_dict(self.model)
            torch.save(state_dict, os.path.join(final_checkpoint, "pytorch_model.pt"))
            logger.info(f"Training complete. Final model saved at {final_checkpoint}")
        
        # å…³é—­W&B
        if self.accelerator.is_main_process:
            wandb.finish()
        
        self.accelerator.wait_for_everyone()

from llavavla.training.metrics import build_param_lr_groups
def train(cfg) -> None:
    overwatch.info("VLA Training :: Warming Up")
    # accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)

    # Start =>> Build Directories and Set Randomness
    overwatch.info('"Do or do not; there is no try."', ctx_level=1)
    # dist.barrier()  # Ensure all processes are synchronized before starting training
    cfg.output_dir = os.path.join(cfg.run_root_dir, cfg.run_id)
    output_dir = Path(cfg.output_dir)
    # Save Configuration =>> additionally save a JSON version for later HF Integration
    if overwatch.is_rank_zero():
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(output_dir / "checkpoints", exist_ok=True)

        # Save as YAML using OmegaConf
        OmegaConf.save(cfg, output_dir / "config.yaml")
        # Additionally save as JSON TODO ä¹‹åè¦å°† .model çš„å‚æ•°å•ç‹¬save json
        with open(output_dir / "config.yaml", "r") as f_yaml, open(output_dir / "config.json", "w") as f_json:
            yaml_cfg = yaml.safe_load(f_yaml)
            json.dump(yaml_cfg, f_json, indent=2)
    
    dist.barrier()
    # Load VLA checkpoint (if resuming from training) or Base VLM otherwise (from `cfg.vla.base_vlm` ID or Path)
    #   =>> Note :: Verifies that all parameters are loaded in FP32 on load!

    overwatch.info(f"Loading Base VLM `{cfg.vla.base_vlm}` from ID/Path")
    vla = build_model_framework(cfg)
    # fast_tokenizer = load_fast_tokenizer() # TODO è€ƒè™‘æ¶æ„æ—¶å€™çš„äº‹æƒ…
    # processor = vla.vlm.processor # @Jinhui TODO ä¸åº”è¯¥åœ¨è¿™ä¸ªåœ°æ–¹ èµ‹å€¼ï¼Œ æ•°æ®å‡†å¤‡åº”è¯¥å’Œ å°è£…ç±»ç»‘å®šä¸ºå‡½æ•°
    # [Validate] Model should be in Full Precision! @Jinhui TODO Why?
    # for param in vla.parameters():
    #     if param.dtype != torch.float32: #@Jinhui TODO Check, why?
    #         param.data = param.data.float()
    #     assert param.dtype == torch.float32, f"Loaded VLM parameter not in full precision: {param}"


    # [Explicit] Call to `freeze_backbones` here for clarity =>> will log exactly what is/is not frozen
    
    vla.freeze_backbones() # TODO åº”è¯¥æ˜¯trainer è¦åšçš„äº‹æƒ…

    # Print number of total/trainable model parameters # TODO åº”è¯¥é›†æˆåˆ°trainer ä¸­
    num_params = sum(p.numel() for p in vla.parameters())
    num_trainable_params = sum(p.numel() for p in vla.parameters() if p.requires_grad)
    overwatch.info(
        f"# Parameters (in millions): {num_params / 10**6:.3f} Total, {num_trainable_params / 10**6:.3f} Trainable"
    )


    overwatch.info(f"Creating VLA Open-X Dataset with Mixture `{cfg.datasets.keys()}`")
    #   text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    vla_dataset = get_vla_dataset( # æ‹’ç»ä»»ä½•å†…éƒ¨è½¬æ¢
        cfg.datasets.vla_data.data_root_dir, # å¤ªå¤šå‚æ•°äº†ï¼Œ åº”è¯¥config ç©¿è¶Šè¿‡å»ï¼Œ æˆ–è€…æ˜¯ ** çš„æ–¹å¼
        cfg.datasets.vla_data.data_mix,
        default_image_resolution=tuple(cfg.datasets.vla_data.default_image_resolution),
        shuffle_buffer_size=cfg.datasets.vla_data.shuffle_buffer_size,
        image_aug=cfg.datasets.vla_data.image_aug,
        future_action_window_size=cfg.framework.action_model.future_action_window_size,
        past_action_window_size=cfg.framework.action_model.past_action_window_size,
        load_all_data_for_training=cfg.datasets.vla_data.load_all_data_for_training,
    )

    # Create DataLoader
    
    vla_train_dataloader = DataLoader(
        vla_dataset,
        batch_size=cfg.datasets.vla_data.per_device_batch_size, # @Jinhui TODO æ„Ÿè§‰å³ä½¿æœ‰ä¸ªç©ºçš„ collate_fn ä¹Ÿä¼šè®©ä»£ç  æ‰©å±•æ€§ æ›´å¥½
        collate_fn=collate_fn
    )

    vlm_data_mudule = make_vlm_dataloader(cfg) # TODO ğŸ‘†æ„å»ºdataloader çš„é€»è¾‘ä¹Ÿä¸èƒ½æ”¾åˆ°è¿™é‡Œã€‚ æ€è€ƒä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆ SFTTrainer éœ€è¦è¿™æ ·å†™
    vlm_train_dataloader = vlm_data_mudule["train_dataloader"]
    # sample = next(iter(vla_dataset)) #for debug

    # Save dataset statistics for de-normalization at inference time
    if overwatch.is_rank_zero():
        save_dataset_statistics(vla_dataset.dataset_statistics, output_dir)
    
    # Create Train Strategy
    dist.barrier()
    accelerator.dataloader_config.dispatch_batches =  False # TODO æ˜¯ä¸æ˜¯å¯ä»¥å†™åˆ° config å†…éƒ¨ï¼Ÿ
    # Initialize optimizer

    param_groups = build_param_lr_groups(vla=vla, cfg=cfg) # TODO è¿™é‡Œçš„å‚æ•°åº”è¯¥æ˜¯ä» config ä¸­è·å–çš„ï¼Œ è€Œä¸æ˜¯ç›´æ¥å†™æ­»
    optimizer = torch.optim.AdamW(
        param_groups,
        lr=cfg.trainer.learning_rate.base,
        betas=tuple(cfg.trainer.optimizer.betas), # è¿™æ˜¯ç”¨äº ä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡ä¼°è®¡ çš„ä¸¤ä¸ªè¶…å‚æ•°ï¼š
        weight_decay=1e-8, # è¿™æ˜¯ç”¨äº L2 æ­£åˆ™åŒ– çš„é¡¹ï¼ˆæƒ©ç½šå‚æ•°å€¼å¤ªå¤§çš„è¶‹åŠ¿ï¼‰ï¼š
        eps=1e-8,
    )
    pass
    dist.barrier()
    if overwatch.is_rank_zero(): # æƒ³åŠæ³•å†™æˆä¸€ä¸ªä¿®é¥°å‡½æ•°
        for i, group in enumerate(optimizer.param_groups):
            print(f"LR Group {group['name']}: lr={group['lr']}, num_params={len(group['params'])}")
    # Initialize learning rate scheduler
    
    num_warmup_steps = min(int(cfg.trainer.max_train_steps*cfg.trainer.warmup_ratio), cfg.trainer.max_warmup_steps)
    cfg.trainer.num_warmup_steps = num_warmup_steps

    lr_scheduler = get_scheduler(
        name=cfg.trainer.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=cfg.trainer.num_warmup_steps,
        num_training_steps=cfg.trainer.max_train_steps
    )

    # Prepare everything with Accelerator, setup
    vla, optimizer, vla_train_dataloader, vlm_train_dataloader = accelerator.prepare( # @JinhuiYE ç¬¬ä¸‰æ–¹å·¥å…· or DDPï¼Ÿ
        vla, optimizer, vla_train_dataloader, vlm_train_dataloader
    )
    

    # Create Metrics =>> Handles on the fly tracking, logging to specified trackers (e.g., JSONL, Weights & Biases)
    overwatch.info(f"Creating Metrics with Active Trackers => `{cfg.trackers}`")

    # Run VLA Training # TODO move them to class tainer 
    # åˆ›å»ºTrainerå®ä¾‹
    trainer = VLAMTrainer(
        cfg=cfg,
        model=vla,
        vla_train_dataloader=vla_train_dataloader,
        vlm_train_dataloader=vlm_train_dataloader,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler,
        accelerator=accelerator
    )
    
    # æ‰§è¡Œè®­ç»ƒ
    trainer.train()

    # And... we're done!
    overwatch.info("... and that's all, folks!")
    dist.barrier()
    dist.destroy_process_group()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config_yaml", type=str, default="llavavla/conf/qwenact.yaml", help="Path to YAML config")
    args, clipargs = parser.parse_known_args()

    # Load YAML config & Convert CLI overrides to dotlist config
    cfg = OmegaConf.load(args.config_yaml)
    dotlist = normalize_dotlist_args(clipargs)  # Normalize CLI args to dotlist format
    cli_cfg = OmegaConf.from_dotlist(dotlist)
    cfg = OmegaConf.merge(cfg, cli_cfg)

    # if cfg.is_debug:
    if cfg.is_debug and overwatch.is_rank_zero():
        import debugpy
        debugpy.listen(("0.0.0.0", 10092))
        print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
        debugpy.wait_for_client()

    train(cfg)
