"""
datasets.py

Lightweight PyTorch Dataset Definition for wrapping RLDS TFDS Pipeline; just defines transform from RLDS default
format to OpenVLA, IterableDataset shim.
"""
import time

import os, json, pickle, bisect, logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Tuple, Type, List, Optional
from omegaconf import OmegaConf
import lmdb
from itertools import accumulate
import cv2
import numpy as np
import torch
from PIL import Image
from torch.utils.data import Dataset
from llavavla.dataloader.lmdb.data_utils import get_lmdb_dataset_statistics, save_dataset_statistics, NormalizationType

from llavavla.dataloader.lmdb.grounding_func import *

# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100

logger = logging.getLogger(__name__)


@dataclass
class LMDBBatchTransform:
    def __call__(self, lmdb_batch: Dict[str, Any]) -> Dict[str, Any]:
        """Convert a LMDB batch to the format expected by the OpenVLA collator/models."""
        dataset_name, action = lmdb_batch["dataset_name"], lmdb_batch["action"][0]

        # For future action predictions
        if lmdb_batch["action"].shape[0] > 1:
            dataset_name, action = lmdb_batch["dataset_name"], lmdb_batch["action"]
        else:
            dataset_name, action = lmdb_batch["dataset_name"], lmdb_batch["action"][0]

        # img.shape in lmdb_batch = 480,640, 3 = h,w,c, RGB
        img = Image.fromarray(lmdb_batch["observation"]["image_primary"][0]) # B è¦è¢«å»æ‰ï¼Ÿ
        
        # img = torch.tensor(img, dtype=torch.float32)  # TODO Check è¿™é‡Œè¦çœ‹æ˜¯å¦æ‰§è¡Œäº†æ•°æ®å¢å¼º h,w,c
        lang = lmdb_batch["task"]["language_instruction"].decode().lower() #+ "ğŸ”" #cognition token

        return dict(action=action,image=[img],lang=lang, dataset_name=dataset_name)


class LMDBDataset(Dataset):
    def __init__(
        self,
        dataset_name: str,
        data_dir: str,
        dataset_info_name: str = None,
        obs_type: str = "obs_camera",
        action_type: str = "abs_qpos",
        window_size: int = 16,
        image_aug: bool = False,
        batch_transform: LMDBBatchTransform = None,
        normalization_type: NormalizationType = NormalizationType.BOUNDS_Q99,
        save_statistics_dir: str = None,
        config: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        """Dataset wrapper for LMDB format data."""
        self.dataset_name = dataset_name
        self.data_dir = data_dir
        self.dataset_info_name = dataset_info_name if dataset_info_name is not None else dataset_name
        self.dataset_path = f'{data_dir}/{dataset_name}/render'
        self.obs_type = obs_type
        self.action_type = action_type
        self.window_size = window_size
        self.image_aug = image_aug
        self.normalization_type = normalization_type
        self.config = config
        # Load dataset info
        # æ£€æŸ¥ JSON æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        json_file_path = os.path.join(data_dir, "data_info", self.dataset_info_name + ".json")
        logger.info(f"loading dataset at {data_dir}/{dataset_name}")
        assert os.path.exists(json_file_path)
        with open(json_file_path, 'r') as f:
            self.episode_info_list = json.load(f)
            self.episode_list = [f[0] for f in self.episode_info_list]
            self.num_step_per_episode = [f[1] - self.window_size for f in self.episode_info_list]
            self.num_episode = len(self.episode_list)

        # Get dataset statistics (with caching)
        self.dataset_statistics = get_lmdb_dataset_statistics(
            dataset_name=self.dataset_name,
            data_dir=self.data_dir,
            action_type=self.action_type,
            dataset_info_name=self.dataset_info_name,
            save_dir=save_statistics_dir,
        )

        # Load action statistics
        meta_info = pickle.load(open(f"{data_dir}/data_info/{self.dataset_info_name}.pkl", "rb"))
        try:
            if self.action_type == "abs_qpos":
                self.arm_action_mean = np.array(meta_info["abs_arm_action_mean"])
                self.arm_action_std = np.array(meta_info["abs_arm_action_std"])
                self.arm_action_min = np.array(meta_info["abs_arm_action_min"])
                self.arm_action_max = np.array(meta_info["abs_arm_action_max"])
            elif self.action_type == "delta_qpos":
                self.arm_action_mean = np.array(meta_info["delta_arm_action_mean"])
                self.arm_action_std = np.array(meta_info["delta_arm_action_std"])
                self.arm_action_min = np.array(meta_info["delta_arm_action_min"])
                self.arm_action_max = np.array(meta_info["delta_arm_action_max"])
            elif self.action_type == "abs_ee_pose":
                self.arm_action_mean = np.array(meta_info["abs_eepose_action_mean"])
                self.arm_action_std = np.array(meta_info["abs_eepose_action_std"])
                self.arm_action_min = np.array(meta_info["abs_eepose_action_min"])
                self.arm_action_max = np.array(meta_info["abs_eepose_action_max"])
            elif self.action_type == "delta_ee_pose":
                self.arm_action_mean = np.array(meta_info["delta_eepose_action_mean"])
                self.arm_action_std = np.array(meta_info["delta_eepose_action_std"])
                self.arm_action_min = np.array(meta_info["delta_eepose_action_min"])
                self.arm_action_max = np.array(meta_info["delta_eepose_action_max"])
            else:
                raise NotImplementedError(f"Action type {self.action_type} not supported")
        except Exception as e:
            logger.error(f"Error loading action statistics: {e}")
            raise e

        self.accumulated_num_step = list(accumulate(self.num_step_per_episode))
        self.length = self.accumulated_num_step[-1]

    def __len__(self) -> int:
        return self.length

    def __getitem(self, idx: int) -> Dict[str, Any]: # TODO è¿™ä¸ªå¤„ç†å¤ªå¤æ‚äº†ï¼Œä½†æ˜¯ä¹‹ååº”è¯¥æ˜¯ç”¨lerobot æ‰€ä»¥æ²¡å¿…è¦æƒ³å¤ªå¤šåœ¨è¿™ä¸ªä¸œè¥¿ä¸Š
        """Get sequence from dataset at given index."""
        episode_id = bisect.bisect_right(self.accumulated_num_step, idx)
        if episode_id - 1 >= 0:
            start_id = idx - self.accumulated_num_step[episode_id - 1]
        else:
            start_id = idx
        # episode_id=  7855, idx = 1264927; idx: 2224297 seed å¯ä»¥å½±å“ï¼Œ ä¸”å¯ä»¥å›ºå®š; # idx:839737 åˆ†å¸ƒå¼å¯ä»¥å½±å“ï¼Œ len(self)=2576621ï¼› 
        episode_name = self.episode_list[episode_id] # id=0, '2025-06-29_08_30_56_142693' 

        # Open LMDB environment
        lmdb_env = lmdb.open(
            f"{self.dataset_path}/{episode_name}/lmdb",
            readonly=True,
            lock=False,
            readahead=True,
            meminit=True
        )

        # Load meta info
        meta_info = pickle.load(open(f"{self.dataset_path}/{episode_name}/meta_info.pkl", "rb"))

        # Get data keys
        if self.action_type == "abs_qpos":
            arm_index = meta_info["keys"]["scalar_data"].index(b'arm_action')
            arm_key = meta_info["keys"]["scalar_data"][arm_index]
            state_index = meta_info["keys"]["scalar_data"].index(b'observation/robot/qpos')
            state_key = meta_info["keys"]["scalar_data"][state_index]
        elif self.action_type == "delta_qpos":
            arm_index = meta_info["keys"]["scalar_data"].index(b'delta_arm_action')
            arm_key = meta_info["keys"]["scalar_data"][arm_index]
            state_index = meta_info["keys"]["scalar_data"].index(b'observation/robot/qpos')
            state_key = meta_info["keys"]["scalar_data"][state_index]
        elif self.action_type == "abs_ee_pose":
            arm_index = meta_info["keys"]["scalar_data"].index(b'ee_pose_action')
            arm_key = meta_info["keys"]["scalar_data"][arm_index]
            state_index = meta_info["keys"]["scalar_data"].index(b'observation/robot/ee_pose_state')
            state_key = meta_info["keys"]["scalar_data"][state_index]
        elif self.action_type == "delta_ee_pose":
            arm_index = meta_info["keys"]["scalar_data"].index(b'delta_ee_pose_action')
            arm_key = meta_info["keys"]["scalar_data"][arm_index]
            state_index = meta_info["keys"]["scalar_data"].index(b'observation/robot/ee_pose_state')
            state_key = meta_info["keys"]["scalar_data"][state_index]
        else:
            raise NotImplementedError(f"Action type {self.action_type} not supported")
        
        gripper_index = meta_info["keys"]["scalar_data"].index(b'gripper_close') 
        gripper_key = meta_info["keys"]["scalar_data"][gripper_index]
        # qpos_index = meta_info["keys"]["scalar_data"].index(b'observation/robot/qpos')
        # qpos_key = meta_info["keys"]["scalar_data"][qpos_index]

        primary_index = meta_info["keys"][f"observation/{self.obs_type}/color_image"]
        wrist_index = meta_info["keys"]["observation/realsense/color_image"]

        language_instruction = meta_info["language_instruction"]

        # get grounding info
        pick_uid = meta_info['task_data']['goal'][0][0]['obj1_uid']
        place_uid = meta_info['task_data']['goal'][0][0]['obj2_uid']
        target_position = meta_info['task_data']['goal'][0][0]['position']
        
        
        # ä¸€æ¬¡æ€§ç¼©æ”¾è½¨è¿¹ --> # TODO è¿™äº›é€»è¾‘å°±ä¸åº”è¯¥åœ¨è¿™é‡Œï¼Œåº”è¯¥æ˜¯æ•°æ®é¢„å¤„ç†é˜¶æ®µå°±éœ€è¦å‡†å¤‡å¥½ä¸­é—´è¡¨å¾ TODO ç­‰ç¨³å®šåæŠ½è±¡ä¸º é¢„å¤„ç†ä»£ç 
        # Load sequence data
        sequence = []
        with lmdb_env.begin(write=False) as txn:

            arm_action = pickle.loads(txn.get(arm_key))
            if self.action_type == "abs_ee_pose" or self.action_type == "delta_ee_pose":
                positions = np.array([pos for pos, quat in arm_action])
                quaternions = np.array([quat for pos, quat in arm_action])
                arm_action = np.concatenate([positions, quaternions], axis=1).tolist()
            gripper_action = pickle.loads(txn.get(gripper_key))

            primary_data = pickle.loads(txn.get(primary_index[start_id]))
            primary_data = cv2.imdecode(np.frombuffer(primary_data, np.uint8), cv2.IMREAD_COLOR)
            # convert to PIL Image
            primary_data = Image.fromarray(primary_data)

            # get wrist data
            wrist_data = pickle.loads(txn.get(wrist_index[start_id]))
            wrist_data = cv2.imdecode(np.frombuffer(wrist_data, np.uint8), cv2.IMREAD_COLOR)
            # convert to PIL Image
            wrist_data = Image.fromarray(wrist_data)
            wrist_data = wrist_data.resize((224, 224))
            
            # TODO mv to func
            all_trace_2d = pickle.loads(txn.get(b'observation/obs_camera/tcp_2d_trace'))
            all_gripper_close = pickle.loads(txn.get(b'gripper_close'))

            # 2) current / target bbox
            all_raw_boxes = pickle.loads(txn.get( b'observation/obs_camera/bbox2d_loose' ))
            all_labels_list = pickle.loads(txn.get(b'observation/obs_camera/bbox2d_loose_id2labels'))
            curr_boxes, curr_labels = get_bbox_and_label_arrays(all_raw_boxes, all_labels_list, start_id)

            # resize to 224*224
            image_size = primary_data.size
            primary_data = primary_data.resize((224, 224), Image.BILINEAR)
            scale_x = 224 / image_size[0]
            scale_y = 224 / image_size[1]
            all_trace_2d_scaled = scale_trace(all_trace_2d, scale_x, scale_y)

            # 2) current / target bbox
            current_pick_bbox, pick_name = compute_bbox_for_uid(
                curr_boxes, curr_labels, pick_uid, scale_x, scale_y)
            
            target_place_bbox, place_name = compute_bbox_for_uid(
                curr_boxes, curr_labels, place_uid, scale_x, scale_y)

             # 3) affordance point
             # å¹¶ä¸æ˜¯å…¨éƒ¨éƒ½æœ‰ è¿™ä¸ªï¼Ÿ
            future_griper_change = detect_first_change_point(start_id, all_gripper_close,
                                                    all_trace_2d_scaled,
                                                    point_index=1)


            # 4) è½¨è¿¹
            j = future_griper_change["frame_idx"] #future_griper_change_idx

            future_traj = get_trajectory_plan(all_trace_2d_scaled, start_id, j,
                                                horizon=10)
            
            # prompt = f"What is the key object to finish the task: {language_instruction}. Output the bbox to locate the object" # TODO ç†è®ºä¸Š QwenVL é‚£è¾¹çš„é€»è¾‘è¦ç§»åŠ¨åˆ°è¿™é‡Œï¼Œ ä½†æ˜¯é—®é¢˜æ˜¯ infer çš„æ—¶å€™æ˜¯æ²¡æœ‰dataloader é€»è¾‘çš„ã€‚ éœ€è¦åœ¨æƒ³æƒ³
            # --> config åŒ–å°±å°±å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜äº† --> ç›®å‰ä¸ºäº†é€»è¾‘æ˜æ˜¾ï¼Œ è¿˜æ˜¯å…ˆæ”¾åˆ°VLä¸­
            solution = f"Pick {pick_name} at {current_pick_bbox}, then place {place_name} at {target_place_bbox}."
            # æ‹¼æ¥ solution ä¸ºå­—ç¬¦ä¸²æ ¼å¼çš„ JSONï¼Œ è¿˜å·®æœ‰ä¸€ç‚¹ç‚¹gap
            # solution = f'''{{
            # "pick": {{
            #     "bbox_2d": {current_pick_bbox},
            #     "label": "{pick_name}"
            # }},
            # "place": {{
            #     "bbox_2d": {target_place_bbox},
            #     "label": "{place_name}"
            # }}
            # }}'''

        lmdb_env.close()
        # action chuck: window_size
        action_length = len(arm_action)
        if action_length >= self.window_size + start_id:
            action = arm_action[start_id:start_id + self.window_size]
            gripper = gripper_action[start_id:start_id + self.window_size]
        else:
            # last action repeat
            action = arm_action[start_id:action_length] + np.repeat(arm_action[-1], self.window_size - (action_length - start_id), axis=0)
            gripper = gripper_action[start_id:action_length] + np.repeat(gripper_action[-1], self.window_size - (action_length - start_id), axis=0)

        collected_action = []
        for a, g in zip(action, gripper):
            collected_action.append(self.load_robot_action(a, g).astype(np.float16))

        obs_dict = {
            "primary_data": primary_data,
            "wrist_data": wrist_data,
        }
        obs_list = self.config.datasets.vla_data.get("obs", ["primary_data"])
        image = [obs_dict[key] for key in obs_list] # TODO éœ€è¦å˜æˆcfgæ§åˆ¶

        CoT_sentences = None
        CoT_type = self.config.datasets.vla_data.get("CoT_answer", False)
        if CoT_type: # è¿™é‡Œåº”è¯¥æ˜¯å®šä¹‰ä¸ºä¸€ä¸ª get cot func
            if CoT_type == "bbox":
                CoT_sentences = solution
            else:
                # print(f"CoT type {CoT_type} not supported, returning None")
                CoT_sentences = None

        return dict(action=collected_action,image=image,lang=language_instruction, 
                    solution=CoT_sentences,
                    dataset_name=self.dataset_name)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """Get sequence from dataset at given index with retry logic."""
        num_base_retries = 3
        num_random_retries = 30

        # Try other samples, in case it is a file corruption issue
        for attempt_idx in range(num_base_retries):
            try:
                next_index = random.randint(0, len(self) - 1)  # Select a random index
                sample = self.__getitem(idx)
                return sample
            except Exception as e:
                logger.warning(f"[Try other #{attempt_idx}] Failed to fetch sample {next_index}. Exception: {e}")
                pass

        # Final attempt: fetch a random sample
        for attempt_idx in range(num_random_retries):
            try:
                random_idx = random.randint(0, len(self) - 1)  # Select a random index
                sample = self.__getitem(random_idx)
                logger.warning(f"Returning random sample {random_idx} due to repeated failures for sample {idx}.")
                return sample
            except Exception as e:
                logger.error(f"Final attempt failed to fetch sample {idx}. Exception: {e}")
                raise e
        
    def __iter__(self):
        """Iterate through the dataset sequentially, retrying with random indices on error."""
        "DataLoaderShard é»˜è®¤ä¸ä¼šè°ƒç”¨åº•å±‚æ•°æ®é›†çš„ __iter__ æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥é€šè¿‡ __getitem__ è·å–æ•°æ®ã€‚"
        
        # indices = list(range(len(self)))  # åˆ›å»ºæ‰€æœ‰æ ·æœ¬çš„ç´¢å¼•åˆ—è¡¨
        # random.shuffle(indices)  # æ‰“ä¹±ç´¢å¼•é¡ºåº 
        # # TODO check ä¸ºä»€ä¹ˆ DataLoaderShard æ²¡æœ‰èµ°è¿™ä¸ªè·¯å¾„ --> æˆ‘å¯ä»¥é»˜è®¤åˆ†å¸ƒå¼å·²ç»å¤„ç†äº†è¿™ä¸ªä¹ˆï¼Ÿ --> check
        
        for idx in range(len(self)): # æ„Ÿè§‰è¿™é‡Œæ²¡æœ‰è¢«æ•ˆç”¨ï¼Ÿ
            attempt = 0  # åˆå§‹åŒ–å°è¯•è®¡æ•°
            while attempt < 10:  # æœ€å¤šå°è¯• 10 æ¬¡
                try:
                    yield self.__getitem__(idx)
                    break  # å¦‚æœæˆåŠŸï¼Œè·³å‡ºå¾ªç¯
                except Exception as e:
                    logger.warning(f"Error at index {idx}, attempt {attempt + 1}: {e}. Retrying with a random index.")
                    random_idx = random.randint(0, len(self) - 1)
                    idx = random_idx
                attempt += 1  # å¢åŠ å°è¯•è®¡æ•°

    # TODO è¿™ä¸ªå‡½æ•° éœ€è¦é‡æ„ï¼Œ ä¸èƒ½å’Œæ•°æ®é›†è€¦åˆ
    def load_robot_action(self, arm_action, gripper_action):
        if self.action_type in ["abs_qpos", "delta_qpos", "abs_ee_pose", "delta_ee_pose"]:
            actions = np.zeros(8)
            actions[:7] = 2 * (arm_action[:7] - self.arm_action_min[:7]) / (self.arm_action_max[:7] - self.arm_action_min[:7] + 1e-8) - 1
            # normalize gripper_action to 0 or 1
            actions[-1] = (gripper_action + 1) / 2
            assert np.all(actions <= 1) and np.all(actions >= -1)
            return actions
        else:
            raise NotImplementedError(f"Action type {self.action_type} not supported")

    def get_dataset_statistics(self) -> Dict:
        """Return dataset statistics in the same format as RLDS datasets."""
        return self.dataset_statistics

    def save_statistics(self, run_dir: Path) -> None:
        """Save dataset statistics to the specified directory."""
        save_dataset_statistics(self.dataset_statistics, run_dir)

class EpisodicLMDBDataset(LMDBDataset):
    """Returns full episodes as list of steps instead of individual transitions (useful for visualizations)."""

    # TODO å®ç°
    pass


def scale_point(p, scale_x, scale_y):
    return [int(p[0] * scale_x), int(p[1] * scale_y)]

def scale_trace(trace_2d, scale_x, scale_y):
    return [np.array([scale_point(p,scale_x, scale_y) for p in frame]) for frame in trace_2d]

def get_bbox_and_label_arrays(key_bbox, key_label, frame_idx: int) -> tuple:
    """
    ä» txn ä¸­è¯»å–æŒ‡å®šå¸§çš„ loose bbox æ•°ç»„ä¸ id2label dictã€‚
    è¿”å›ï¼š (boxes_np, labels_dict)
    boxes_np: numpy array shape (N,5) æ¯è¡Œ [label, xmin,ymin,xmax,ymax]
    labels_dict: dict[label_id->{class: uid}]
    """

    frame_boxes = np.stack([np.array(x).item() for x in key_bbox[frame_idx]])
    labels_list = key_label[frame_idx]
    return frame_boxes, labels_list

def get_dummy_dataset(dataconfig: dict):

    pass



def get_trajectory_plan(
    all_trace_2d: list,
    start_idx: int = 0,
    end_idx: int = -1,
    horizon: int=10,
    scale_x: float = 1.0,
    scale_y: float = 1.0
) -> list:
    """
    ä» start_idx åˆ° end_idx ä¹‹é—´ï¼Œå‡åŒ€é‡‡æ · horizon ä¸ªç¬¬äºŒæŠ•å½±ç‚¹ï¼Œå¹¶ç¼©æ”¾åˆ°å›¾åƒåæ ‡ã€‚
    å¦‚æœ horizon å¤§äºå¸§æ•°å·®ï¼ˆend_idx - start_idx + 1ï¼‰ï¼Œä¼šå¯¹ç›¸é‚»å¸§åšçº¿æ€§æ’å€¼ä»¥è¡¥é½é‡‡æ ·ç‚¹ã€‚

    å‚æ•°:
      all_trace_2d: list of shape [(n_points, 2), ...]ï¼Œæ¯å¸§è‹¥æœ‰å¤šç‚¹ï¼Œåˆ™é€‰ç¬¬ 2 ä¸ªç‚¹
      start_idx: èµ·å§‹å¸§ç´¢å¼•
      end_idx: ç»“æŸå¸§ç´¢å¼•ï¼ˆåŒ…å«ï¼‰
      horizon: é‡‡æ ·ç‚¹æ€»æ•°
      scale_x, scale_y: ç¼©æ”¾å› å­

    è¿”å›:
      traj: list of [x, y]ï¼Œé•¿åº¦ä¸º horizon
    """
    T = len(all_trace_2d)
    if end_idx == -1:
        end_idx = T - 1
    if not (0 <= start_idx < T):
        raise IndexError(f"start_idx {start_idx} out of range [0, {T})")
    if not (0 <= end_idx < T):
        raise IndexError(f"end_idx {end_idx} out of range [0, {T})")
    if end_idx <= start_idx:
        raise ValueError(f"end_idx ({end_idx}) must be > start_idx ({start_idx})")
    if horizon < 2:
        raise ValueError("horizon must be at least 2 to interpolate")

    # åœ¨ [start_idx, end_idx] åŒºé—´ç”Ÿæˆå‡åŒ€æµ®ç‚¹ç´¢å¼•
    positions = np.linspace(start_idx, end_idx, num=horizon)

    traj = []
    for pos in positions:
        # è‹¥ pos æ­£å¥½æ˜¯æ•´æ•°ç´¢å¼•ï¼Œç›´æ¥å–è¯¥å¸§
        if pos.is_integer():
            idx = int(pos)
            pts = all_trace_2d[idx]
            x, y = pts[1]
        else:
            # å¦åˆ™å¯¹ floor å’Œ ceil å¸§åæ ‡åšçº¿æ€§æ’å€¼
            lo, hi = int(np.floor(pos)), int(np.ceil(pos))
            alpha = pos - lo
            pts_lo = all_trace_2d[lo][1]
            pts_hi = all_trace_2d[hi][1]
            x = (1 - alpha) * pts_lo[0] + alpha * pts_hi[0]
            y = (1 - alpha) * pts_lo[1] + alpha * pts_hi[1]

        # ç¼©æ”¾å¹¶å–æ•´
        traj.append([int(x * scale_x), int(y * scale_y)])

    return traj


def detect_first_change_point(
    current_index: int,
    gripper_close: List[int],
    trace_2d: List[List[List[float]]],
    point_index: int = 1
) -> Optional[Dict]:
    """
    ä» current_index å¼€å§‹ï¼Œæ‰¾åˆ° gripper_close åˆ—è¡¨ä¸­ç¬¬ä¸€æ¬¡å‘ç”Ÿå€¼å˜åŒ–çš„æ—¶åˆ»ï¼Œ
    å¹¶è¿”å›è¯¥å¸§çš„æŒ‡å®šå…³é”®ç‚¹åæ ‡ã€‚

    å‚æ•°:
        current_index: å¼€å§‹æ£€æµ‹çš„å¸§ç´¢å¼•
        gripper_close: æ‰‹çˆªçŠ¶æ€åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼Œ-1 è¡¨ç¤ºå¼ å¼€ï¼Œ1 è¡¨ç¤ºé—­åˆï¼‰
        trace_2d: æ¯å¸§çš„ 2D å…³é”®ç‚¹åˆ—è¡¨ï¼Œå½¢çŠ¶ä¸º [å¸§æ•°][å…³é”®ç‚¹æ•°][2]
        point_index: è¦è¿”å›çš„å…³é”®ç‚¹åœ¨æ¯å¸§åˆ—è¡¨ä¸­çš„ç´¢å¼•

    è¿”å›:
        {'frame_idx': i, 'point': [x, y]} æˆ– Noneï¼ˆå¦‚æœåˆ°æœ«å°¾éƒ½æ²¡æœ‰å˜åŒ–ï¼‰
    """
    assert len(gripper_close) == len(trace_2d), "é•¿åº¦ä¸ä¸€è‡´ï¼Œæ— æ³•å¯¹åº”æ¯ä¸€å¸§"

    prev = gripper_close[current_index]
    # ä»ä¸‹ä¸€ä¸ªå¸§å¼€å§‹æ£€æµ‹
    for i in range(current_index + 1, len(gripper_close)):
        curr = gripper_close[i]
        x, y = trace_2d[i][point_index]
        change_point =  {
                'frame_idx': i,
                'point': [int(x), int(y)]
            }
        if curr != prev:
            return change_point
        prev = curr

    # æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å˜åŒ–, è¿”å›æœ€åä¸€ä¸ªç‚¹
    return change_point

from typing import List, Dict, Any, Callable, Optional
from torch.utils.data import DataLoader


def get_lmdb_dataset(
    data_root_dir: Path,
    data_mix: str,
    data_mix_info: str,
    obs_type: str = "obs_camera",
    action_type: str = "abs_qpos",
    window_size: int = 16,
    image_aug: bool = False,
    episodic: bool = False,
    normalization_type: NormalizationType = NormalizationType.BOUNDS_Q99,
    save_statistics_dir: str = None,
    obs_list: Optional[List[str]] = None,
    is_return_CoT: bool = False,
    **kwargs: Any,
) -> Tuple[Dataset]:
    """Initialize LMDB Dataset and optionally save statistics."""

    batch_transform = LMDBBatchTransform()
    
    # Build LMDB Dataset
    cls = LMDBDataset # TODO ä¹‹å å†è€ƒè™‘æ”¯æŒ EpisodicLMDBDataset dataset if not episodic else EpisodicLMDBDataset
    dataset = cls(
        data_mix,
        data_root_dir,
        data_mix_info,
        obs_type=obs_type,
        action_type=action_type,
        window_size=window_size,
        image_aug=image_aug,
        batch_transform=batch_transform,
        normalization_type=normalization_type,
        save_statistics_dir=save_statistics_dir,
        # obs_list=obs_list,
        # is_return_CoT=is_return_CoT, ç­‰ç¨³å®šåå†è€ƒå¯Ÿå˜æˆ æ˜¾ç¤ºå‚æ•°
        config=kwargs["config"]
    )

    # Optionally save statistics to run directory
    if save_statistics_dir is not None:
        dataset.save_statistics(Path(save_statistics_dir))

    return dataset

from torch.utils.data._utils.collate import default_collate
from torchvision.transforms import ToTensor

import torch.distributed as dist
from types import SimpleNamespace

def collate_fn(batch):
    # batch: list of items, å‡è®¾æ¯ä¸ª item æ˜¯ (PIL.Image, other_info)

    pass # TODO å¦‚æœè¦åŠ¨æ€ inputï¼Œ å°±ä¸èƒ½ç”¨ default_collate
    # dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åœ¨åŒä¸€æ—¶é—´ç‚¹

    return batch # æˆ‘ä»¬å®æ„¿è¿”å›ä¸€ä¸ª list_of_dict for åŠ¨æ€çš„ inputs

def dict_to_namespace(d):
    if isinstance(d, dict):
        return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})
    elif isinstance(d, list):
        return [dict_to_namespace(i) for i in d]
    else:
        return d

if __name__ == "__main__":
    pass
    #@Jinhui TODO å…¨éƒ¨ æ¨¡å—æ–‡ä»¶å¿…é¡»èƒ½å¤Ÿç‹¬ç«‹ æ‰§è¡Œæµ‹è¯•å•å…ƒ

    import debugpy
    debugpy.listen(("0.0.0.0", 10092))  # ç›‘å¬ç«¯å£ 
    print("Waiting for debugger to attach 10092...")
    debugpy.wait_for_client()  # ç­‰å¾… VS Code é™„åŠ 

    # # test  get_vla_dataset

    # Load YAML config & Convert CLI overrides to dotlist config
    config_yaml = "llavavla/conf/qwenvla_lmdb_IROC.yaml"
    cfg = OmegaConf.load(config_yaml)


    vla_dataset = get_lmdb_dataset( # æ‹’ç»ä»»ä½•å†…éƒ¨è½¬æ¢ --> è¿™é‡Œä¿æŒ @Jinhui çš„ version
        data_root_dir=cfg.datasets.vla_data.data_root_dir, # å¤ªå¤šå‚æ•°äº†ï¼Œ åº”è¯¥config ç©¿è¶Šè¿‡å»ï¼Œ æˆ–è€…æ˜¯ ** çš„æ–¹å¼
        data_mix=cfg.datasets.vla_data.data_mix,
        data_mix_info=cfg.datasets.vla_data.data_mix_info,
        action_type=cfg.datasets.vla_data.action_type,
        default_image_resolution=tuple(cfg.datasets.vla_data.default_image_resolution),
        shuffle_buffer_size=cfg.datasets.vla_data.shuffle_buffer_size,
        image_aug=cfg.datasets.vla_data.image_aug,
        future_action_window_size=cfg.framework.action_model.future_action_window_size,
        past_action_window_size=cfg.framework.action_model.past_action_window_size,
        load_all_data_for_training=cfg.datasets.vla_data.load_all_data_for_training,
        config=cfg,
    )
    
    # æ–¹æ³•2: ä½¿ç”¨è¿­ä»£å™¨
    dataset_iter = iter(vla_dataset)
    count = 0
    while True and count < 20 :
        try:
            batch_samples = next(dataset_iter)
            count += 1
            print(batch_samples['action'])
            
        except StopIteration:
            break

    count = 0
    for batch_samples in vla_dataset:
        print(batch_samples['action'])
        count += 1
        if count > 20:
            break
