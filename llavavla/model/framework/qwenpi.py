"""
cogactvla.py

"""
from __future__ import annotations
from typing import Union, List

import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from types import SimpleNamespace
import torch, json
import torch.nn as nn
import numpy as np
from PIL import Image
import re
from prismatic.overwatch import initialize_overwatch

from llavavla.model.action_model.action_model import ActionModel
import torch.distributed as dist
# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100

def dict_to_namespace(d):
    if isinstance(d, dict):
        return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})
    elif isinstance(d, list):
        return [dict_to_namespace(i) for i in d]
    else:
        return d

# get QWen2.5
from llavavla.model.vlm import _QWen_VL_Interface #ä¸åº”è¯¥å¼ºä¾èµ–äºè¿™ä¸ªï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªæ¥å£ç±»ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„ç±», TODO ä¸è¦å®ç° hard æ¥å£ç±»ï¼Œ ä½¿ç”¨ **kwargs
from llavavla.model.tools import auto_get_module_keys, auto_get_trainable_modules # åç»­åº”è¯¥æ˜¯trainer çš„èŒè´£èŒƒå›´
from llavavla.model.vlm.QWen2_5 import get_qwen2_5_interface
from llavavla.model.projector.QFormer import get_layerwise_qformer

class QwenQFormerDiT(nn.Module):
    def __init__(
        self,
        qwen_model_name:str = './playground/Pretrained_models/Qwen2.5-VL-3B-Instruct', # è¿™æ˜¯ä¸å¥½çš„å®ç°ï¼Œ ä¸€å®šä¸èƒ½æ˜¯äº’ç›¸ä¾èµ–
        action_model_type: str = 'DiT-B', 
        vl_token_dim: int = 2048,
        action_hidden_dim: int = 768,  # @Jinhui # è¿™ä¸ª åº”è¯¥æ˜¯å’ŒDiT-B
        action_dim: int = 7,
        future_action_window_size: int = 15,
        past_action_window_size: int = 0,
        use_ema: bool = False,
        norm_stats: Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]] = None,
        config: Optional[dict] = None,  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config, ä½†æ˜¯ç°åœ¨æ˜¯ç›´æ¥ä¼ å…¥å‚æ•°
        **kwargs,
    ) -> None:
        super().__init__()
        
        # TODO å…¨éƒ¨è½¬ å…¨å±€config, è¦é¢å‘å¯¹è±¡ç¼–ç¨‹
        self.qwen_vl_interface = get_qwen2_5_interface(qwen_model_name, config) 
        self.layer_qformer = get_layerwise_qformer(config=config) # @Jinhui éœ€è¦é€»è¾‘ä»QWen ä¸­å¯¹é½ hidden
        self.action_model = ActionModel(model_type = action_model_type,  # TODO @Jinhui åº”è¯¥å†™åˆ° get_action_model()
                                            action_hidden_dim = action_hidden_dim, # è¿™äº›å‚æ•°å…³ç³»è¦ TODOé›†ä¸­ è®¾ç½®åˆ°config
                                            in_channels = action_dim, 
                                            future_action_window_size = future_action_window_size, 
                                            past_action_window_size = past_action_window_size) # ä¹Ÿåº”è¯¥ç”¨ å‡½æ•°å°è£…
        
        # TODO ActionModel éœ€è¦å’Œqformer ä¸€èµ·è®¾è®¡
        self.config = config
        # self.qwen_processor = vlm.processor # è¦é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œ ä¸è¦ å±æ€§å¤–æ³„
        # è¿™äº›æ˜¯ action chunck çš„å‚æ•°
        self.future_action_window_size = future_action_window_size
        self.past_action_window_size = past_action_window_size

        # self.all_module_keys = auto_get_module_keys(self) #  TODO è¿™ä¸ªæ˜¯trainerçš„ funxï¼Œ æˆ–è®¸æ˜¯å¤šä½™çš„
        self.norm_stats = norm_stats # è¿™ä¸ªæ˜¯ inference æ—¶å€™ç”¨åˆ°çš„ï¼Œ ä¸åº”è¯¥æ˜¯æ”¾åˆ°è¿™ä¸ªä½ç½®ï¼Ÿ

        # if we need some pretrain prameters, we can load them here
        # TODO éœ€è¦è€ƒè™‘è¿™ä¸ªæ˜¯è°çš„èŒè´£ --> æŒ‰ç…§æ‰å¹³ç®¡ç†ï¼Œåˆ‡å®åº”è¯¥åœ¨å†…éƒ¨åšæ¡ä»¶åˆ¤æ–­


    @property
    def trainable_module_keys(self) -> List[str]:

        # TODO check, åŸç‰ˆè¿”å›çš„æ­» vlm.model, æ–°çš„å®ç°æ˜¯vlm --> çœ‹ä¸€ä¸‹ä¿å­˜é€»è¾‘æ˜¯å¦å‘ä¸Šå˜åŒ–
        keys = auto_get_trainable_modules(self, max_depth=1)# auto å»åˆ¤æ–­å“ªäº›moduleæ˜¯trainableçš„
        return keys
    

    def forward( # TODO éœ€è¦å°† loss è®¡ç®—åˆ†ç¦»å‡ºæ¥
        self, # åªé¢å¯¹æœ€åŸå§‹çš„ data exmaples, ä¸ºäº†å¯è¯»æ€§ï¼Œè¿™é‡Œè¿˜æ˜¯è¦å†™æˆæ˜¾ç¤ºçš„å‚æ•°
        examples: List[dict] = None,  # è¿™é‡Œçš„ examples æ˜¯æŒ‡åŸå§‹çš„è¾“å…¥æ•°æ®
        repeated_diffusion_steps: int = 4,
        **kwargs,  # ğŸ‘ˆ æ•æ·ä»£ç çš„çµæ´»æ€§ï¼Œ å…è®¸ä»»ä½•å½¢å¼çš„ä¼ å‚æ•°
    ) -> Tuple:
        """Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss)."""
        # @Jinhui TBD TODO 
        images = [example["image"] for example in examples]  #  TODO check æ˜¯ä»€ä¹ˆ
        instructions = [example["lang"] for example in examples]  # [B, str]
        actions = [example["action"] for example in examples] #label
        if "solution" in examples[0]:  # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼
            solutions = [example["solution"] for example in examples]  # [B, dict]
        else: #  è¿˜æœ‰if else å’Œæ¨¡å‹å¯é˜…è¯»æ€§çš„ trade off
            solutions = None

        # print("DEBUG"*10)
        # dist.barrier
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=images, instructions = instructions, solutions=solutions) # @Jinhui TODO å†è€ƒè™‘ä¸€ä¸‹è¿™é‡Œçš„åˆ†æ”¯åˆ†æµåº”è¯¥æœ‰.pyæ§åˆ¶è¿˜æ˜¯ç”± if else
        
        if DEBUG := os.environ.get("DEBUG"):
            _, num_dict = read_mode_config(self.config.vla.pretrained_checkpoint)
            self.norm_stats = num_dict
            self.predict_action_withCoT(image=images[0], instruction=instructions[0])
            
        with torch.autocast("cuda", dtype=torch.float16):
            # dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åŠ è½½å®Œæ¯•
            qwenvl_outputs = self.qwen_vl_interface( # éƒ½æ˜¯localçš„å‚æ•°å˜åŒ–ï¼Œ ä¸è¦å†™åˆ°config, ä½†æ˜¯ä¸ºäº†ä¿æŒå¯å¤ç°ï¼Œåº”è¯¥æœ‰ä¸ªé»˜è®¤çš„ yaml
                **qwen_inputs, # å…¼å®¹æ€§å’Œå¯è¯»æ€§çš„ trade off
                # use_cache=use_cache,
                output_attentions=False, # Flash attention è¿˜ä¸ç¡®å®šæ˜¯å¦æ”¯æŒè¿”å›attentionï¼Œ å®˜æ–¹ä»£ç æœ‰bug
                output_hidden_states=True,
                return_dict=True,
                # **kwargs
                )
            pass
            # dist.barrier()
        Intern_vlm_loss = qwenvl_outputs.loss # @Jinhui TODO è¿™é‡Œæ˜¯å¯ä»¥study çš„åœ°æ–¹ï¼Œ æ˜¯å¦ training lang
        
        if Intern_vlm_loss is None or torch.isnan(Intern_vlm_loss): # TODO å°†ä¸åŒé€»è¾‘çš„ forward ç½—æ°å†™æˆ if else ä¼šç ´åå¯è¯»æ€§
            Intern_vlm_loss = torch.tensor(0.0, device=self.qwen_vl_interface.model.device)

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.vla.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.vla.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[start_layer:end_layer]) # [B, 64, D_action]
    
        # actions = torch.stack([torch.tensor(a) for a in actions], dim=0).to(action_latent_feature.device)  # [B, chunk, 7] @Jinhui TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
        # å…ˆå°† actions è½¬æ¢ä¸ºå•ä¸ª NumPy æ•°ç»„ï¼Œå†è½¬æ¢ä¸º PyTorch å¼ é‡
        actions = torch.tensor(np.array(actions), device=action_latent_feature.device)  # [B, chunk, 7] TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
        actions_future = actions[:, -(self.future_action_window_size+1):, :]
        
        # Repeat 'actions' 'repeated_diffusion_steps' times, resulting in [repeated_diffusion_steps*B, T, D]
        actions_repeated = actions_future.repeat(repeated_diffusion_steps, 1, 1)
        action_latent_feature = action_latent_feature.repeat(repeated_diffusion_steps, 1, 1)  # [repeated_diffusion_steps*B, T, D_action]
        # Action model forward and compute loss # è¿™é‡ŒåŠŸèƒ½æœ‰ç‚¹ è¶Šä¿ä»£åº– TODO å°†loss é›†ä¸­åˆ° main moduleä¸­ç»Ÿä¸€å¤„ç†
        action_loss = self.action_model.loss(actions_repeated, action_latent_feature) # TODO loss åº”è¯¥æ”¾åˆ°å¦ä¸€ä¸ªå‡½æ•°
        return action_loss, Intern_vlm_loss

    # @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action( # 
        self, image: Union[Image, List[Image]],
        instruction: str, 
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference; maps input image and task instruction to continuous action.

        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        if not isinstance(image, list):
            imgs = [image.resize((224, 224))]  # list of PIL RGB for one instruction
        else:
            imgs = [img.resize((224, 224)) for img in image]
        
        lang = instruction.lower() 

        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=[imgs], instructions = [lang]) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        
        # add by Jinhui

        # end add by Jinhui
        # Generate cognition feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface( # TODO è¿™é‡Œä¹‹åè¦ç”¨generation func
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                output_hidden_states=True, 
                return_dict=True,
            ) # generation æ‹¿ä¸åˆ°å‰é¢token çš„ä¿¡æ¯ï¼Œè€ƒè™‘ä½¿ç”¨ forward?

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer 
            end_layer = self.config.framework.layer_qformer.qformer_end_layer
            
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[start_layer:end_layer]) # [B, 64, D_action]
            
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = action_latent_feature.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0) #[2,16,7]
            uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
            uncondition_shape = uncondition.shape
            uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
            uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
            z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=action_latent_feature)
            sample_fn = self.action_model.net.forward
        
        # if os.environ.get("DEBUG"):
        #     print(z .shape)
        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=action_latent_feature.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()

        # Un-normalize Actions        
        action_norm_stats = self.get_action_stats(unnorm_key)
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high, action_low = np.array(action_norm_stats["q99"]), np.array(action_norm_stats["q01"])
        normalized_actions = np.clip(normalized_actions, -1, 1)
        normalized_actions[:, 6] = np.where(normalized_actions[:, 6] < 0.5, 0, 1) 
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions,
        )
        # actions max 1, min -0.05 # æ„Ÿè§‰ä¸å†ä¸€ä¸ª scale
        return actions, normalized_actions

    # @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action_withCoT( # 
        self, image: Union[Image, List[Image]],
        instruction: str, 
        solution: Union[Dict, List[Dict]] = None, # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼, å¯ä»¥ç”¨äºå‡ºä¸­é—´è¡¨å¾çš„è¯„æµ‹ï¼Ÿ
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        if not isinstance(image, list):
            imgs = [image.resize((224, 224))]  # list of PIL RGB for one instruction
        else:
            imgs = [img.resize((224, 224)) for img in image]
        
        lang = instruction.lower() 
        
        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=[imgs], instructions = [lang]) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        # Generate feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface.model.generate(
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, 
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                output_hidden_states=True,
                 max_new_tokens=256,
                return_dict_in_generate=True,
            ) 
            # for check output format
            decoded_sequences = self.qwen_vl_interface.processor.tokenizer.batch_decode(
            qwenvl_outputs.sequences, 
            skip_special_tokens=True 
            )
            print(decoded_sequences[0])

            hidden_states = qwenvl_outputs.hidden_states # [num_layers, batch_size, 1 + new token, hidden_dim]

            # è¿™é‡Œè¦å°†ç”Ÿæˆçš„tokenæ‹¼æ¥å›æ¥
            prefix_hidden_states = hidden_states[0]  # Shape: [num_layers, B, prefix_len, hidden_dim]
            prefix_hidden_states = torch.stack(prefix_hidden_states, dim=0)  # Shape: [num_layers, B, prefix_len, hidden_dim]
            
            # Step 1: Convert list of lists to a tensor [num_new_tokens, num_layers, B, 1, hidden_dim]
            new_hidden_states = torch.stack([
                torch.stack(layer_hiddens, dim=0) 
                for layer_hiddens in hidden_states[1:]
            ], dim=0)
            
            # Step 2: Remove singleton dimension and transpose to [num_layers, B, num_new_tokens, hidden_dim]
            new_hidden_states = new_hidden_states.squeeze(2).permute(1,2,0,3)  # [num_layers, B, num_new_tokens, hidden_dim]
            
            # Concatenate prefix and new tokens
            combined_hidden_states = torch.cat([
                prefix_hidden_states,  # [num_layers, B, prefix_len, hidden_dim]
                new_hidden_states     # [num_layers, B, num_new_tokens, hidden_dim]
            ], dim=2)  # Shape: [num_layers, B, total_len, hidden_dim]

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer # TODO è¿™äº›æ¨¡å‹framework å±‚é¢çš„setting åº”è¯¥æ”¾åˆ° initial ä¸­å°±æ‹¿åˆ°ï¼Ÿ è¿˜æ˜¯å…¨å±€ç»Ÿä¸€ åœ¨å‚æ•°ä¸­æ‹¿åˆ°ï¼Ÿ
            end_layer = self.config.framework.layer_qformer.qformer_end_layer
            
            latent_features = []
            # TODO ä¸Šé¢ä¸ºå¯è¯»æ€§ï¼Œç‰ºç‰²äº†é€Ÿåº¦, ç¨³å®šåå¯ä»¥è€ƒè™‘ åªè½¬æ¢éœ€è¦ç”¨çš„feature
            for i in range(start_layer, end_layer):
                latent_features.append(combined_hidden_states[i]) # 
            action_latent_feature = self.layer_qformer(latent_features) # [B, 64, D_action]
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = action_latent_feature.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0) #[2,16,7]
            uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
            uncondition_shape = uncondition.shape
            uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
            uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
            z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=action_latent_feature)
            sample_fn = self.action_model.net.forward
        
        # if os.environ.get("DEBUG"):
        #     print(z .shape)
        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=action_latent_feature.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()
        # Un-normalize Actions --> è¿™ä¸ªä¿¡æ¯åº”è¯¥é›†æˆåœ¨å“ªé‡Œï¼Œèƒ½å¤Ÿèƒ½å¤Ÿå–æ¶ˆåŠ¨æ€
        return normalized_actions, normalized_actions # TODO Debug with stats is dim=7

    def freeze_backbones(self):
        """
        æ ¹æ®ç›¸å¯¹æ¨¡å—è·¯å¾„åˆ—è¡¨ï¼ˆpatternsï¼‰ç›´æ¥å†»ç»“æŒ‡å®šå­æ¨¡å—ï¼Œä¸å†é€’å½’æŸ¥æ‰¾æ‰€æœ‰å­æ¨¡å—åç§°ï¼š
          - patterns: ä» config.vla.freeze_modules ä¸­è¯»å–ï¼Œç”¨é€—å·åˆ†éš”å¾—åˆ°çš„â€œç›¸å¯¹è·¯å¾„â€åˆ—è¡¨
            ä¾‹å¦‚ "qwen_vl_interface, action_model.net"ï¼Œ
            å°±æ„å‘³ç€å†»ç»“ self.qwen_vl_interface å’Œ self.action_model.netã€‚
        è¿”å›å€¼ï¼š
          - frozen: å®é™…æ‰¾åˆ°å¹¶å†»ç»“çš„æ¨¡å—è·¯å¾„åˆ—è¡¨
        """
        freeze_modules = ( # æˆ‘è§‰å¾—å…¨å±€å°±åº”è¯¥åªæœ‰ä¸€ä¸ªconfigï¼Œ ä½¿ç”¨æ²¡å¿…è¦ç›¸å¯¹è·¯å¾„
            self.config.trainer.freeze_modules
            if (self.config and hasattr(self.config.trainer, "freeze_modules"))
            else None
        )
        # æ‹†åˆ†å¹¶å»é™¤ç©ºç™½
        patterns = [p.strip() for p in freeze_modules.split(",") if p.strip()] if freeze_modules else []

        frozen = []
        for path in patterns:
            # å°†â€œç›¸å¯¹è·¯å¾„â€æŒ‰ç‚¹æ‹†åˆ†ï¼Œä¾‹å¦‚ "action_model.net" â†’ ["action_model", "net"]
            attrs = path.split(".")
            module = self
            try:
                for attr in attrs:
                    module = getattr(module, attr)
                # å¦‚æœæˆåŠŸ get åˆ° moduleï¼Œå°±æŠŠå®ƒå’Œå®ƒçš„æ‰€æœ‰å­æ¨¡å—å‚æ•°éƒ½ freeze
                for param in module.parameters():
                    param.requires_grad = False
                frozen.append(path)
            except AttributeError:
                # å¦‚æœæŸä¸€çº§å±æ€§ä¸å­˜åœ¨ï¼Œå°±è·³è¿‡å¹¶æ‰“å°è­¦å‘Š
                print(f"âš ï¸ æ¨¡å—è·¯å¾„ä¸å­˜åœ¨ï¼Œæ— æ³•å†»ç»“ï¼š{path}")
                continue

        dist.barrier()  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶åŒæ­¥
        print(f"ğŸ”’ Frozen modules (by relative path): {frozen}")
        return frozen
    
    def load_pretrained_backbones(self, config): # TODO Jinhui è¿™åœ¨å“ªé‡Œè¢«è°ƒç”¨è¿˜æ˜¯éœ€è¦å•†é‡
        """
        åŠ è½½ checkpointï¼š
        - å¦‚æœè®¾ç½®äº† config.vla.reload_modulesï¼ˆé€—å·åˆ†éš”çš„æ¨¡å—è·¯å¾„ï¼‰â†’ æŒ‰è·¯å¾„éƒ¨åˆ†åŠ è½½
        - å¦åˆ™ â†’ åŠ è½½æ•´ä¸ªæ¨¡å‹å‚æ•°ï¼ˆè¦†ç›– selfï¼‰

        è¿”å›ï¼š
            æ›¿æ¢ï¼Œloaded_modules: æˆåŠŸåŠ è½½å‚æ•°çš„æ¨¡å—è·¯å¾„åˆ—è¡¨ï¼›è‹¥å…¨å±€åŠ è½½åˆ™ä¸º ["<full_model>"]
        """
        # TODO å¥½åƒå°±æ²¡æœ‰æ‰§è¡Œè¿™é‡Œ
        # print("å¥½åƒå°±æ²¡æœ‰æ‰§è¡Œè¿™é‡Œ"*100)
        checkpoint_path = getattr(self.config.trainer, "pretrained_checkpoint", None)
        reload_module_name = getattr(self.config.trainer, "reload_modules", None)

        if not checkpoint_path:
            return []  
        if dist.get_rank() == 0:
            print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ checkpoint: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location="cpu")
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ checkpoint å¤±è´¥: {e}")

        loaded_modules = []

        if reload_module_name:  # éƒ¨åˆ†åŠ è½½
            module_paths = [p.strip() for p in reload_module_name.split(",") if p.strip()]
            for path in module_paths:
                reload_module_name = path.split(".")
                module = self
                try:
                    for module_name in reload_module_name: # è¿™é‡Œ top2down çš„æ‰¾åˆ° è¦ä¿®æ”¹çš„module
                        module = getattr(module, module_name)
                    prefix = path + "."
                    sub_state_dict = {
                        k[len(prefix):]: v
                        for k, v in checkpoint.items()
                        if k.startswith(prefix)
                    }

                    if sub_state_dict:
                        module.load_state_dict(sub_state_dict, strict=True)
                        if dist.get_rank() == 0:
                            print(f"âœ… å‚æ•°å·²åŠ è½½åˆ°æ¨¡å— '{path}'")
                        loaded_modules.append(path)
                    else:
                        print(f"âš ï¸ checkpoint ä¸­æœªæ‰¾åˆ° '{path}' ç›¸å…³å‚æ•°")
                except AttributeError:
                    print(f"âŒ æ— æ³•æ‰¾åˆ°æ¨¡å—è·¯å¾„ï¼š{path}")
        else:  # å…¨éƒ¨åŠ è½½
            try:
                self.load_state_dict(checkpoint, strict=True)
                if dist.get_rank() == 0:
                    print("âœ… å·²åŠ è½½<full_model>æ¨¡å‹å‚æ•°")
                loaded_modules = ["<full_model>"]
            except Exception as e:
                raise RuntimeError(f"âŒ åŠ è½½å®Œæ•´æ¨¡å‹å¤±è´¥: {e}")

        return loaded_modules
    def print_freeze_status(self): # è¿™ä¸ªæ˜¯ å·¥å…·ç±»æ–¹æ³•ã€‚ å¯ä»¥è€ƒè™‘ç§»åŠ¨
        for name, param in self.named_parameters():
            status = "Frozen" if not param.requires_grad else "Trainable"
            print(f"{name:60s}  |  {status}")

    @classmethod
    def from_pretrained( # @Jinhui TODO è¿™é‡Œè¦å†™å¦‚ä½•resume checkpoints
        cls,
        pretrained_checkpoint: str,
        **kwargs,
    ) -> None:
        pretrained_checkpoint = Path(pretrained_checkpoint)
        model_config, norm_stats = read_mode_config(pretrained_checkpoint) # è¯»å– config å’Œ norm_stats
        # Initialize CogACT
        # model_config TODO DEBUE @JinhuiYE è¿™é‡Œåº”è¯¥ä¿è¯training infer çš„å‚æ•°å’Œæ¨¡å‹ğŸ”—æ˜¯ä¸€è‡´çš„ ï¼ˆç‰¹åˆ«æ˜¯ QFormer)
        # TODO 
        config = dict_to_namespace(model_config)
        model_config = config # TODO ä¸è¦ä½¿ç”¨ç›¸å¯¹å˜é‡ model_configï¼Œ éœ€è¦æ¢åå­—
        model_config.trainer.pretrained_checkpoint = None # ä¸ºäº†åŠ å¿«åŠ è½½é€Ÿåº¦ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œ TODO å…¶å®ä¸åº”è¯¥åœ¨initialçš„ä½ç½®è®¾ç½® load_pretrained_backbones
        qwenQFormerACT = build_model_framework(model_config) 
        # set for action un-norm
        qwenQFormerACT.norm_stats = norm_stats
        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)
        model_state_dict = torch.load(pretrained_checkpoint, map_location="cpu") #["model"]
        
        model_keys = set(qwenQFormerACT.state_dict().keys())
        checkpoint_keys = set(model_state_dict.keys())

        # âœ… 1. åŠ è½½åŒ¹é…çš„æƒé‡
        for key in checkpoint_keys:
            if key in model_keys:
                try:
                    qwenQFormerACT.state_dict()[key].copy_(model_state_dict[key])
                    # overwatch.info(f"âœ… Loaded: {key}")
                except Exception as e:
                    overwatch.warning(f"âš ï¸ Failed to copy weight for key '{key}': {e}")
            else:
                overwatch.warning(f"âš ï¸ Checkpoint has unknown key '{key}' (not in model). Ignoring.")

        # âœ… 2. åå‘æ£€æŸ¥ï¼šæ¨¡å‹ä¸­æœ‰ä½† checkpoint ä¸­ç¼ºå¤±çš„
        missing_keys = model_keys - checkpoint_keys # TODO è¿™é‡Œä¹‹åè¦è€ƒè™‘ nontrainable params --> æˆ‘è§‰å¾—æ²¡å¿…è¦çœå­˜å‚¨ç©ºé—´
        for key in sorted(missing_keys):
                overwatch.warning(f"âš ï¸ Model expects key '{key}' but it's missing in checkpoint.")
                
        return qwenQFormerACT
    
    @staticmethod
    def _check_unnorm_key(norm_stats, unnorm_key):
        if unnorm_key is None:
            assert len(norm_stats) == 1, (
                f"Your model was trained on more than one dataset, "
                f"please pass a `unnorm_key` from the following options to choose the statistics "
                f"used for un-normalizing actions: {norm_stats.keys()}"
            )
            unnorm_key = next(iter(norm_stats.keys()))

        assert unnorm_key in norm_stats, (
            f"The `unnorm_key` you chose is not in the set of available dataset statistics, "
            f"please choose from: {norm_stats.keys()}"
        )
        return unnorm_key

    def get_action_dim(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return len(self.norm_stats[unnorm_key]["action"]["q01"])

    def get_action_stats(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return self.norm_stats[unnorm_key]["action"]

# TODO å†™ä¸€ä¸ªbuild model å‡½æ•°

def build_model_framework(model_config: dict = {}) -> QwenQFormerDiT:
    # TODO  å®ç°å’Œ config å¯¹åº”çš„ load é€»è¾‘

    model = QwenQFormerDiT(
    qwen_model_name='/mnt/petrelfs/yejinhui/Projects/llavavla/playground/Pretrained_models/Qwen2.5-VL-3B-Instruct',
    action_model_type='DiT-B',
    vl_token_dim=2048,
    action_dim=model_config.framework.action_model.action_dim,
    future_action_window_size=15,
    past_action_window_size=0,
    # use_ema=False,
    config=model_config
    )
    if (hasattr(model_config.trainer, 'pretrained_checkpoint') and model_config.trainer.pretrained_checkpoint):
        # overwatch.info(f"Loading pretrained backbones from `{model_config.vla.pretrained_checkpoint}`")
        model.load_pretrained_backbones(model_config)
        
    return model


def read_mode_config(pretrained_checkpoint):
    if os.path.isfile(pretrained_checkpoint):
        overwatch.info(f"Loading from local checkpoint path `{(checkpoint_pt := Path(pretrained_checkpoint))}`")

        # [Validate] Checkpoint Path should look like `.../<RUN_ID>/checkpoints/<CHECKPOINT_PATH>.pt`
        assert (checkpoint_pt.suffix == ".pt")
        run_dir = checkpoint_pt.parents[1]

        # Get paths for `config.json`, `dataset_statistics.json` and pretrained checkpoint
        config_json, dataset_statistics_json = run_dir / "config.json", run_dir / "dataset_statistics.json"
        assert config_json.exists(), f"Missing `config.json` for `{run_dir = }`"
        assert dataset_statistics_json.exists(), f"Missing `dataset_statistics.json` for `{run_dir = }`"

    # Otherwise =>> try looking for a match on `model_id_or_path` on the HF Hub (`model_id_or_path`)

        # Load VLA Config (and corresponding base VLM `ModelConfig`) from `config.json`
        with open(config_json, "r") as f:
            vla_cfg = json.load(f) #["vla"]
            # model_cfg = ModelConfig.get_choice_class(vla_cfg["base_vlm"])() #@TODO check æˆ‘è§‰å¾—å…¶å®ä¸é‡è¦ï¼Œ

        # Load Dataset Statistics for Action Denormalization
        with open(dataset_statistics_json, "r") as f:
            norm_stats = json.load(f)
    else:
        overwatch.error(f"âŒ Pretrained checkpoint `{pretrained_checkpoint}` does not exist.")
        raise FileNotFoundError(f"Pretrained checkpoint `{pretrained_checkpoint}` does not exist.")
    return vla_cfg, norm_stats

def load_from_pretrained(pretrained_checkpoint):
    """Load a pretrained QwenQFormerDiT model from a checkpoint."""

    # TODO è¿™é‡Œåº”è¯¥æ˜¯ä»configä¸­åŠ è½½
    
    model = QwenQFormerDiT.from_pretrained(
        pretrained_checkpoint=pretrained_checkpoint)
    return model


if __name__ == "__main__":
    from omegaconf import OmegaConf
    # æ¨¡å‹å‚æ•°
    import debugpy
    debugpy.listen(("0.0.0.0", 5678))
    print("ğŸ” Rank 0 waiting for debugger attach on port 5878...")
    debugpy.wait_for_client()
    samples = {}

    config_yaml = "llavavla/conf/qwenvla_cotrain_v2.yaml"
    cfg = OmegaConf.load(config_yaml)

    model_framework = build_model_framework(cfg)
    model_framework(samples)
    pass

    # git remote add gitee https://gitee.pjlab.org.cn/L2/MultimodalVLA/llavavla.git
    # git push -u gitee master