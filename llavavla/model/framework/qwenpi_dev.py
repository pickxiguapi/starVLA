"""
cogactvla.py

"""
from __future__ import annotations
from typing import Union, List

import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from types import SimpleNamespace
import torch, json
import torch.nn as nn
import numpy as np
from PIL import Image
import re
from prismatic.overwatch import initialize_overwatch

from llavavla.model.action_model.action_model import ActionModel
import torch.distributed as dist
# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100

def dict_to_namespace(d):
    if isinstance(d, dict):
        return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})
    elif isinstance(d, list):
        return [dict_to_namespace(i) for i in d]
    else:
        return d

# get QWen2.5
from llavavla.model.tools import auto_get_module_keys, auto_get_trainable_modules # åç»­åº”è¯¥æ˜¯trainer çš„èŒè´£èŒƒå›´
from llavavla.model.vlm.QWen2_5 import get_qwen2_5_interface
from llavavla.model.projector.QFormer import get_layerwise_qformer
from llavavla.model.action_model.action_model import get_action_model 

class QwenQFormerDiT(nn.Module):
    def __init__(
        self,
        config: Optional[dict] = None,  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config, ä½†æ˜¯ç°åœ¨æ˜¯ç›´æ¥ä¼ å…¥å‚æ•°
        norm_stats: Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]] = None,
        **kwargs,
    ) -> None:
        super().__init__()
        self.config = config

        # TODO å…¨éƒ¨è½¬ å…¨å±€config, è¦é¢å‘å¯¹è±¡ç¼–ç¨‹
        self.qwen_vl_interface = get_qwen2_5_interface(model_id=config.framework.qwenvl.base_vlm, config=self.config) 
        self.layer_qformer = get_layerwise_qformer(config=self.config) # @Jinhui ä¸€èˆ¬æ¥è¯´ äººä»¬å–œæ¬¢æ€»åˆ†ç»“æ„ï¼Œ ä½†æ˜¯æœ‰è®¨åŒé€’å½’ï¼Œ å®éªŒframework ä¸‹é¢å°±ä¸èƒ½å¤ªæ€»åˆ†äº†
        self.action_model = get_action_model(config=self.config)
        
       
        # TODO ä¸ºä»€ä¹ˆè¦åœ¨è¿™ä¸ªä½ç½®å¼€å§‹ çœ‹åˆ° è¿™äº›ï¼Ÿ--> å»æ€è€ƒï¼Œ framework level ç”¨æˆ·å…¶ä»–çœ‹åˆ°ä»€ä¹ˆï¼Œ éœ€è¦çœ‹åˆ°ä»€ä¹ˆ
        self.future_action_window_size = config.framework.action_model.future_action_window_size
        self.past_action_window_size = config.framework.action_model.past_action_window_size

        # self.all_module_keys = auto_get_module_keys(self) #  TODO è¿™ä¸ªæ˜¯trainerçš„ funxï¼Œ æˆ–è®¸æ˜¯å¤šä½™çš„
        self.norm_stats = norm_stats # è¿™ä¸ªæ˜¯ inference æ—¶å€™ç”¨åˆ°çš„ï¼Œ ä¸åº”è¯¥æ˜¯æ”¾åˆ°è¿™ä¸ªä½ç½®ï¼Ÿ
        self.use_ema = config.framework.action_model.use_ema


    @property
    def trainable_module_keys(self) -> List[str]:

        # TODO check, åŸç‰ˆè¿”å›çš„æ­» vlm.model, æ–°çš„å®ç°æ˜¯vlm --> çœ‹ä¸€ä¸‹ä¿å­˜é€»è¾‘æ˜¯å¦å‘ä¸Šå˜åŒ–
        keys = auto_get_trainable_modules(self, max_depth=1)# auto å»åˆ¤æ–­å“ªäº›moduleæ˜¯trainableçš„
        return keys
    

    def forward( # TODO éœ€è¦å°† loss è®¡ç®—åˆ†ç¦»å‡ºæ¥
        self, # åªé¢å¯¹æœ€åŸå§‹çš„ data exmaples, ä¸ºäº†å¯è¯»æ€§ï¼Œè¿™é‡Œè¿˜æ˜¯è¦å†™æˆæ˜¾ç¤ºçš„å‚æ•°
        examples: List[dict] = None,  # è¿™é‡Œçš„ examples æ˜¯æŒ‡åŸå§‹çš„è¾“å…¥æ•°æ®
        repeated_diffusion_steps: int = 4,
        **kwargs,  # ğŸ‘ˆ æ•æ·ä»£ç çš„çµæ´»æ€§ï¼Œ å…è®¸ä»»ä½•å½¢å¼çš„ä¼ å‚æ•°
    ) -> Tuple:
        """Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss)."""
        # @Jinhui TBD TODO 
        images = [example["image"] for example in examples]  #  TODO check æ˜¯ä»€ä¹ˆ
        instructions = [example["lang"] for example in examples]  # [B, str]
        actions = [example["action"] for example in examples] #label
        if "solution" in examples[0]:  # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼
            solutions = [example["solution"] for example in examples]  # [B, dict]
        else: #  è¿˜æœ‰if else å’Œæ¨¡å‹å¯é˜…è¯»æ€§çš„ trade off
            solutions = None

        # dist.barrier
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=images, instructions = instructions, solutions=solutions) # @Jinhui TODO å†è€ƒè™‘ä¸€ä¸‹è¿™é‡Œçš„åˆ†æ”¯åˆ†æµåº”è¯¥æœ‰.pyæ§åˆ¶è¿˜æ˜¯ç”± if else
        
        if DEBUG := os.environ.get("DEBUG"):
            _, num_dict = read_mode_config(self.config.trainer.pretrained_checkpoint)
            self.norm_stats = num_dict
            self.predict_action_withCoT(image=images[0], instruction=instructions[0])
            
        with torch.autocast("cuda", dtype=torch.float16):
            # dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åŠ è½½å®Œæ¯•
            qwenvl_outputs = self.qwen_vl_interface( # éƒ½æ˜¯localçš„å‚æ•°å˜åŒ–ï¼Œ ä¸è¦å†™åˆ°config, ä½†æ˜¯ä¸ºäº†ä¿æŒå¯å¤ç°ï¼Œåº”è¯¥æœ‰ä¸ªé»˜è®¤çš„ yaml
                **qwen_inputs, # å…¼å®¹æ€§å’Œå¯è¯»æ€§çš„ trade off
                output_attentions=False, # Flash attention è¿˜ä¸ç¡®å®šæ˜¯å¦æ”¯æŒè¿”å›attentionï¼Œ å®˜æ–¹ä»£ç æœ‰bug
                output_hidden_states=True,
                return_dict=True,
                )
            pass
            # dist.barrier()
        Intern_vlm_loss = qwenvl_outputs.loss # @Jinhui TODO è¿™é‡Œæ˜¯å¯ä»¥study çš„åœ°æ–¹ï¼Œ æ˜¯å¦ training lang
        
        if Intern_vlm_loss is None or torch.isnan(Intern_vlm_loss): # TODO å°†ä¸åŒé€»è¾‘çš„ forward ç½—æ°å†™æˆ if else ä¼šç ´åå¯è¯»æ€§
            Intern_vlm_loss = torch.tensor(0.0, device=self.qwen_vl_interface.model.device)

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.framework.layer_qformer.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[start_layer:end_layer]) # [B, 64, D_action]
    
        # actions = torch.stack([torch.tensor(a) for a in actions], dim=0).to(action_latent_feature.device)  # [B, chunk, 7] @Jinhui TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
        # å…ˆå°† actions è½¬æ¢ä¸ºå•ä¸ª NumPy æ•°ç»„ï¼Œå†è½¬æ¢ä¸º PyTorch å¼ é‡
        actions = torch.tensor(np.array(actions), device=action_latent_feature.device)  # [B, chunk, 7] TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
        actions_future = actions[:, -(self.future_action_window_size+1):, :]
        
        # Repeat 'actions' 'repeated_diffusion_steps' times, resulting in [repeated_diffusion_steps*B, T, D]
        actions_repeated = actions_future.repeat(repeated_diffusion_steps, 1, 1)
        action_latent_feature = action_latent_feature.repeat(repeated_diffusion_steps, 1, 1)  # [repeated_diffusion_steps*B, T, D_action]
        # Action model forward and compute loss # è¿™é‡ŒåŠŸèƒ½æœ‰ç‚¹ è¶Šä¿ä»£åº– TODO å°†loss é›†ä¸­åˆ° main moduleä¸­ç»Ÿä¸€å¤„ç†
        action_loss = self.action_model.loss(actions_repeated, action_latent_feature) # TODO loss åº”è¯¥æ”¾åˆ°å¦ä¸€ä¸ªå‡½æ•°
        return action_loss, Intern_vlm_loss

    # @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action( # 
        self, image: Union[Image, List[Image]],
        instruction: str, 
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference; maps input image and task instruction to continuous action.

        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        if not isinstance(image, list):
            imgs = [image.resize((224, 224))]  # list of PIL RGB for one instruction
        else:
            imgs = [img.resize((224, 224)) for img in image]
        
        lang = instruction.lower() 

        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=[imgs], instructions = [lang]) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        
        # add by Jinhui

        # end add by Jinhui
        # Generate cognition feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface( # TODO è¿™é‡Œä¹‹åè¦ç”¨generation func
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                output_hidden_states=True, 
                return_dict=True,
            ) # generation æ‹¿ä¸åˆ°å‰é¢token çš„ä¿¡æ¯ï¼Œè€ƒè™‘ä½¿ç”¨ forward?

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.framework.layer_qformer.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[start_layer:end_layer]) # [B, 64, D_action]
            
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = action_latent_feature.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0) #[2,16,7]
            uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
            uncondition_shape = uncondition.shape
            uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
            uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
            z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=action_latent_feature)
            sample_fn = self.action_model.net.forward
        
        # if os.environ.get("DEBUG"):
        #     print(z .shape)
        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=action_latent_feature.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()

        # Un-normalize Actions        
        action_norm_stats = self.get_action_stats(unnorm_key)
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high, action_low = np.array(action_norm_stats["q99"]), np.array(action_norm_stats["q01"])
        normalized_actions = np.clip(normalized_actions, -1, 1)
        normalized_actions[:, 6] = np.where(normalized_actions[:, 6] < 0.5, 0, 1) 
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions,
        )
        # actions max 1, min -0.05 # æ„Ÿè§‰ä¸å†ä¸€ä¸ª scale
        return actions, normalized_actions

    # @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action_withCoT( # 
        self, image: Union[Image, List[Image]],
        instruction: str, 
        solution: Union[Dict, List[Dict]] = None, # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼, å¯ä»¥ç”¨äºå‡ºä¸­é—´è¡¨å¾çš„è¯„æµ‹ï¼Ÿ
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        if not isinstance(image, list):
            imgs = [image.resize((224, 224))]  # list of PIL RGB for one instruction
        else:
            imgs = [img.resize((224, 224)) for img in image]
        
        lang = instruction.lower() 
        
        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=[imgs], instructions = [lang]) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        # Generate feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface.model.generate(
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, 
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                output_hidden_states=True,
                 max_new_tokens=256,
                return_dict_in_generate=True,
            ) 
            # for check output format
            decoded_sequences = self.qwen_vl_interface.processor.tokenizer.batch_decode(
            qwenvl_outputs.sequences, 
            skip_special_tokens=True 
            )
            print(decoded_sequences[0])

            hidden_states = qwenvl_outputs.hidden_states # [num_layers, batch_size, 1 + new token, hidden_dim]

            # è¿™é‡Œè¦å°†ç”Ÿæˆçš„tokenæ‹¼æ¥å›æ¥
            prefix_hidden_states = hidden_states[0]  # Shape: [num_layers, prefix_len, hidden_dim]
            prefix_hidden_states = torch.stack(prefix_hidden_states, dim=0)  # Shape: [num_layers, B, prefix_len, hidden_dim]
            
            # Step 1: Convert list of lists to a tensor [num_new_tokens, num_layers, 1, hidden_dim]
            new_hidden_states = torch.stack([
                torch.stack(layer_hiddens, dim=0) 
                for layer_hiddens in hidden_states[1:]
            ], dim=0)
            
            # Step 2: Remove singleton dimension and transpose to [num_layers, B, num_new_tokens, hidden_dim]
            new_hidden_states = new_hidden_states.squeeze(2).permute(1,2,0,3)  # [num_layers, B, num_new_tokens, hidden_dim]
            
            # Concatenate prefix and new tokens
            combined_hidden_states = torch.cat([
                prefix_hidden_states,  # [num_layers, B, prefix_len, hidden_dim]
                new_hidden_states     # [num_layers, B, num_new_tokens, hidden_dim]
            ], dim=2)  # Shape: [num_layers, B, total_len, hidden_dim]

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer
            end_layer = self.config.framework.layer_qformer.qformer_end_layer
            latent_features = []
            # TODO ä¸Šé¢ä¸ºå¯è¯»æ€§ï¼Œç‰ºç‰²äº†é€Ÿåº¦, ç¨³å®šåå¯ä»¥è€ƒè™‘ åªè½¬æ¢éœ€è¦ç”¨çš„feature
            for i in range(start_layer, end_layer):
                latent_features.append(combined_hidden_states[i]) # 
            action_latent_feature = self.layer_qformer(latent_features) # [B, 64, D_action]
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = action_latent_feature.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0) #[2,16,7]
            uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
            uncondition_shape = uncondition.shape
            uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
            uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
            z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=action_latent_feature)
            sample_fn = self.action_model.net.forward
        
        # if os.environ.get("DEBUG"):
        #     print(z .shape)
        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=action_latent_feature.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()
        # Un-normalize Actions --> è¿™ä¸ªä¿¡æ¯åº”è¯¥é›†æˆåœ¨å“ªé‡Œï¼Œèƒ½å¤Ÿèƒ½å¤Ÿå–æ¶ˆåŠ¨æ€
        return normalized_actions, normalized_actions # TODO Debug with stats is dim=7

    @classmethod
    def from_pretrained( # @Jinhui TODO è¿™é‡Œè¦å†™å¦‚ä½•resume checkpoints
        cls,
        pretrained_checkpoint: str,
        **kwargs,
    ) -> None:
        pretrained_checkpoint = Path(pretrained_checkpoint)
        model_config, norm_stats = read_mode_config(pretrained_checkpoint) # è¯»å– config å’Œ norm_stats
        # Initialize CogACT
        # model_config TODO DEBUE @JinhuiYE è¿™é‡Œåº”è¯¥ä¿è¯training infer çš„å‚æ•°å’Œæ¨¡å‹ğŸ”—æ˜¯ä¸€è‡´çš„ ï¼ˆç‰¹åˆ«æ˜¯ QFormer)
        # TODO 
        config = dict_to_namespace(model_config)
        model_config = config # TODO ä¸è¦ä½¿ç”¨ç›¸å¯¹å˜é‡ model_configï¼Œ éœ€è¦æ¢åå­—
        model_config.trainer.pretrained_checkpoint = None # ä¸ºäº†åŠ å¿«åŠ è½½é€Ÿåº¦ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œ TODO å…¶å®ä¸åº”è¯¥åœ¨initialçš„ä½ç½®è®¾ç½® load_pretrained_backbones
        qwenQFormerACT = build_model_framework(model_config) 
        # set for action un-norm
        qwenQFormerACT.norm_stats = norm_stats
        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)
        model_state_dict = torch.load(pretrained_checkpoint, map_location="cpu") #["model"]
        
        model_keys = set(qwenQFormerACT.state_dict().keys())
        checkpoint_keys = set(model_state_dict.keys())

        # âœ… 1. åŠ è½½åŒ¹é…çš„æƒé‡
        for key in checkpoint_keys:
            if key in model_keys:
                try:
                    qwenQFormerACT.state_dict()[key].copy_(model_state_dict[key])
                    # overwatch.info(f"âœ… Loaded: {key}")
                except Exception as e:
                    overwatch.warning(f"âš ï¸ Failed to copy weight for key '{key}': {e}")
            else:
                overwatch.warning(f"âš ï¸ Checkpoint has unknown key '{key}' (not in model). Ignoring.")

        # âœ… 2. åå‘æ£€æŸ¥ï¼šæ¨¡å‹ä¸­æœ‰ä½† checkpoint ä¸­ç¼ºå¤±çš„
        missing_keys = model_keys - checkpoint_keys # TODO è¿™é‡Œä¹‹åè¦è€ƒè™‘ nontrainable params --> æˆ‘è§‰å¾—æ²¡å¿…è¦çœå­˜å‚¨ç©ºé—´
        for key in sorted(missing_keys):
                overwatch.warning(f"âš ï¸ Model expects key '{key}' but it's missing in checkpoint.")
                
        return qwenQFormerACT
    
    @staticmethod
    def _check_unnorm_key(norm_stats, unnorm_key):
        if unnorm_key is None:
            assert len(norm_stats) == 1, (
                f"Your model was trained on more than one dataset, "
                f"please pass a `unnorm_key` from the following options to choose the statistics "
                f"used for un-normalizing actions: {norm_stats.keys()}"
            )
            unnorm_key = next(iter(norm_stats.keys()))

        assert unnorm_key in norm_stats, (
            f"The `unnorm_key` you chose is not in the set of available dataset statistics, "
            f"please choose from: {norm_stats.keys()}"
        )
        return unnorm_key

    def get_action_dim(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return len(self.norm_stats[unnorm_key]["action"]["q01"])

    def get_action_stats(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return self.norm_stats[unnorm_key]["action"]

# TODO å†™ä¸€ä¸ªbuild model å‡½æ•°

def build_model_framework(config: dict = {}) -> QwenQFormerDiT:
    # TODO  å®ç°å’Œ config å¯¹åº”çš„ load é€»è¾‘

    model = QwenQFormerDiT(config=config)

    return model


def read_mode_config(pretrained_checkpoint):
    if os.path.isfile(pretrained_checkpoint):
        overwatch.info(f"Loading from local checkpoint path `{(checkpoint_pt := Path(pretrained_checkpoint))}`")

        # [Validate] Checkpoint Path should look like `.../<RUN_ID>/checkpoints/<CHECKPOINT_PATH>.pt`
        assert (checkpoint_pt.suffix == ".pt")
        run_dir = checkpoint_pt.parents[1]

        # Get paths for `config.json`, `dataset_statistics.json` and pretrained checkpoint
        config_json, dataset_statistics_json = run_dir / "config.json", run_dir / "dataset_statistics.json"
        assert config_json.exists(), f"Missing `config.json` for `{run_dir = }`"
        assert dataset_statistics_json.exists(), f"Missing `dataset_statistics.json` for `{run_dir = }`"

    # Otherwise =>> try looking for a match on `model_id_or_path` on the HF Hub (`model_id_or_path`)

        # Load VLA Config (and corresponding base VLM `ModelConfig`) from `config.json`
        with open(config_json, "r") as f:
            vla_cfg = json.load(f) #["vla"]
            # model_cfg = ModelConfig.get_choice_class(vla_cfg["base_vlm"])() #@TODO check æˆ‘è§‰å¾—å…¶å®ä¸é‡è¦ï¼Œ

        # Load Dataset Statistics for Action Denormalization
        with open(dataset_statistics_json, "r") as f:
            norm_stats = json.load(f)
    else:
        overwatch.error(f"âŒ Pretrained checkpoint `{pretrained_checkpoint}` does not exist.")
        raise FileNotFoundError(f"Pretrained checkpoint `{pretrained_checkpoint}` does not exist.")
    return vla_cfg, norm_stats

def load_from_pretrained(pretrained_checkpoint):
    """Load a pretrained QwenQFormerDiT model from a checkpoint."""

    # TODO è¿™é‡Œåº”è¯¥æ˜¯ä»configä¸­åŠ è½½
    
    model = QwenQFormerDiT.from_pretrained(
        pretrained_checkpoint=pretrained_checkpoint)
    return model


if __name__ == "__main__":
    from omegaconf import OmegaConf
    # æ¨¡å‹å‚æ•°
    import debugpy
    debugpy.listen(("0.0.0.0", 10092))
    print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
    debugpy.wait_for_client()
    samples = {}

    config_yaml = "llavavla/conf/qwenvla_cotrain_dev.yaml"
    cfg = OmegaConf.load(config_yaml)

    model_framework = build_model_framework(cfg)
    print(model_framework)
    # model_framework(samples)
    pass

    # git remote add gitee https://gitee.pjlab.org.cn/L2/MultimodalVLA/llavavla.git
    # git push -u gitee master