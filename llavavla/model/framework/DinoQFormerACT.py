"""
cogactvla.py

"""
from __future__ import annotations
from typing import Union, List
import torchvision
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from types import SimpleNamespace
import torch, json
import torch.nn as nn
import numpy as np
from PIL import Image
import re
from prismatic.overwatch import initialize_overwatch

from llavavla.model.action_model.action_model import ActionModel
import torch.distributed as dist

# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100

from llavavla.model.framework.share_tools import dict_to_namespace


# get QWen2.5
from llavavla.model.tools import auto_get_module_keys, auto_get_trainable_modules # åç»­åº”è¯¥æ˜¯trainer çš„èŒè´£èŒƒå›´
from llavavla.model.vlm.QWen2_5 import get_qwen2_5_interface
from llavavla.model.projector.QFormer import get_layerwise_qformer
from llavavla.model.action_model.action_model import get_action_model
from llavavla.training.metrics import resize_images
from llavavla.model.vlm.dino import get_dino_model


class QwenQFormerDiT(nn.Module):
    def __init__(
        self,
        config: Optional[dict] = None,  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config, ä½†æ˜¯ç°åœ¨æ˜¯ç›´æ¥ä¼ å…¥å‚æ•°
        norm_stats: Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]] = None,
        **kwargs,
    ) -> None:
        super().__init__()
        self.config = config

        # TODO å…¨éƒ¨è½¬ å…¨å±€config, è¦é¢å‘å¯¹è±¡ç¼–ç¨‹
        self.qwen_vl_interface = get_qwen2_5_interface(model_id=config.framework.qwenvl.base_vlm, config=self.config) 
        self.layer_qformer = get_layerwise_qformer(config=self.config) # @Jinhui ä¸€èˆ¬æ¥è¯´ äººä»¬å–œæ¬¢æ€»åˆ†ç»“æ„ï¼Œ ä½†æ˜¯æœ‰è®¨åŒé€’å½’ï¼Œ å®éªŒframework ä¸‹é¢å°±ä¸èƒ½å¤ªæ€»åˆ†äº†
        self.action_model = get_action_model(config=self.config)
        self.dino_encoder = get_dino_model(backone_name=getattr(self.config.framework.dino, "dino_backbone", "dinov2_vits14"))
        # map dino feature to self.qwen_vl_interface.model.config.hidden_size
        self.dino_pro = nn.Linear(
            in_features=self.dino_encoder.num_channels,  # DINO è¾“å‡ºçš„ç‰¹å¾ç»´åº¦  
            out_features=self.qwen_vl_interface.model.config.hidden_size)
        
        # get dino
        
        
        # TODO ä¸ºä»€ä¹ˆè¦åœ¨è¿™ä¸ªä½ç½®å¼€å§‹ çœ‹åˆ° è¿™äº›ï¼Ÿ--> å»æ€è€ƒï¼Œ framework level ç”¨æˆ·å…¶ä»–çœ‹åˆ°ä»€ä¹ˆï¼Œ éœ€è¦çœ‹åˆ°ä»€ä¹ˆ
        self.future_action_window_size = config.framework.action_model.future_action_window_size
        self.past_action_window_size = config.framework.action_model.past_action_window_size

        # self.all_module_keys = auto_get_module_keys(self) #  TODO è¿™ä¸ªæ˜¯trainerçš„ funxï¼Œ æˆ–è®¸æ˜¯å¤šä½™çš„
        self.norm_stats = norm_stats # è¿™ä¸ªæ˜¯ inference æ—¶å€™ç”¨åˆ°çš„ï¼Œ ä¸åº”è¯¥æ˜¯æ”¾åˆ°è¿™ä¸ªä½ç½®ï¼Ÿ
        self.use_ema = config.framework.action_model.use_ema


    @property
    def trainable_module_keys(self) -> List[str]:

        # TODO check, åŸç‰ˆè¿”å›çš„æ­» vlm.model, æ–°çš„å®ç°æ˜¯vlm --> çœ‹ä¸€ä¸‹ä¿å­˜é€»è¾‘æ˜¯å¦å‘ä¸Šå˜åŒ–
        keys = auto_get_trainable_modules(self, max_depth=1)# auto å»åˆ¤æ–­å“ªäº›moduleæ˜¯trainableçš„
        return keys
    

    def forward( # TODO éœ€è¦å°† loss è®¡ç®—åˆ†ç¦»å‡ºæ¥
        self, # åªé¢å¯¹æœ€åŸå§‹çš„ data exmaples, ä¸ºäº†å¯è¯»æ€§ï¼Œè¿™é‡Œè¿˜æ˜¯è¦å†™æˆæ˜¾ç¤ºçš„å‚æ•°
        examples: List[dict] = None,  # è¿™é‡Œçš„ examples æ˜¯æŒ‡åŸå§‹çš„è¾“å…¥æ•°æ®
        repeated_diffusion_steps: int = 4,
        **kwargs,  # ğŸ‘ˆ æ•æ·ä»£ç çš„çµæ´»æ€§ï¼Œ å…è®¸ä»»ä½•å½¢å¼çš„ä¼ å‚æ•°
    ) -> Tuple:
        """Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss)."""
        # @Jinhui TBD TODO 
        images = [example["image"] for example in examples]  #  [Bï¼Œ[PLT]]
        instructions = [example["lang"] for example in examples]  # [B, str]
        actions = [example["action"] for example in examples] #label [Bï¼Œ len, 7]
        if "solution" in examples[0]:  # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼
            solutions = [example["solution"] for example in examples]  # [B, dict]
        else: #  è¿˜æœ‰if else å’Œæ¨¡å‹å¯é˜…è¯»æ€§çš„ trade off
            solutions = None

        # Step 1: ä½¿ç”¨ DINO imaga processing æ˜¯ä»€ä¹ˆï¼Ÿ
        # å°† PIL å›¾ç‰‡è½¬æ¢ä¸ºå¼ é‡å¹¶ **å½’ä¸€åŒ–**
        image_tensors = self.dino_encoder.prepare_dino_input(images) # 
        B = len(images)
        dino_features = self.dino_encoder(image_tensors)  # DINO è¾“å‡ºä¸º [B*num_view, token, dim]

        # æå– DINO çš„ç‰¹å¾
        dino_encoded_features = dino_features.reshape(B, -1, dino_features.shape[-1])  # [B, num_view * token, dim]

        dino_encoded_features = self.dino_pro(dino_encoded_features) # [B, num_view * token, hidden_size]
        # Step 2: QWenVL è¾“å…¥æ ¼å¼
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=images, instructions = instructions, solutions=solutions) # @Jinhui TODO å†è€ƒè™‘ä¸€ä¸‹è¿™é‡Œçš„åˆ†æ”¯åˆ†æµåº”è¯¥æœ‰.pyæ§åˆ¶è¿˜æ˜¯ç”± if else
        
        with torch.autocast("cuda", dtype=torch.bfloat16): # @Jinhui TODO è¿™é‡Œçš„ dtype æ˜¯ä¸æ˜¯åº”è¯¥æ˜¯configé‡Œé¢çš„
            # dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åŠ è½½å®Œæ¯•
            qwenvl_outputs = self.qwen_vl_interface( # éƒ½æ˜¯localçš„å‚æ•°å˜åŒ–ï¼Œ ä¸è¦å†™åˆ°config, ä½†æ˜¯ä¸ºäº†ä¿æŒå¯å¤ç°ï¼Œåº”è¯¥æœ‰ä¸ªé»˜è®¤çš„ yaml
                **qwen_inputs, # å…¼å®¹æ€§å’Œå¯è¯»æ€§çš„ trade off
                output_attentions=False, # Flash attention è¿˜ä¸ç¡®å®šæ˜¯å¦æ”¯æŒè¿”å›attentionï¼Œ å®˜æ–¹ä»£ç æœ‰bug
                output_hidden_states=True,
                return_dict=True,
                )
            pass
            # dist.barrier()
        action_cot_loss = qwenvl_outputs.loss # @Jinhui TODO è¿™é‡Œæ˜¯å¯ä»¥study çš„åœ°æ–¹ï¼Œ æ˜¯å¦ training lang
        
        if action_cot_loss is None or torch.isnan(action_cot_loss): # TODO å°†ä¸åŒé€»è¾‘çš„ forward ç½—æ°å†™æˆ if else ä¼šç ´åå¯è¯»æ€§
            action_cot_loss = torch.tensor(0.0, device=self.qwen_vl_interface.model.device)

        with torch.autocast("cuda", dtype=torch.float32):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.framework.layer_qformer.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            condition_features = qwenvl_outputs.hidden_states[start_layer:end_layer]
            
            # ç»™ DiT æ·»åŠ æ¥è‡ªdino çš„è¾“å…¥
            cat_conditions = []
            for layer_index in range(len(condition_features)):
                layer_features = condition_features[layer_index]  # [B, n_qformer_token, D]
                layer_features = torch.cat([layer_features, dino_encoded_features], dim=1)  # [B, n_qformer_token + num_view * token, D] 
                cat_conditions.append(layer_features)

            action_latent_feature = self.layer_qformer(cat_conditions) # --> [B, 64, D_action]
    
            # actions = torch.stack([torch.tensor(a) for a in actions], dim=0).to(action_latent_feature.device)  # [B, chunk, 7] @Jinhui TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
            # å…ˆå°† actions è½¬æ¢ä¸ºå•ä¸ª NumPy æ•°ç»„ï¼Œå†è½¬æ¢ä¸º PyTorch å¼ é‡
            actions = torch.tensor(np.array(actions), device=action_latent_feature.device)  # [B, chunk, 7] TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
            actions_future = actions[:, -(self.future_action_window_size+1):, :]
            
            # Repeat 'actions' 'repeated_diffusion_steps' times, resulting in [repeated_diffusion_steps*B, T, D]
            actions_repeated = actions_future.repeat(repeated_diffusion_steps, 1, 1)
            action_latent_feature = action_latent_feature.repeat(repeated_diffusion_steps, 1, 1)  # [repeated_diffusion_steps*B, T, D_action]
            # Action model forward and compute loss # è¿™é‡ŒåŠŸèƒ½æœ‰ç‚¹ è¶Šä¿ä»£åº– TODO å°†loss é›†ä¸­åˆ° main moduleä¸­ç»Ÿä¸€å¤„ç†
            action_loss = self.action_model.loss(actions_repeated, action_latent_feature) # TODO loss åº”è¯¥æ”¾åˆ°å¦ä¸€ä¸ªå‡½æ•°
        return action_loss, action_cot_loss

    @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action( # 
        self, image: Union[Image, List[Image]],
        instruction: str,
        unnorm_key: Optional[str] = None,
        cfg_scale: float = 1.5,
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference; maps input image and task instruction to continuous action.

        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        if not isinstance(image, list):
            imgs = [image.resize((224, 224))]  # list of PIL RGB for one instruction
        else:
            imgs = [img.resize((224, 224)) for img in image]
        images = [imgs]

        lang = instruction.lower() 
        instructions = [lang]  # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼ï¼Œ éœ€è¦è€ƒè™‘æ˜¯å¦è¦å°†lang å˜æˆ list
        
        

        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=images, instructions = instructions) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        # Generate cognition feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface( # TODO è¿™é‡Œä¹‹åè¦ç”¨generation func
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                output_hidden_states=True, 
                return_dict=True,
            ) # generation æ‹¿ä¸åˆ°å‰é¢token çš„ä¿¡æ¯ï¼Œè€ƒè™‘ä½¿ç”¨ forward?

        # get dino feature
        image_tensors = self.dino_encoder.prepare_dino_input(images)
        dino_features = self.dino_encoder(image_tensors)  # DINO è¾“å‡ºä¸º OrderedDict

        # æå– DINO çš„ç‰¹å¾
        B = dino_features.shape[0]
        dino_encoded_features = dino_features.view(B, -1,dino_features.shape[-1])  # [B, num_view * token, dim]
        dino_encoded_features = self.dino_pro(dino_encoded_features) # B, 256, D

        with torch.autocast("cuda", dtype=torch.float32):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.framework.layer_qformer.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            condition_features = qwenvl_outputs.hidden_states[start_layer:end_layer]
            
            # ç»™ DiT æ·»åŠ æ¥è‡ªdino çš„è¾“å…¥
            cat_conditions = []
            for layer_index in range(len(condition_features)):
                layer_features = condition_features[layer_index]  # [B, n_qformer_token, D]
                layer_features = torch.cat([layer_features, dino_encoded_features], dim=1)  # [B, n_qformer_token + num_view * token, D] 
                cat_conditions.append(layer_features)

            action_latent_feature = self.layer_qformer(cat_conditions) # --> [B, 64, D_action]
    
            using_cfg = cfg_scale > 1.0

            model_dtype = next(self.action_model.net.parameters()).dtype
            B = action_latent_feature.shape[0]

            # Sample random noise
            noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

            # Setup classifier-free guidance:
            if using_cfg:
                noise = torch.cat([noise, noise], 0) #[2,16,7]
                uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
                uncondition_shape = uncondition.shape
                uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
                uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
                z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
                cfg_scale = cfg_scale
                model_kwargs = dict(z=z, cfg_scale=cfg_scale)
                sample_fn = self.action_model.net.forward_with_cfg
            else:
                model_kwargs = dict(z=action_latent_feature)
                sample_fn = self.action_model.net.forward
            
            # if os.environ.get("DEBUG"):
            #     print(z .shape)
            # DDIM Sampling
            if use_ddim and num_ddim_steps is not None:
                if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                    self.action_model.create_ddim(ddim_step=num_ddim_steps)
                samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device,
                                                                    eta=0.0
                                                                    )
            else:
                # DDPM Sampling
                samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                        noise.shape, 
                                                                        noise, 
                                                                        clip_denoised=False,
                                                                        model_kwargs=model_kwargs,
                                                                        progress=False,
                                                                        device=action_latent_feature.device
                                                                        )
            if using_cfg:
                samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
            normalized_actions = samples[0].cpu().numpy()
            
            # Un-normalize Actions --> è¿™ä¸ªä¿¡æ¯åº”è¯¥é›†æˆåœ¨å“ªé‡Œï¼Œèƒ½å¤Ÿèƒ½å¤Ÿå–æ¶ˆåŠ¨æ€
            action_norm_stats = self.get_action_stats(unnorm_key)
            raw_actions = self.unnormalize_actions(normalized_actions=normalized_actions, action_norm_stats=action_norm_stats) 
        return raw_actions, normalized_actions # actions, normalized_actions #TODO å¾—æƒ³æ¸…æ¥šï¼Œ Un-normalize Actions  åˆ°åº•æ˜¯è°æ§åˆ¶çš„ã€‚ æˆ‘è§‰å¾—å¿…é¡»æ˜¯æ¨¡å‹ï¼Œ å› ä¸ºå‡å°‘ç›¸å¯¹å˜åŒ–ï¼Œ æ‰å¹³ç®¡ç†ã€‚ ä½†æ˜¯è¦å•ç‹¬å†™æˆå‡½æ•°

    @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action_withCoT( # TODO å’Œ predict action æ˜¯æœ‰å·®åˆ«äº†ï¼Œä¹‹åè¦å»è§£é‚£è¾¹ä¸ºæƒ…å†µ# é‚£è¾¹æš‚æ—¶æ˜¯for windox
        self, images: Union[Image, List[Image]], # å˜æˆå¯ä»¥ batch infer
        instructions: List[str], 
        solutions: Union[Dict, List[Dict]] = None, # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼, å¯ä»¥ç”¨äºå‡ºä¸­é—´è¡¨å¾çš„è¯„æµ‹ï¼Ÿ
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        batch_images = resize_images(images, target_size=(224, 224))  # list of PIL RGB for one instruction
        
        # instructions = [instruction.lower()  for instruction in instructions] # @Jinhui TODO è¿™é‡Œæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„æ ¼å¼ï¼Œ éœ€è¦è€ƒè™‘æ˜¯å¦è¦å°†lang å˜æˆ list
        
        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=batch_images, instructions=instructions) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        # Generate feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            # TODO Qwen èƒŒåä¼šå¤šä¸€ä¸ª ç»ˆæ­¢ç¬¦å·ï¼Œ éœ€è¦ [:, :-2] å»æ‰ã€‚æ˜¯éœ€è¦æ€è€ƒæ˜¯å¦ä½¿ç”¨ v1 ç‰ˆæœ¬æ¥äº§ç”Ÿ å¯¹åº”çš„token
            qwenvl_outputs = self.qwen_vl_interface.model.generate(
                input_ids=qwen_inputs.input_ids[:, :-2],
                attention_mask=qwen_inputs.attention_mask[:, :-2],
                pixel_values=qwen_inputs.pixel_values, 
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                output_hidden_states=True,
                max_new_tokens=256,
                return_dict_in_generate=True,
            )
            # for check output format
            decoded_sequences = self.qwen_vl_interface.processor.tokenizer.batch_decode(
            qwenvl_outputs.sequences, 
            skip_special_tokens=True 
            )
            # print(decoded_sequences[0])

            hidden_states = qwenvl_outputs.hidden_states  # [1 + new token, num_layers, batch_size, attention token, hidden_dim]

            if len(hidden_states) == 1: # è¡¨æ˜æ²¡æœ‰æ–°çš„token æ®‹ç”Ÿ
                # å¦‚æœç”Ÿæˆçš„ token ä¸º 0ï¼Œä»…ä¿ç•™ prefix_hidden_states
                prefix_hidden_states = hidden_states[0]  # Shape: [num_layers, B, prefix_len, hidden_dim]
                prefix_hidden_states = torch.stack(prefix_hidden_states, dim=0)  # Shape: [num_layers, B, prefix_len, hidden_dim]
                combined_hidden_states = prefix_hidden_states  # Shape: [num_layers, B, prefix_len, hidden_dim]
            else: # ä¸ºäº†é€»è¾‘æ¸…æ™°è€Œä½¿ç”¨äº† if else, 
                # æ­£å¸¸å¤„ç†ç”Ÿæˆçš„ token
                prefix_hidden_states = hidden_states[0]  # Shape: [num_layers, B, prefix_len, hidden_dim]
                prefix_hidden_states = torch.stack(prefix_hidden_states, dim=0)  # Shape: [num_layers, B, prefix_len, hidden_dim]

                # Step 1: Convert list of lists to a tensor [num_new_tokens, num_layers, B, 1, hidden_dim]
                new_hidden_states = torch.stack([
                    torch.stack(layer_hiddens, dim=0)
                    for layer_hiddens in hidden_states[1:]
                ], dim=0) # 

                # Step 2: Remove singleton dimension and transpose to [num_layers, B, num_new_tokens, hidden_dim]
                new_hidden_states = new_hidden_states.squeeze(3).permute(1, 2, 0, 3)  # [num_layers, B, num_new_tokens, hidden_dim]

                # Concatenate prefix and new tokens
                combined_hidden_states = torch.cat([
                    prefix_hidden_states,  # [num_layers, B, prefix_len, hidden_dim]
                    new_hidden_states      # [num_layers, B, num_new_tokens, hidden_dim]
                ], dim=2)  # Shape: [num_layers, B, total_len, hidden_dim]
        
        with torch.autocast("cuda", dtype=torch.float32):
            start_layer = self.config.framework.layer_qformer.qformer_start_layer
            end_layer = self.config.framework.layer_qformer.qformer_end_layer
            latent_features = []
            # TODO ä¸Šé¢ä¸ºå¯è¯»æ€§ï¼Œç‰ºç‰²äº†é€Ÿåº¦, ç¨³å®šåå¯ä»¥è€ƒè™‘ åªè½¬æ¢éœ€è¦ç”¨çš„feature
            for i in range(start_layer, end_layer):
                latent_features.append(combined_hidden_states[i]) # 
            action_latent_feature = self.layer_qformer(latent_features) # [B, 64, D_action]
            using_cfg = cfg_scale > 1.0

            model_dtype = next(self.action_model.net.parameters()).dtype
            B = action_latent_feature.shape[0]

            # Sample random noise
            noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

            # Setup classifier-free guidance:
            if using_cfg:
                noise = torch.cat([noise, noise], 0) #[2,16,7]
                uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
                uncondition_shape = uncondition.shape
                uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
                uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
                z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
                cfg_scale = cfg_scale
                model_kwargs = dict(z=z, cfg_scale=cfg_scale)
                sample_fn = self.action_model.net.forward_with_cfg
            else:
                model_kwargs = dict(z=action_latent_feature)
                sample_fn = self.action_model.net.forward
            
            # if os.environ.get("DEBUG"):
            #     print(z .shape)
            # DDIM Sampling
            if use_ddim and num_ddim_steps is not None:
                if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                    self.action_model.create_ddim(ddim_step=num_ddim_steps)
                samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device,
                                                                    eta=0.0
                                                                    )
            else:
                # DDPM Sampling
                samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                        noise.shape, 
                                                                        noise, 
                                                                        clip_denoised=False,
                                                                        model_kwargs=model_kwargs,
                                                                        progress=False,
                                                                        device=action_latent_feature.device
                                                                        )
            if using_cfg:
                samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
            normalized_actions = samples.cpu().numpy()
            # Un-normalize Actions --> è¿™ä¸ªä¿¡æ¯åº”è¯¥é›†æˆåœ¨å“ªé‡Œï¼Œèƒ½å¤Ÿèƒ½å¤Ÿå–æ¶ˆåŠ¨æ€
            # unnorm_key = self.config.datasets.vla_data.data_mix
            # raw_actions = self.unnormalize_actions(normalized_actions, self.norm_stats[unnorm_key]) # TODO è¿™é‡Œåº”è¯¥æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œ ä½†æ˜¯ç°åœ¨æ˜¯æ”¾åœ¨æ¨¡å‹é‡Œé¢ï¼Œ ä¸åº”è¯¥ï¼Œè¿™ä¸ªæ˜¯å’Œå…·ä½“å‡½æ•°ç»‘å®šçš„   
            # TODO unnormalize_actions ä¼šå› ä¸ºæ•°æ®é›†ä¸ä¸€æ ·è€Œä¸ä¸€æ ·ï¼Œ è¿™ä¸ªåŠ¨æ€ä¸åº”è¯¥æ˜¯ frameworkåº”è¯¥æ§åˆ¶çš„ï¼Ÿ ä½†æ˜¯ framework æœ‰å¿…é¡»è¦çŸ¥é“è‡ªå·±çš„ æ˜¯ä»€ä¹ˆactionï¼Ÿ
        return decoded_sequences, normalized_actions # B, action_chunk, action_dim
    
    @staticmethod
    def unnormalize_actions(normalized_actions: np.ndarray, action_norm_stats: Dict[str, np.ndarray]) -> np.ndarray:
        """
        å°†å½’ä¸€åŒ–çš„åŠ¨ä½œè½¬æ¢ä¸ºåŸå§‹åŠ¨ä½œç©ºé—´ã€‚
        
        :param normalized_actions: å½’ä¸€åŒ–çš„åŠ¨ä½œæ•°ç»„ï¼Œå½¢çŠ¶ä¸º [B, T, D]ã€‚
        :param action_norm_stats: åŒ…å«åŠ¨ä½œå½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹é”®ï¼š
            - "q01": åŠ¨ä½œçš„ç¬¬ 1 ç™¾åˆ†ä½å€¼ã€‚
            - "q99": åŠ¨ä½œçš„ç¬¬ 99 ç™¾åˆ†ä½å€¼ã€‚
            - "mask": å¯é€‰ï¼Œå¸ƒå°”æ•°ç»„ï¼Œç”¨äºæ ‡è®°å“ªäº›åŠ¨ä½œéœ€è¦åå½’ä¸€åŒ–ã€‚
        :return: åå½’ä¸€åŒ–åçš„åŠ¨ä½œæ•°ç»„ï¼Œå½¢çŠ¶ä¸è¾“å…¥ `normalized_actions` ç›¸åŒã€‚
        """
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high = np.array(action_norm_stats["q99"])
        action_low = np.array(action_norm_stats["q01"])

        # Clip normalized actions to [-1, 1]
        normalized_actions = np.clip(normalized_actions, -1, 1)

        # ç‰¹æ®Šå¤„ç†ç¬¬ 6 ç»´åº¦çš„åŠ¨ä½œï¼ˆä¾‹å¦‚åˆ†ç±»ä»»åŠ¡ï¼‰
        normalized_actions[:, 6] = np.where(normalized_actions[:, 6] < 0.5, 0, 1) 
        # æ ¹æ® mask å’Œç»Ÿè®¡ä¿¡æ¯è¿›è¡Œåå½’ä¸€åŒ–
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions
        )

        return actions
    @classmethod
    def from_pretrained( # @Jinhui TODO è¿™é‡Œè¦å†™å¦‚ä½•resume checkpoints
        cls,
        pretrained_checkpoint: str,
        **kwargs,
    ) -> None:
        pretrained_checkpoint = Path(pretrained_checkpoint)
        model_config, norm_stats = read_mode_config(pretrained_checkpoint) # è¯»å– config å’Œ norm_stats
        # Initialize CogACT
        # model_config TODO DEBUE @JinhuiYE è¿™é‡Œåº”è¯¥ä¿è¯training infer çš„å‚æ•°å’Œæ¨¡å‹ğŸ”—æ˜¯ä¸€è‡´çš„ ï¼ˆç‰¹åˆ«æ˜¯ QFormer)
        # TODO 
        config = dict_to_namespace(model_config)
        model_config = config # TODO ä¸è¦ä½¿ç”¨ç›¸å¯¹å˜é‡ model_configï¼Œ éœ€è¦æ¢åå­—
        model_config.trainer.pretrained_checkpoint = None # ä¸ºäº†åŠ å¿«åŠ è½½é€Ÿåº¦ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œ TODO å…¶å®ä¸åº”è¯¥åœ¨initialçš„ä½ç½®è®¾ç½® load_pretrained_backbones
        qwenQFormerACT = build_model_framework(model_config) 
        # set for action un-norm
        qwenQFormerACT.norm_stats = norm_stats
        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)
        model_state_dict = torch.load(pretrained_checkpoint, map_location="cpu") #["model"]
        overwatch.info(f"Loading model weights from `{pretrained_checkpoint}`")
        model_keys = set(qwenQFormerACT.state_dict().keys())
        checkpoint_keys = set(model_state_dict.keys())
        
        # âœ… 1. åŠ è½½åŒ¹é…çš„æƒé‡
        for key in checkpoint_keys:
            if key in model_keys:
                try:
                    qwenQFormerACT.state_dict()[key].copy_(model_state_dict[key])
                    # overwatch.info(f"âœ… Loaded: {key}")
                except Exception as e:
                    overwatch.warning(f"âš ï¸ Failed to copy weight for key '{key}': {e}")
            else:
                overwatch.warning(f"âš ï¸ Checkpoint has unknown key '{key}' (not in model). Ignoring.")

        # âœ… 2. åå‘æ£€æŸ¥ï¼šæ¨¡å‹ä¸­æœ‰ä½† checkpoint ä¸­ç¼ºå¤±çš„
        missing_keys = model_keys - checkpoint_keys # TODO è¿™é‡Œä¹‹åè¦è€ƒè™‘ nontrainable params --> æˆ‘è§‰å¾—æ²¡å¿…è¦çœå­˜å‚¨ç©ºé—´
        for key in sorted(missing_keys):
                overwatch.warning(f"âš ï¸ Model expects key '{key}' but it's missing in checkpoint.")
        miss_keys = list(missing_keys) + list(checkpoint_keys - model_keys)

        # **ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š**
        qwenQFormerACT = qwenQFormerACT.to("cuda") # TODO å…¶å®ä¸åº”è¯¥æ˜¯è¿™é‡Œç®¡ç†to GPUçš„
        return qwenQFormerACT
    
    @staticmethod
    def _check_unnorm_key(norm_stats, unnorm_key):
        if unnorm_key is None:
            assert len(norm_stats) == 1, (
                f"Your model was trained on more than one dataset, "
                f"please pass a `unnorm_key` from the following options to choose the statistics "
                f"used for un-normalizing actions: {norm_stats.keys()}"
            )
            unnorm_key = next(iter(norm_stats.keys()))

        assert unnorm_key in norm_stats, (
            f"The `unnorm_key` you chose is not in the set of available dataset statistics, "
            f"please choose from: {norm_stats.keys()}"
        )
        return unnorm_key

    def get_action_dim(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return len(self.norm_stats[unnorm_key]["action"]["q01"])

    def get_action_stats(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return self.norm_stats[unnorm_key]["action"]

# TODO å†™ä¸€ä¸ªbuild model å‡½æ•°

def build_model_framework(config: dict = {}) -> QwenQFormerDiT:
    # TODO  å®ç°å’Œ config å¯¹åº”çš„ load é€»è¾‘

    model = QwenQFormerDiT(config=config)

    return model


def read_mode_config(pretrained_checkpoint):
    if os.path.isfile(pretrained_checkpoint):
        overwatch.info(f"Loading from local checkpoint path `{(checkpoint_pt := Path(pretrained_checkpoint))}`")

        # [Validate] Checkpoint Path should look like `.../<RUN_ID>/checkpoints/<CHECKPOINT_PATH>.pt`
        assert (checkpoint_pt.suffix == ".pt")
        run_dir = checkpoint_pt.parents[1]

        # Get paths for `config.json`, `dataset_statistics.json` and pretrained checkpoint
        config_json, dataset_statistics_json = run_dir / "config.json", run_dir / "dataset_statistics.json"
        assert config_json.exists(), f"Missing `config.json` for `{run_dir = }`"
        assert dataset_statistics_json.exists(), f"Missing `dataset_statistics.json` for `{run_dir = }`"

    # Otherwise =>> try looking for a match on `model_id_or_path` on the HF Hub (`model_id_or_path`)

        # Load VLA Config (and corresponding base VLM `ModelConfig`) from `config.json`
        with open(config_json, "r") as f:
            vla_cfg = json.load(f) #["vla"]
            # model_cfg = ModelConfig.get_choice_class(vla_cfg["base_vlm"])() #@TODO check æˆ‘è§‰å¾—å…¶å®ä¸é‡è¦ï¼Œ

        # Load Dataset Statistics for Action Denormalization
        with open(dataset_statistics_json, "r") as f:
            norm_stats = json.load(f)
    else:
        overwatch.error(f"âŒ Pretrained checkpoint `{pretrained_checkpoint}` does not exist.")
        raise FileNotFoundError(f"Pretrained checkpoint `{pretrained_checkpoint}` does not exist.")
    return vla_cfg, norm_stats

def load_from_pretrained(pretrained_checkpoint):
    """Load a pretrained QwenQFormerDiT model from a checkpoint."""

    # TODO è¿™é‡Œåº”è¯¥æ˜¯ä»configä¸­åŠ è½½
    
    model = QwenQFormerDiT.from_pretrained(
        pretrained_checkpoint=pretrained_checkpoint)
    return model



if __name__ == "__main__":
    from omegaconf import OmegaConf
    # æ¨¡å‹å‚æ•°
    import debugpy
    debugpy.listen(("0.0.0.0", 10092))
    print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
    debugpy.wait_for_client() 
    samples = {}

    config_yaml = "llavavla/conf/qwenvla_cotrain.yaml"
    cfg = OmegaConf.load(config_yaml)

    dino = get_dino_model()
    dino.body.get_image_transform()
    model_framework = build_model_framework(cfg)
    print(model_framework)
    # model_framework(samples)
    pass

    # git remote add gitee https://gitee.pjlab.org.cn/L2/MultimodalVLA/llavavla.git
    # git push -u gitee master