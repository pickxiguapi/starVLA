"""
cogactvla.py

"""

from __future__ import annotations

from pathlib import Path, os
from typing import Dict, List, Optional, Tuple


import torch, json
import torch.nn as nn
import numpy as np
from PIL import Image
import re
from prismatic.overwatch import initialize_overwatch

from llavavla.model.action_model.action_model import ActionModel
import torch.distributed as dist
# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100


# get QWen2.5
from llavavla.model.vlm import _QWen_VL_Interface #ä¸åº”è¯¥å¼ºä¾èµ–äºè¿™ä¸ªï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªæ¥å£ç±»ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„ç±», TODO ä¸è¦å®ç° hard æ¥å£ç±»ï¼Œ ä½¿ç”¨ **kwargs
from llavavla.model.tools import auto_get_module_keys, auto_get_trainable_modules # åç»­åº”è¯¥æ˜¯trainer çš„èŒè´£èŒƒå›´
from llavavla.model.vlm.QWen2_5 import get_qwen2_5_interface
from llavavla.model.projector.QFormer import get_layerwise_qformer

class QwenQFormerDiT(nn.Module):
    def __init__(
        self,
        qwen_model_name:str = './playground/Pretrained_models/Qwen2.5-VL-3B-Instruct', # è¿™æ˜¯ä¸å¥½çš„å®ç°ï¼Œ ä¸€å®šä¸èƒ½æ˜¯äº’ç›¸ä¾èµ–
        action_model_type: str = 'DiT-B', 
        vl_token_dim: int = 2048,
        action_hidden_dim: int = 768,  # @Jinhui # è¿™ä¸ª åº”è¯¥æ˜¯å’ŒDiT-B
        action_dim: int = 7,
        future_action_window_size: int = 15,
        past_action_window_size: int = 0,
        use_ema: bool = False,
        norm_stats: Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]] = None,
        **kwargs,
    ) -> None:
        super().__init__()
        
        # TODO å…¨éƒ¨è½¬ å…¨å±€config, è¦é¢å‘å¯¹è±¡ç¼–ç¨‹
        self.qwen_vl_interface = get_qwen2_5_interface(qwen_model_name) 
        self.layer_qformer = get_layerwise_qformer(input_hidden_dim=vl_token_dim, output_hidden_dim=action_hidden_dim) # @Jinhui éœ€è¦é€»è¾‘ä»QWen ä¸­å¯¹é½ hidden
        self.action_model = ActionModel(model_type = action_model_type,  # TODO @Jinhui åº”è¯¥å†™åˆ° get_action_model()
                                            action_hidden_dim = action_hidden_dim, # è¿™äº›å‚æ•°å…³ç³»è¦ TODOé›†ä¸­ è®¾ç½®åˆ°config
                                            in_channels = action_dim, 
                                            future_action_window_size = future_action_window_size, 
                                            past_action_window_size = past_action_window_size) # ä¹Ÿåº”è¯¥ç”¨ å‡½æ•°å°è£…
        # TODO ActionModel éœ€è¦å’Œqformer ä¸€èµ·è®¾è®¡

        # self.qwen_processor = vlm.processor # è¦é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œ ä¸è¦ å±æ€§å¤–æ³„
        # è¿™äº›æ˜¯ action chunck çš„å‚æ•°
        self.future_action_window_size = future_action_window_size
        self.past_action_window_size = past_action_window_size

        self.all_module_keys = auto_get_module_keys(self) #  TODO è¿™ä¸ªæ˜¯trainerçš„ funx
        self.norm_stats = norm_stats # è¿™ä¸ªæ˜¯ inference æ—¶å€™ç”¨åˆ°çš„ï¼Œ ä¸åº”è¯¥æ˜¯æ”¾åˆ°è¿™ä¸ªä½ç½®ï¼Ÿ


    @property
    def trainable_module_keys(self) -> List[str]:

        # TODO check, åŸç‰ˆè¿”å›çš„æ­» vlm.model, æ–°çš„å®ç°æ˜¯vlm --> çœ‹ä¸€ä¸‹ä¿å­˜é€»è¾‘æ˜¯å¦å‘ä¸Šå˜åŒ–
        keys = auto_get_trainable_modules(self, max_depth=1)# auto å»åˆ¤æ–­å“ªäº›moduleæ˜¯trainableçš„
        return keys
    

    def forward( # TODO éœ€è¦å°† loss è®¡ç®—åˆ†ç¦»å‡ºæ¥
        self, # åªé¢å¯¹æœ€åŸå§‹çš„ data exmaples, ä¸ºäº†å¯è¯»æ€§ï¼Œè¿™é‡Œè¿˜æ˜¯è¦å†™æˆæ˜¾ç¤ºçš„å‚æ•°
        examples: List[dict] = None,  # è¿™é‡Œçš„ examples æ˜¯æŒ‡åŸå§‹çš„è¾“å…¥æ•°æ®
        repeated_diffusion_steps: int = 4,
        **kwargs,  # ğŸ‘ˆ æ•æ·ä»£ç çš„çµæ´»æ€§ï¼Œ å…è®¸ä»»ä½•å½¢å¼çš„ä¼ å‚æ•°
    ) -> Tuple:
        """Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss)."""
        # @Jinhui TBD TODO 
        # pixel_values = pixel_values["pixel_values"] # labeles = pixel_values["labels"]
        # dist.barrier()

        # images: Optional[torch.FloatTensor] = None,
        # instructions: Optional[List] = None,
        # actions: Optional[torch.FloatTensor] = None,
        images = [example["image"] for example in examples]  #  TODO check æ˜¯ä»€ä¹ˆ
        instructions = [example["lang"] for example in examples]  # [B, str]
        actions = [example["action"] for example in examples]
        
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=images, instructions = instructions) # @Jinhui TODO add instruction to qwenvl inputs
        with torch.autocast("cuda", dtype=torch.float16):

            qwenvl_outputs = self.qwen_vl_interface( # éƒ½æ˜¯localçš„å‚æ•°å˜åŒ–ï¼Œ ä¸è¦å†™åˆ°config, ä½†æ˜¯ä¸ºäº†ä¿æŒå¯å¤ç°ï¼Œåº”è¯¥æœ‰ä¸ªé»˜è®¤çš„ yaml
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                # use_cache=use_cache,
                output_attentions=True,
                output_hidden_states=True,
                return_dict=True,
                # past_key_values=past_key_values,
                # **kwargs
                )
        
        vlm_loss = qwenvl_outputs.loss # @Jinhui TODO è¿™é‡Œæ˜¯å¯ä»¥study çš„åœ°æ–¹ï¼Œ æ˜¯å¦ training lang
        with torch.autocast("cuda", dtype=torch.bfloat16):
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[-6:]) # [B, 64, D_action]
            
        actions = torch.stack([torch.tensor(a) for a in actions], dim=0).to(action_latent_feature.device)  # [B, chunk, 7] @Jinhui TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
        actions_future = actions[:, -(self.future_action_window_size+1):, :]
        
        # Repeat 'actions' 'repeated_diffusion_steps' times, resulting in [repeated_diffusion_steps*B, T, D]
        actions_repeated = actions_future.repeat(repeated_diffusion_steps, 1, 1)
        action_latent_feature = action_latent_feature.repeat(repeated_diffusion_steps, 1, 1)  # [repeated_diffusion_steps*B, T, D_action]
        # Action model forward and compute loss # è¿™é‡ŒåŠŸèƒ½æœ‰ç‚¹ è¶Šä¿ä»£åº– TODO å°†loss é›†ä¸­åˆ° main moduleä¸­ç»Ÿä¸€å¤„ç†
        action_loss = self.action_model.loss(actions_repeated, action_latent_feature) # TODO loss åº”è¯¥æ”¾åˆ°å¦ä¸€ä¸ªå‡½æ•°
        return action_loss, qwenvl_outputs

    # @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action(
        self, image: Image, 
        instruction: str, 
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference; maps input image and task instruction to continuous action.

        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        imgs = [image.resize((224, 224))]  # list of PLT RGB for one instruction
        lang = instruction.lower() 

        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=[imgs], instructions = [lang]) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        
        # add by Jinhui

        # end add by Jinhui
        # Generate cognition feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface( # TODO è¿™é‡Œä¹‹åè¦ç”¨generation func
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                output_hidden_states=True, 
                return_dict=True,
            ) # generation æ‹¿ä¸åˆ°å‰é¢token çš„ä¿¡æ¯ï¼Œè€ƒè™‘ä½¿ç”¨ forward?

        with torch.autocast("cuda", dtype=torch.bfloat16):
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[-6:]) # [B, 64, D_action]
            
            # Jinhui see text # outputs.sequences.shape: B, len with prefix
            # outputs.input_ids = outputs.sequences # ä¸ºäº†å’Œ input dict ä¿æŒä¸€è‡´ï¼Œ æ–¹ä¾¿è°ƒç”¨ self._get_cognition_features# è¿˜çœŸä¸å¤ªä¸€æ ·ï¼Œå› ä¸ºgenerationçš„é€»è¾‘å’Œ forwardä¸ä¸€æ ·
            # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs.sequences)]
            # output_text = self.qwen_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            # print("output:\n",output_text[0])
            # fmt: on
            # æˆ‘ä»¬trainingçš„æ—¶å€™æ˜¯ image ä¸å›ºå®šåœ¨æœ€å‰é¢æ²¡ï¼Œæ˜¯æ²¡åŠæ³•åªmax_new = 1 çš„

        
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = action_latent_feature.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0) #[2,16,7]
            uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
            uncondition_shape = uncondition.shape
            uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
            uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
            z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=action_latent_feature)
            sample_fn = self.action_model.net.forward
        
        # if os.environ.get("DEBUG"):
        #     print(z .shape)
        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=action_latent_feature.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()

        # Un-normalize Actions        
        action_norm_stats = self.get_action_stats(unnorm_key)
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high, action_low = np.array(action_norm_stats["q99"]), np.array(action_norm_stats["q01"])
        normalized_actions = np.clip(normalized_actions, -1, 1)
        normalized_actions[:, 6] = np.where(normalized_actions[:, 6] < 0.5, 0, 1) 
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions,
        )
        # actions max 1, min -0.05 # æ„Ÿè§‰ä¸å†ä¸€ä¸ª scale
        return actions, normalized_actions


    def freeze_backbones(self, stage): # TODO freeze æ˜¯æ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œ å¾—å…¨å±€æ§åˆ¶ï¼Œ è¦å†™ä¸ªæç¤º ç”¨æˆ·åœ¨è¿™é‡Œ åšå£°æ˜
        # self.vlm.freeze_backbones(stage)

        """
        æ ¹æ®ç»™å®šçš„æ­£åˆ™æ¨¡å¼åˆ—è¡¨å†»ç»“æ¨¡å—ã€‚
        å¦‚æœæŸä¸ªæ¨¡å—çš„åç§°åŒ¹é…ï¼ˆä¾‹å¦‚å…¬å…±å‰ç¼€åŒ¹é…ï¼‰ï¼Œåˆ™å†»ç»“è¯¥æ¨¡å—ä¸‹æ‰€æœ‰å‚æ•°ï¼ˆä¸å†é€’å½’å†»ç»“å­æ¨¡å—ï¼‰ï¼Œ
        å¹¶è¿”å›å†»ç»“æ¨¡å—åç§°çš„æœ‰åºæµ…å±‚åˆ—è¡¨ã€‚
        
        å‚æ•°ï¼š
            patterns: æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¨¡å—åç§°åªè¦åŒ¹é…å…¶ä¸­ä¸€ä¸ªæ¨¡å¼ï¼Œå°±ä¼šè¢«å†»ç»“ã€‚
            
        è¿”å›ï¼š
            ä¸€ä¸ªå†»ç»“æ¨¡å—åç§°çš„åˆ—è¡¨ï¼ˆæŒ‰é€’å½’é¡ºåºï¼‰ã€‚
        """
        # r"^vlm\.model\.visual", r"^action_model"
        # return []
        patterns = ["qwen_vl_interface"] #TODO æ—¶å€™è¦å‚æ•°åŒ–
        def freeze_module(module: nn.Module, prefix: str) -> List[str]:
            # å¦‚æœå½“å‰æ¨¡å—åç§°åŒ¹é…ä»»ä¸€æ¨¡å¼ï¼Œåˆ™å†»ç»“å½“å‰æ¨¡å—ï¼Œä¸å†é€’å½’å­æ¨¡å—
            if any(re.match(pattern, prefix) for pattern in patterns if prefix):
                for param in module.parameters(recurse=False):
                    param.requires_grad = False
                return [prefix]
            # å¦åˆ™ï¼Œé€’å½’éå†å­æ¨¡å—
            frozen_keys = []
            for name, child in module.named_children():
                full_name = f"{prefix}.{name}" if prefix else name
                frozen_keys.extend(freeze_module(child, full_name))
            
            self.qwen_vl_interface.eval()
            # åœ¨æ¨¡å‹åˆå§‹åŒ–æˆ–åŠ è½½ä¹‹åè°ƒç”¨
            for param in self.qwen_vl_interface.parameters():
                param.requires_grad = False
            # freeze the qwen_vl_interface
            
            return frozen_keys
            

        # å¯¹æ•´ä¸ªæ¨¡å—ï¼ˆselfï¼‰é€’å½’æ£€æŸ¥ã€‚æ³¨æ„ï¼Œæ ¹ç›®å½•é€šå¸¸ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿™é‡Œä¸å†»ç»“æ ¹æ¨¡å—
        frozen = []
        for name, child in self.named_children():
            full_name = name  # é¡¶å±‚æ¨¡å—åç§°
            frozen.extend(freeze_module(child, full_name))
        dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆå†»ç»“æ“ä½œ
        return frozen


    @classmethod
    def from_pretrained( # @Jinhui TODO è¿™é‡Œè¦å†™å¦‚ä½•resume checkpoints
        cls,
        pretrained_checkpoint: str,
        **kwargs,
    ) -> None:
        pretrained_checkpoint = Path(pretrained_checkpoint)
        model_config, norm_stats = read_mode_config(pretrained_checkpoint) # è¯»å– config å’Œ norm_stats
        # Initialize CogACT
        qwenQFormerACT = build_model_framework(model_config) 
        # set for action un-norm
        qwenQFormerACT.norm_stats = norm_stats
        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)
        model_state_dict = torch.load(pretrained_checkpoint, map_location="cpu") #["model"]
        
        model_keys = set(qwenQFormerACT.state_dict().keys())
        checkpoint_keys = set(model_state_dict.keys())

        # âœ… 1. åŠ è½½åŒ¹é…çš„æƒé‡
        for key in checkpoint_keys:
            if key in model_keys:
                try:
                    qwenQFormerACT.state_dict()[key].copy_(model_state_dict[key])
                    # overwatch.info(f"âœ… Loaded: {key}")
                except Exception as e:
                    overwatch.warning(f"âš ï¸ Failed to copy weight for key '{key}': {e}")
            else:
                overwatch.warning(f"âš ï¸ Checkpoint has unknown key '{key}' (not in model). Ignoring.")

        # âœ… 2. åå‘æ£€æŸ¥ï¼šæ¨¡å‹ä¸­æœ‰ä½† checkpoint ä¸­ç¼ºå¤±çš„
        missing_keys = model_keys - checkpoint_keys # TODO è¿™é‡Œä¹‹åè¦è€ƒè™‘ nontrainable params --> æˆ‘è§‰å¾—æ²¡å¿…è¦çœå­˜å‚¨ç©ºé—´
        for key in sorted(missing_keys):
                overwatch.warning(f"âš ï¸ Model expects key '{key}' but it's missing in checkpoint.")
                
        return qwenQFormerACT
    
    @staticmethod
    def _check_unnorm_key(norm_stats, unnorm_key):
        if unnorm_key is None:
            assert len(norm_stats) == 1, (
                f"Your model was trained on more than one dataset, "
                f"please pass a `unnorm_key` from the following options to choose the statistics "
                f"used for un-normalizing actions: {norm_stats.keys()}"
            )
            unnorm_key = next(iter(norm_stats.keys()))

        assert unnorm_key in norm_stats, (
            f"The `unnorm_key` you chose is not in the set of available dataset statistics, "
            f"please choose from: {norm_stats.keys()}"
        )
        return unnorm_key

    def get_action_dim(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return len(self.norm_stats[unnorm_key]["action"]["q01"])

    def get_action_stats(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return self.norm_stats[unnorm_key]["action"]

# TODO å†™ä¸€ä¸ªbuild model å‡½æ•°

def build_model_framework(model_config: dict = {}) -> QwenQFormerDiT:
    # TODO  å®ç°å’Œconfig å¯¹åº”çš„ load é€»è¾‘

    model = QwenQFormerDiT(
    qwen_model_name='/mnt/petrelfs/yejinhui/Projects/llavavla/playground/Pretrained_models/Qwen2.5-VL-3B-Instruct',
    action_model_type='DiT-B',
    vl_token_dim=2048,
    action_dim=7,
    future_action_window_size=15,
    past_action_window_size=0,
    # use_ema=False,
    )
        
    return model


def read_mode_config(pretrained_checkpoint):
    if os.path.isfile(pretrained_checkpoint):
        overwatch.info(f"Loading from local checkpoint path `{(checkpoint_pt := Path(pretrained_checkpoint))}`")

        # [Validate] Checkpoint Path should look like `.../<RUN_ID>/checkpoints/<CHECKPOINT_PATH>.pt`
        assert (checkpoint_pt.suffix == ".pt")
        run_dir = checkpoint_pt.parents[1]

        # Get paths for `config.json`, `dataset_statistics.json` and pretrained checkpoint
        config_json, dataset_statistics_json = run_dir / "config.json", run_dir / "dataset_statistics.json"
        assert config_json.exists(), f"Missing `config.json` for `{run_dir = }`"
        assert dataset_statistics_json.exists(), f"Missing `dataset_statistics.json` for `{run_dir = }`"

    # Otherwise =>> try looking for a match on `model_id_or_path` on the HF Hub (`model_id_or_path`)

        # Load VLA Config (and corresponding base VLM `ModelConfig`) from `config.json`
        with open(config_json, "r") as f:
            vla_cfg = json.load(f)["vla"]
            # model_cfg = ModelConfig.get_choice_class(vla_cfg["base_vlm"])() #@TODO check æˆ‘è§‰å¾—å…¶å®ä¸é‡è¦ï¼Œ

        # Load Dataset Statistics for Action Denormalization
        with open(dataset_statistics_json, "r") as f:
            norm_stats = json.load(f)
    return vla_cfg, norm_stats

def load_from_pretrained(pretrained_checkpoint):
    """Load a pretrained QwenQFormerDiT model from a checkpoint."""



    # = Load Individual Components necessary for Instantiating a VLA (via base VLM components) =


    # TODO è¿™é‡Œåº”è¯¥æ˜¯ä»configä¸­åŠ è½½
    model = QwenQFormerDiT.from_pretrained(
        pretrained_checkpoint=pretrained_checkpoint)
    return model

if __name__ == "__main__":

    # æ¨¡å‹å‚æ•°
    import debugpy
    debugpy.listen(("0.0.0.0", 5878))
    print("ğŸ” Rank 0 waiting for debugger attach on port 5878...")
    debugpy.wait_for_client()

    model_frameword = build_model_framework()
    pass