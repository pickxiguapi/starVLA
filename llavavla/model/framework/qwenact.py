"""
cogactvla.py

"""
from __future__ import annotations

import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from types import SimpleNamespace
import torch, json
import torch.nn as nn
import numpy as np
from PIL import Image
import re
from prismatic.overwatch import initialize_overwatch

from llavavla.model.action_model.action_model import ActionModel
import torch.distributed as dist
# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100

def dict_to_namespace(d):
    if isinstance(d, dict):
        return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})
    elif isinstance(d, list):
        return [dict_to_namespace(i) for i in d]
    else:
        return d

# get QWen2.5
from llavavla.model.vlm import _QWen_VL_Interface #ä¸åº”è¯¥å¼ºä¾èµ–äºè¿™ä¸ªï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªæ¥å£ç±»ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„ç±», TODO ä¸è¦å®ç° hard æ¥å£ç±»ï¼Œ ä½¿ç”¨ **kwargs
from llavavla.model.tools import auto_get_module_keys, auto_get_trainable_modules # åç»­åº”è¯¥æ˜¯trainer çš„èŒè´£èŒƒå›´
from llavavla.model.vlm.QWen2_5 import get_qwen2_5_interface
from llavavla.model.projector.QFormer import get_layerwise_qformer

class QwenQFormerDiT(nn.Module):
    def __init__(
        self,
        qwen_model_name:str = './playground/Pretrained_models/Qwen2.5-VL-3B-Instruct', # è¿™æ˜¯ä¸å¥½çš„å®ç°ï¼Œ ä¸€å®šä¸èƒ½æ˜¯äº’ç›¸ä¾èµ–
        action_model_type: str = 'DiT-B', 
        vl_token_dim: int = 2048,
        action_hidden_dim: int = 768,  # @Jinhui # è¿™ä¸ª åº”è¯¥æ˜¯å’ŒDiT-B
        action_dim: int = 7,
        future_action_window_size: int = 15,
        past_action_window_size: int = 0,
        use_ema: bool = False,
        norm_stats: Dict[str, Dict[str, Dict[str, Dict[str, List[float]]]]] = None,
        config: Optional[dict] = None,  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config, ä½†æ˜¯ç°åœ¨æ˜¯ç›´æ¥ä¼ å…¥å‚æ•°
        **kwargs,
    ) -> None:
        super().__init__()
        
        # TODO å…¨éƒ¨è½¬ å…¨å±€config, è¦é¢å‘å¯¹è±¡ç¼–ç¨‹
        self.qwen_vl_interface = get_qwen2_5_interface(qwen_model_name, config) 
        self.layer_qformer = get_layerwise_qformer(input_hidden_dim=vl_token_dim, output_hidden_dim=action_hidden_dim,config=config) # @Jinhui éœ€è¦é€»è¾‘ä»QWen ä¸­å¯¹é½ hidden
        self.action_model = ActionModel(model_type = action_model_type,  # TODO @Jinhui åº”è¯¥å†™åˆ° get_action_model()
                                            action_hidden_dim = action_hidden_dim, # è¿™äº›å‚æ•°å…³ç³»è¦ TODOé›†ä¸­ è®¾ç½®åˆ°config
                                            in_channels = action_dim, 
                                            future_action_window_size = future_action_window_size, 
                                            past_action_window_size = past_action_window_size) # ä¹Ÿåº”è¯¥ç”¨ å‡½æ•°å°è£…
        
        # TODO ActionModel éœ€è¦å’Œqformer ä¸€èµ·è®¾è®¡
        self.config = config
        # self.qwen_processor = vlm.processor # è¦é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œ ä¸è¦ å±æ€§å¤–æ³„
        # è¿™äº›æ˜¯ action chunck çš„å‚æ•°
        self.future_action_window_size = future_action_window_size
        self.past_action_window_size = past_action_window_size

        self.all_module_keys = auto_get_module_keys(self) #  TODO è¿™ä¸ªæ˜¯trainerçš„ funx
        self.norm_stats = norm_stats # è¿™ä¸ªæ˜¯ inference æ—¶å€™ç”¨åˆ°çš„ï¼Œ ä¸åº”è¯¥æ˜¯æ”¾åˆ°è¿™ä¸ªä½ç½®ï¼Ÿ

    @property
    def trainable_module_keys(self) -> List[str]:

        # TODO check, åŸç‰ˆè¿”å›çš„æ­» vlm.model, æ–°çš„å®ç°æ˜¯vlm --> çœ‹ä¸€ä¸‹ä¿å­˜é€»è¾‘æ˜¯å¦å‘ä¸Šå˜åŒ–
        keys = auto_get_trainable_modules(self, max_depth=1)# auto å»åˆ¤æ–­å“ªäº›moduleæ˜¯trainableçš„
        return keys
    

    def forward( # TODO éœ€è¦å°† loss è®¡ç®—åˆ†ç¦»å‡ºæ¥
        self, # åªé¢å¯¹æœ€åŸå§‹çš„ data exmaples, ä¸ºäº†å¯è¯»æ€§ï¼Œè¿™é‡Œè¿˜æ˜¯è¦å†™æˆæ˜¾ç¤ºçš„å‚æ•°
        examples: List[dict] = None,  # è¿™é‡Œçš„ examples æ˜¯æŒ‡åŸå§‹çš„è¾“å…¥æ•°æ®
        repeated_diffusion_steps: int = 4,
        **kwargs,  # ğŸ‘ˆ æ•æ·ä»£ç çš„çµæ´»æ€§ï¼Œ å…è®¸ä»»ä½•å½¢å¼çš„ä¼ å‚æ•°
    ) -> Tuple:
        """Run a forward pass through the VLM, returning a CausalLMOutputWithPast instance (contains loss)."""
        # @Jinhui TBD TODO 
        # pixel_values = pixel_values["pixel_values"] # labeles = pixel_values["labels"]
        # dist.barrier()

        # images: Optional[torch.FloatTensor] = None,
        # instructions: Optional[List] = None,
        # actions: Optional[torch.FloatTensor] = None,
        images = [example["image"] for example in examples]  #  TODO check æ˜¯ä»€ä¹ˆ
        instructions = [example["lang"] for example in examples]  # [B, str]
        actions = [example["action"] for example in examples] #label
        
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=images, instructions = instructions) # @Jinhui TODO add instruction to qwenvl inputs
        with torch.autocast("cuda", dtype=torch.float16):
            # dist.barrier()  # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åŠ è½½å®Œæ¯•
            qwenvl_outputs = self.qwen_vl_interface( # éƒ½æ˜¯localçš„å‚æ•°å˜åŒ–ï¼Œ ä¸è¦å†™åˆ°config, ä½†æ˜¯ä¸ºäº†ä¿æŒå¯å¤ç°ï¼Œåº”è¯¥æœ‰ä¸ªé»˜è®¤çš„ yaml
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                # use_cache=use_cache,
                output_attentions=False, # Flash attention è¿˜ä¸ç¡®å®šæ˜¯å¦æ”¯æŒè¿”å›attentionï¼Œ å®˜æ–¹ä»£ç æœ‰bug
                output_hidden_states=True,
                return_dict=True,
                # past_key_values=past_key_values,
                # **kwargs
                )
        
        vlm_loss = qwenvl_outputs.loss # @Jinhui TODO è¿™é‡Œæ˜¯å¯ä»¥study çš„åœ°æ–¹ï¼Œ æ˜¯å¦ training lang
        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.vla.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.vla.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[start_layer:end_layer]) # [B, 64, D_action]
    
        actions = torch.stack([torch.tensor(a) for a in actions], dim=0).to(action_latent_feature.device)  # [B, chunk, 7] @Jinhui TODO to tensor çš„é€»è¾‘å¯ä»¥æ”¾åˆ° transform é‡Œé¢
        actions_future = actions[:, -(self.future_action_window_size+1):, :]
        
        # Repeat 'actions' 'repeated_diffusion_steps' times, resulting in [repeated_diffusion_steps*B, T, D]
        actions_repeated = actions_future.repeat(repeated_diffusion_steps, 1, 1)
        action_latent_feature = action_latent_feature.repeat(repeated_diffusion_steps, 1, 1)  # [repeated_diffusion_steps*B, T, D_action]
        # Action model forward and compute loss # è¿™é‡ŒåŠŸèƒ½æœ‰ç‚¹ è¶Šä¿ä»£åº– TODO å°†loss é›†ä¸­åˆ° main moduleä¸­ç»Ÿä¸€å¤„ç†
        action_loss = self.action_model.loss(actions_repeated, action_latent_feature) # TODO loss åº”è¯¥æ”¾åˆ°å¦ä¸€ä¸ªå‡½æ•°
        return action_loss, qwenvl_outputs

    # @torch.inference_mode() # @Jinhui DEBUG ä¸´æ—¶å–æ¶ˆ
    def predict_action( # 
        self, image: Image, 
        instruction: str, 
        unnorm_key: Optional[str] = None, 
        cfg_scale: float = 1.5, 
        use_ddim: bool = False,
        num_ddim_steps: int = 5,
        **kwargs: str
    ) -> np.ndarray:
        """
        Core function for VLA inference; maps input image and task instruction to continuous action.

        @param image: PIL Image as [height, width, 3]
        @param instruction: Task instruction string
        @param unnorm_key: Optional dataset name for retrieving un-normalizing statistics; if None, checks that model
                           was trained only on a single dataset, and retrieves those statistics.
        @param cfg_scale: Scaling factor for classifier-free guidance (CFG); if == 1.0, CFG is disabled.
        @param use_ddim: Use DDIM sampling instead of DDPM sampling.
        @param num_ddim_steps: Number of DDIM steps to use for sampling.

        @return Unnormalized (continuous) action vector --> end-effector deltas.
        """

        # @ä¹‹åå†™å…¥æ¨¡å‹å†…éƒ¨ï¼Œ å˜æˆç§æœ‰åŒ–æ–¹æ³•
        imgs = [image.resize((224, 224))]  # list of PLT RGB for one instruction
        lang = instruction.lower() 

        inferface_inputs =  self.qwen_vl_interface.build_qwenvl_inputs(images=[imgs], instructions = [lang]) # @Jinhui TODO add instruction to qwenvl inputs
        qwen_inputs = inferface_inputs
        # Invoke super().generate --> taps into `GenerationMixin` which (redirects) to `forward()`

        
        # add by Jinhui

        # end add by Jinhui
        # Generate cognition feature through vlm
        with torch.autocast("cuda", dtype=torch.bfloat16):
            # fmt: off
            qwenvl_outputs = self.qwen_vl_interface( # TODO è¿™é‡Œä¹‹åè¦ç”¨generation func
                input_ids=qwen_inputs.input_ids,
                attention_mask=qwen_inputs.attention_mask,
                pixel_values=qwen_inputs.pixel_values, # [512, 1176] çŸ³æ–›æ²¡æœ‰ B,  
                image_grid_thw =qwen_inputs.image_grid_thw, # 2* [1,16,16] --> 512 = 16*16*2, 1176 = (224/16)^2 * 3 * 2 @JinhuiYE TODO è¿™ä¸ªéœ€è¦æ‰¾Qwen çš„å®˜æ–¹æ–‡æ¡£éªŒè¯
                labels= qwen_inputs.input_ids.clone(),
                output_hidden_states=True, 
                return_dict=True,
            ) # generation æ‹¿ä¸åˆ°å‰é¢token çš„ä¿¡æ¯ï¼Œè€ƒè™‘ä½¿ç”¨ forward?

        with torch.autocast("cuda", dtype=torch.bfloat16):
            start_layer = self.config.vla.qformer_start_layer if self.config else -6  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            end_layer = self.config.vla.qformer_end_layer if self.config else -1  # @Jinhui TODO è¿™é‡Œåº”è¯¥æ˜¯config
            
            action_latent_feature = self.layer_qformer(qwenvl_outputs.hidden_states[start_layer:end_layer]) # [B, 64, D_action]
            
            # Jinhui see text # outputs.sequences.shape: B, len with prefix
            # outputs.input_ids = outputs.sequences # ä¸ºäº†å’Œ input dict ä¿æŒä¸€è‡´ï¼Œ æ–¹ä¾¿è°ƒç”¨ self._get_cognition_features# è¿˜çœŸä¸å¤ªä¸€æ ·ï¼Œå› ä¸ºgenerationçš„é€»è¾‘å’Œ forwardä¸ä¸€æ ·
            # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs.sequences)]
            # output_text = self.qwen_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            # print("output:\n",output_text[0])
            # fmt: on
            # æˆ‘ä»¬trainingçš„æ—¶å€™æ˜¯ image ä¸å›ºå®šåœ¨æœ€å‰é¢æ²¡ï¼Œæ˜¯æ²¡åŠæ³•åªmax_new = 1 çš„

        
        using_cfg = cfg_scale > 1.0

        model_dtype = next(self.action_model.net.parameters()).dtype
        B = action_latent_feature.shape[0]

        # Sample random noise
        noise = torch.randn(B, self.future_action_window_size+1, self.action_model.in_channels, device=action_latent_feature.device).to(model_dtype)  #[B, T, D]

        # Setup classifier-free guidance:
        if using_cfg:
            noise = torch.cat([noise, noise], 0) #[2,16,7]
            uncondition = self.action_model.net.z_embedder.uncondition # [64, 768]
            uncondition_shape = uncondition.shape
            uncondition = uncondition.unsqueeze(0)  #[1, 64, D]
            uncondition = uncondition.expand(B, uncondition_shape[0], uncondition_shape[1]) #[B, n_qformer_token, D] # 
            z = torch.cat([action_latent_feature, uncondition], 0) # [2, 64, 768] TODO check çœ‹çœ‹ trainingçš„æ—¶å€™æ˜¯å‰æ‰‹
            cfg_scale = cfg_scale
            model_kwargs = dict(z=z, cfg_scale=cfg_scale)
            sample_fn = self.action_model.net.forward_with_cfg
        else:
            model_kwargs = dict(z=action_latent_feature)
            sample_fn = self.action_model.net.forward
        
        # if os.environ.get("DEBUG"):
        #     print(z .shape)
        # DDIM Sampling
        if use_ddim and num_ddim_steps is not None:
            if self.action_model.ddim_diffusion is None: #@JinhuiYE =TODO check, shape ä¸Šæ²¡é—®é¢˜ï¼Œ å°±ä¸çŸ¥é“traine / infer å’Œå†…éƒ¨æ“ä½œæ˜¯å¦æœ‰é—®é¢˜äº†
                self.action_model.create_ddim(ddim_step=num_ddim_steps)
            samples = self.action_model.ddim_diffusion.ddim_sample_loop(sample_fn, 
                                                                noise.shape, 
                                                                noise, 
                                                                clip_denoised=False,
                                                                model_kwargs=model_kwargs,
                                                                progress=False,
                                                                device=action_latent_feature.device,
                                                                eta=0.0
                                                                )
        else:
            # DDPM Sampling
            samples = self.action_model.diffusion.p_sample_loop(sample_fn, 
                                                                    noise.shape, 
                                                                    noise, 
                                                                    clip_denoised=False,
                                                                    model_kwargs=model_kwargs,
                                                                    progress=False,
                                                                    device=action_latent_feature.device
                                                                    )
        if using_cfg:
            samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
        normalized_actions = samples[0].cpu().numpy()

        # Un-normalize Actions        
        action_norm_stats = self.get_action_stats(unnorm_key)
        mask = action_norm_stats.get("mask", np.ones_like(action_norm_stats["q01"], dtype=bool))
        action_high, action_low = np.array(action_norm_stats["q99"]), np.array(action_norm_stats["q01"])
        normalized_actions = np.clip(normalized_actions, -1, 1)
        normalized_actions[:, 6] = np.where(normalized_actions[:, 6] < 0.5, 0, 1) 
        actions = np.where(
            mask,
            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,
            normalized_actions,
        )
        # actions max 1, min -0.05 # æ„Ÿè§‰ä¸å†ä¸€ä¸ª scale
        return actions, normalized_actions


    def freeze_backbones(self):
        """
        æ ¹æ®ç›¸å¯¹æ¨¡å—è·¯å¾„åˆ—è¡¨ï¼ˆpatternsï¼‰ç›´æ¥å†»ç»“æŒ‡å®šå­æ¨¡å—ï¼Œä¸å†é€’å½’æŸ¥æ‰¾æ‰€æœ‰å­æ¨¡å—åç§°ï¼š
          - patterns: ä» config.vla.freeze_modules ä¸­è¯»å–ï¼Œç”¨é€—å·åˆ†éš”å¾—åˆ°çš„â€œç›¸å¯¹è·¯å¾„â€åˆ—è¡¨
            ä¾‹å¦‚ "qwen_vl_interface,action_model.net"ï¼Œ
            å°±æ„å‘³ç€å†»ç»“ self.qwen_vl_interface å’Œ self.action_model.netã€‚
        è¿”å›å€¼ï¼š
          - frozen: å®é™…æ‰¾åˆ°å¹¶å†»ç»“çš„æ¨¡å—è·¯å¾„åˆ—è¡¨
        """
        freeze_modules = (
            self.config.vla.freeze_modules
            if (self.config and hasattr(self.config.vla, "freeze_modules"))
            else None
        )
        # æ‹†åˆ†å¹¶å»é™¤ç©ºç™½
        patterns = [p.strip() for p in freeze_modules.split(",") if p.strip()] if freeze_modules else []

        frozen = []
        for path in patterns:
            # å°†â€œç›¸å¯¹è·¯å¾„â€æŒ‰ç‚¹æ‹†åˆ†ï¼Œä¾‹å¦‚ "action_model.net" â†’ ["action_model", "net"]
            attrs = path.split(".")
            module = self
            try:
                for attr in attrs:
                    module = getattr(module, attr)
                # å¦‚æœæˆåŠŸ get åˆ° moduleï¼Œå°±æŠŠå®ƒå’Œå®ƒçš„æ‰€æœ‰å­æ¨¡å—å‚æ•°éƒ½ freeze
                for param in module.parameters():
                    param.requires_grad = False
                frozen.append(path)
            except AttributeError:
                # å¦‚æœæŸä¸€çº§å±æ€§ä¸å­˜åœ¨ï¼Œå°±è·³è¿‡å¹¶æ‰“å°è­¦å‘Š
                print(f"âš ï¸ æ¨¡å—è·¯å¾„ä¸å­˜åœ¨ï¼Œæ— æ³•å†»ç»“ï¼š{path}")
                continue

        dist.barrier()  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶åŒæ­¥
        print(f"ğŸ”’ Frozen modules (by relative path): {frozen}")
        return frozen
    
    def print_freeze_status(self): # è¿™ä¸ªæ˜¯ å·¥å…·ç±»æ–¹æ³•ã€‚ å¯ä»¥è€ƒè™‘ç§»åŠ¨
        for name, param in self.named_parameters():
            status = "Frozen" if not param.requires_grad else "Trainable"
            print(f"{name:60s}  |  {status}")

    @classmethod
    def from_pretrained( # @Jinhui TODO è¿™é‡Œè¦å†™å¦‚ä½•resume checkpoints
        cls,
        pretrained_checkpoint: str,
        **kwargs,
    ) -> None:
        pretrained_checkpoint = Path(pretrained_checkpoint)
        model_config, norm_stats = read_mode_config(pretrained_checkpoint) # è¯»å– config å’Œ norm_stats
        # Initialize CogACT
        # model_config TODO DEBUE @JinhuiYE è¿™é‡Œåº”è¯¥ä¿è¯training infer çš„å‚æ•°å’Œæ¨¡å‹ğŸ”—æ˜¯ä¸€è‡´çš„ ï¼ˆç‰¹åˆ«æ˜¯ QFormer)
        # TODO 
        model_config = dict_to_namespace(model_config)
        if os.getenv("DEBUG"):
            print(f"ğŸ” Loading config from pretrained checkpoint: {pretrained_checkpoint}")
        # å®‰å…¨è®¾ç½®å±æ€§
        if not hasattr(model_config.vla, "qformer_start_layer"):
            model_config.vla.qformer_start_layer = 31
            model_config.vla.qformer_end_layer = 37
        
        qwenQFormerACT = build_model_framework(model_config) 
        # set for action un-norm
        qwenQFormerACT.norm_stats = norm_stats
        # Load from Checkpoint (Custom --> should load both *projector* and *llm* weights)
        model_state_dict = torch.load(pretrained_checkpoint, map_location="cpu") #["model"]
        
        model_keys = set(qwenQFormerACT.state_dict().keys())
        checkpoint_keys = set(model_state_dict.keys())

        # âœ… 1. åŠ è½½åŒ¹é…çš„æƒé‡
        for key in checkpoint_keys:
            if key in model_keys:
                try:
                    qwenQFormerACT.state_dict()[key].copy_(model_state_dict[key])
                    # overwatch.info(f"âœ… Loaded: {key}")
                except Exception as e:
                    overwatch.warning(f"âš ï¸ Failed to copy weight for key '{key}': {e}")
            else:
                overwatch.warning(f"âš ï¸ Checkpoint has unknown key '{key}' (not in model). Ignoring.")

        # âœ… 2. åå‘æ£€æŸ¥ï¼šæ¨¡å‹ä¸­æœ‰ä½† checkpoint ä¸­ç¼ºå¤±çš„
        missing_keys = model_keys - checkpoint_keys # TODO è¿™é‡Œä¹‹åè¦è€ƒè™‘ nontrainable params --> æˆ‘è§‰å¾—æ²¡å¿…è¦çœå­˜å‚¨ç©ºé—´
        for key in sorted(missing_keys):
                overwatch.warning(f"âš ï¸ Model expects key '{key}' but it's missing in checkpoint.")
                
        return qwenQFormerACT
    
    @staticmethod
    def _check_unnorm_key(norm_stats, unnorm_key):
        if unnorm_key is None:
            assert len(norm_stats) == 1, (
                f"Your model was trained on more than one dataset, "
                f"please pass a `unnorm_key` from the following options to choose the statistics "
                f"used for un-normalizing actions: {norm_stats.keys()}"
            )
            unnorm_key = next(iter(norm_stats.keys()))

        assert unnorm_key in norm_stats, (
            f"The `unnorm_key` you chose is not in the set of available dataset statistics, "
            f"please choose from: {norm_stats.keys()}"
        )
        return unnorm_key

    def get_action_dim(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return len(self.norm_stats[unnorm_key]["action"]["q01"])

    def get_action_stats(self, unnorm_key=None):
        """Dimensionality of the policy's action space."""
        unnorm_key = self._check_unnorm_key(self.norm_stats, unnorm_key)
        return self.norm_stats[unnorm_key]["action"]

# TODO å†™ä¸€ä¸ªbuild model å‡½æ•°

def build_model_framework(model_config: dict = {}) -> QwenQFormerDiT:
    # TODO  å®ç°å’Œconfig å¯¹åº”çš„ load é€»è¾‘

    model = QwenQFormerDiT(
    qwen_model_name='/mnt/petrelfs/yejinhui/Projects/llavavla/playground/Pretrained_models/Qwen2.5-VL-3B-Instruct',
    action_model_type='DiT-B',
    vl_token_dim=2048,
    action_dim=7,
    future_action_window_size=15,
    past_action_window_size=0,
    # use_ema=False,
    config=model_config
    )
        
    return model


def read_mode_config(pretrained_checkpoint):
    if os.path.isfile(pretrained_checkpoint):
        overwatch.info(f"Loading from local checkpoint path `{(checkpoint_pt := Path(pretrained_checkpoint))}`")

        # [Validate] Checkpoint Path should look like `.../<RUN_ID>/checkpoints/<CHECKPOINT_PATH>.pt`
        assert (checkpoint_pt.suffix == ".pt")
        run_dir = checkpoint_pt.parents[1]

        # Get paths for `config.json`, `dataset_statistics.json` and pretrained checkpoint
        config_json, dataset_statistics_json = run_dir / "config.json", run_dir / "dataset_statistics.json"
        assert config_json.exists(), f"Missing `config.json` for `{run_dir = }`"
        assert dataset_statistics_json.exists(), f"Missing `dataset_statistics.json` for `{run_dir = }`"

    # Otherwise =>> try looking for a match on `model_id_or_path` on the HF Hub (`model_id_or_path`)

        # Load VLA Config (and corresponding base VLM `ModelConfig`) from `config.json`
        with open(config_json, "r") as f:
            vla_cfg = json.load(f) #["vla"]
            # model_cfg = ModelConfig.get_choice_class(vla_cfg["base_vlm"])() #@TODO check æˆ‘è§‰å¾—å…¶å®ä¸é‡è¦ï¼Œ

        # Load Dataset Statistics for Action Denormalization
        with open(dataset_statistics_json, "r") as f:
            norm_stats = json.load(f)
    return vla_cfg, norm_stats

def load_from_pretrained(pretrained_checkpoint):
    """Load a pretrained QwenQFormerDiT model from a checkpoint."""



    # = Load Individual Components necessary for Instantiating a VLA (via base VLM components) =


    # TODO è¿™é‡Œåº”è¯¥æ˜¯ä»configä¸­åŠ è½½
    
    model = QwenQFormerDiT.from_pretrained(
        pretrained_checkpoint=pretrained_checkpoint)
    return model

import OmegaConf
if __name__ == "__main__":

    # æ¨¡å‹å‚æ•°
    import debugpy
    debugpy.listen(("0.0.0.0", 5678))
    print("ğŸ” Rank 0 waiting for debugger attach on port 5878...")
    debugpy.wait_for_client()
    samples = {}

    config_yaml = "llavavla/conf/qwenvla_cotrain.yaml"
    cfg = OmegaConf.load(config_yaml)
    vla_cfg = cfg.vla
    model_framework = build_model_framework(vla_cfg)
    model_framework(samples)
    pass

    # git remote add gitee https://gitee.pjlab.org.cn/L2/MultimodalVLA/llavavla.git
    # git push -u gitee master