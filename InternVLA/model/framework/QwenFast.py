"""
Qwen-Fast Framework

A lightweight implementation for autoregressive discrete action prediction conditioned on multi-view images + instruction.

Key Points:
  - Qwen2.5 vision-language backbone
  - Unified action learning via next-token prediction (fast tokenizer)
  - Autoregressive action tokens derived from discretized / symbolized continuous actions

Note: How to add special tokens to Qwen2.5:
  /InternVLA/model/modules/vlm/tools/add_qwen_special_tokens/README.md
"""

from typing import List
from tqdm import tqdm
from typing import List, Optional, Tuple, Any
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
from qwen_vl_utils import process_vision_info


from InternVLA.training.trainer_utils import initialize_overwatch

logger = initialize_overwatch(__name__)

# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100

from InternVLA.model.framework.base_framework import baseframework
from InternVLA.model.modules.vlm.QWen2_5 import get_qwen2_5_interface
from InternVLA.model.modules.action_model.fast_ActionHeader import get_action_model
from InternVLA.training.trainer_utils.metrics import resize_images


class Qwenvl_Fast(baseframework):
    """
    Multimodal vision-language-action model.

    Components:
      - Qwen2.5 VL interface for fused language/vision token embeddings
      - Layer-wise QFormer for multi-layer feature aggregation
      - DINO encoder for dense multi-view spatial tokens
      - DiT diffusion head for future action sequence modeling

    Focus: Predict future continuous actions conditioned on images + instruction.
    """

    def __init__(
        self,
        config: Optional[dict] = None,
        **kwargs,
    ) -> None:
        """
        Construct all submodules and cache key configuration values.

        Args:
            config: Hierarchical configuration (OmegaConf/dict) containing framework + trainer sections.
            **kwargs: Reserved for future overrides (unused).
        """
        super().__init__()
        self.config = config
        self.qwen_vl_interface = get_qwen2_5_interface(config=self.config)
        self.action_model = get_action_model(config=self.config)

        self.future_action_window_size = config.framework.action_model.future_action_window_size
        self.past_action_window_size = config.framework.action_model.past_action_window_size
        self.chunk_len = self.past_action_window_size + 1 + self.future_action_window_size
        self.hidden_dim = config.framework.action_model.action_hidden_dim
        

    def forward(
        self,
        examples: List[dict] = None,
        **kwargs,
    ) -> Tuple:
        """
        è®­ç»ƒå‰å‘ï¼šç›´æ¥next token prediction é¢„æµ‹æœªæ¥åŠ¨ä½œï¼ˆæ— æ‰©æ•£ï¼‰ã€‚

        Flow:
          1. Build QwenVL inputs (images + instruction tokens)
          2. Extract hidden states from configured layer range
          7. Predict action and compute L1 loss

        Args:
            examples: List[dict], each dict requires:
                - image: List[PIL.Image] (multi-view)
                - lang: str instruction
                - action: np.ndarray or list shaped [T, action_dim]
            **kwargs: Reserved.

        Returns:
            dict:
                action_loss (torch.Tensor): Scalar diffusion noise prediction loss.
        """
        batch_images = [example["image"] for example in examples]  #  [Bï¼Œ[PLT]]
        instructions = [example["lang"] for example in examples]  # [B, str]
        actions = [example["action"] for example in examples]  # label [Bï¼Œ len, 7]
        
        # step 0: map_raw_action_to_vlm_action
        vlm_action_tokens = self.action_model.encoder_action2vlmtoken(actions)  # List[str]


        # Step 1: QWenVL input format
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=batch_images, instructions=instructions, solutions=vlm_action_tokens)
        
        with torch.autocast("cuda", dtype=torch.bfloat16):
            qwenvl_outputs = self.qwen_vl_interface(
                **qwen_inputs,
                output_attentions=False,
                output_hidden_states=False,
                return_dict=True,
            )
        
        vlm_action_loss = qwenvl_outputs.loss
        if vlm_action_loss is None or torch.isnan(vlm_action_loss): 
            vlm_action_loss = torch.tensor(0.0, device=self.qwen_vl_interface.model.device)

        return {"action_loss": vlm_action_loss}

    @torch.inference_mode()
    def predict_action(
        self,
        batch_images: List[List[Image.Image]],  # Batch of PIL Image list as [view1, view2]
        instructions: List[str],
        **kwargs: str,
    ) -> np.ndarray:
        """
        æ¨ç†ï¼šå•æ¬¡å‰å‘ç›´æ¥å›å½’æœªæ¥åŠ¨ä½œï¼ˆæ— æ‰©æ•£é‡‡æ ·ï¼‰ã€‚

        Steps:
          1. Resize images to training resolution (if specified)
          2. Encode with QwenVL (hidden states retained)
          6. Return normalized action trajectory

        Args:
            batch_images: List of samples; each sample is List[PIL.Image] (multi-view).
            instructions: List[str] natural language task instructions.
            cfg_scale: >1 enables classifier-free guidance (scales conditional vs unconditional).
            use_ddim: Whether to use DDIM deterministic sampling.
            num_ddim_steps: Number of DDIM steps if enabled.
            **kwargs: Reserved.

        Returns:
            dict:
                normalized_actions (np.ndarray): Shape [B, T, action_dim], diffusion-sampled normalized actions.
        """
        train_obs_image_size = getattr(self.config.datasets.vla_data, "image_size", None)
        if train_obs_image_size:
            batch_images = resize_images(batch_images, target_size=train_obs_image_size)
        instructions = [instruction for instruction in instructions]


        # Step 1: QWenVL input format
        qwen_inputs = self.qwen_vl_interface.build_qwenvl_inputs(images=batch_images, instructions=instructions)

        with torch.autocast("cuda", dtype=torch.bfloat16):
            generated_ids = self.qwen_vl_interface.model.generate(
                **qwen_inputs,
            )
        # --- Extract and decoder vlm_action to continue actions ---
        # --- extrace token (index based on VLM) ---
        batch_vlm_action_token_ids = self._extract_action_token_ids(generated_ids)
        # --- map index to fast tokenizer index space ---
        batch_fast_action_token_idx = self._decode_action_tokens(batch_vlm_action_token_ids)
        normalized_actions = self.action_model.fast_tokenizer.decode(batch_fast_action_token_idx)

        return {"normalized_actions": normalized_actions}

    def _extract_action_token_ids(
        self,
        generated_ids: torch.LongTensor,
    ) -> List[List[int]]:
        """
        ä»ç”Ÿæˆçš„ token åºåˆ—é‡ŒæŠ½å–åŠ¨ä½œ tokenï¼ˆå·²åŠ åç§»ï¼‰ï¼Œè¿”å›ä¸€ä¸ªäºŒç»´åˆ—è¡¨ï¼š
        ret[b] = [vlm_action_token_id_0, vlm_action_token_id_1, ...]
        è§„åˆ™ï¼šä¿ç•™æ‰€æœ‰è½åœ¨ [_ACTION_TOKEN_MIN, _ACTION_TOKEN_MAX] å†…çš„ tokenï¼ŒæŒ‰å‡ºç°é¡ºåºã€‚
        ä½ å¯æŒ‰éœ€è¦æ”¹æˆâ€œåªå–é¦–æ¬¡å‡ºç°åè¿ç»­æ®µâ€ã€‚
        """
        act_min = self.action_model._ACTION_TOKEN_MIN
        act_max = self.action_model._ACTION_TOKEN_MAX
        mask = (generated_ids >= act_min) & (generated_ids <= act_max)  # [B, L]
        results = []
        for b in range(generated_ids.size(0)):
            idx = mask[b].nonzero(as_tuple=False).flatten()
            if idx.numel() == 0:
                results.append([])
                continue
            # å…¨éƒ¨åŠ¨ä½œ token
            tokens = generated_ids[b, idx].tolist()
            results.append(tokens)
        return results

    def _decode_action_tokens(self, batch_vlm_tokens: List[List[int]]) -> List[Any]:
        """
        å°†å¸¦åç§»çš„ VLM åŠ¨ä½œ token åˆ—è¡¨è§£ç å› fast tokenizer è¯­ä¹‰ã€‚
        fast_tokenizer.decode é¢„æœŸè¾“å…¥ï¼šåŸå§‹ fast token id åºåˆ—ï¼ˆæœªåŠ åç§»ï¼‰ã€‚
        """
        act_min = self.action_model._ACTION_TOKEN_MIN
        decoded = []
        for seq in batch_vlm_tokens:
            if not seq:
                decoded.append(None)
                continue
            fast_ids = [t - act_min for t in seq]
            try:
                out = self.action_model.fast_tokenizer.decode(fast_ids)
            except Exception:
                # è‹¥ fast_tokenizer.decode æ¥å£ä¸åŒï¼Œå¯åœ¨è¿™é‡Œè°ƒæ•´
                out = None
            decoded.append(out)
        return decoded


def build_model_framework(config: dict = {}) -> Qwenvl_Fast:
    """
    å·¥å‚æ–¹æ³•ï¼šè¿”å›ç®€åŒ–ç‰ˆ Qwenvl_OFT
    """
    model = Qwenvl_Fast(config=config)
    return model


if __name__ == "__main__":
    from omegaconf import OmegaConf

    # model parameters
    import debugpy

    debugpy.listen(("0.0.0.0", 10092))
    print("ğŸ” Rank 0 waiting for debugger attach on port 10092...")
    debugpy.wait_for_client()

    config_yaml = "InternVLA/config/training/internvla_cotrain_oxe.yaml"
    cfg = OmegaConf.load(config_yaml)
    cfg.framework.qwenvl.base_vlm = "playground/Pretrained_models/nora"

    cfg.framework.action_model.action_hidden_dim 
    # try get model
    model = build_model_framework(cfg)
    print(model)

    # try forward model
    # can be fake sampleï¼Œ but here get from dataloader for simpler
    from InternVLA.dataloader.lerobot_datasets import get_vla_dataset, collate_fn

    vla_dataset_cfg = cfg.datasets.vla_data
    dataset = get_vla_dataset(data_cfg=vla_dataset_cfg)

    from torch.utils.data import DataLoader

    train_dataloader = DataLoader(
        dataset,
        batch_size=2,
        num_workers=1,  # For Debug
        collate_fn=collate_fn,
    )
    # zhe
    for batch in tqdm(train_dataloader, desc="Processing Batches"):
        batch
        break

    # try get model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model(batch)
    pass
    action = model.predict_action(batch_images=[batch[0]["image"]], instructions=[batch[0]["lang"]])
